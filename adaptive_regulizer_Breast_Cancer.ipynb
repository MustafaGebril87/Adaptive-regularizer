{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161d6a9a-44a0-48e1-b412-941a4c94dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940685ea-1863-4c08-9008-b26b0365d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2822266d-1ff2-43f9-9eea-05591d9fae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e66f0850b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split,Subset\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee276a1-b683-4930-810c-b699240f4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c1aab5-2c89-4b17-96d3-091576c96c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv(os.path.join(\"Breast_cancer\",\"data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5999ea0-df6c-4672-ac3b-5c9ee384d2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e20be6c-72be-44ae-b3ac-8d8f700447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"id\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e25c7aa-dcfd-41d5-8321-90705d77c40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           M        17.99         10.38          122.80     1001.0   \n",
       "1           M        20.57         17.77          132.90     1326.0   \n",
       "2           M        19.69         21.25          130.00     1203.0   \n",
       "3           M        11.42         20.38           77.58      386.1   \n",
       "4           M        20.29         14.34          135.10     1297.0   \n",
       "..        ...          ...           ...             ...        ...   \n",
       "564         M        21.56         22.39          142.00     1479.0   \n",
       "565         M        20.13         28.25          131.20     1261.0   \n",
       "566         M        16.60         28.08          108.30      858.1   \n",
       "567         M        20.60         29.33          140.10     1265.0   \n",
       "568         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0           0.2419  ...          17.33           184.60      2019.0   \n",
       "1           0.1812  ...          23.41           158.80      1956.0   \n",
       "2           0.2069  ...          25.53           152.50      1709.0   \n",
       "3           0.2597  ...          26.50            98.87       567.7   \n",
       "4           0.1809  ...          16.67           152.20      1575.0   \n",
       "..             ...  ...            ...              ...         ...   \n",
       "564         0.1726  ...          26.40           166.10      2027.0   \n",
       "565         0.1752  ...          38.25           155.00      1731.0   \n",
       "566         0.1590  ...          34.12           126.70      1124.0   \n",
       "567         0.2397  ...          39.42           184.60      1821.0   \n",
       "568         0.1587  ...          30.37            59.16       268.6   \n",
       "\n",
       "     smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \\\n",
       "0                  0.2654          0.4601                  0.11890   \n",
       "1                  0.1860          0.2750                  0.08902   \n",
       "2                  0.2430          0.3613                  0.08758   \n",
       "3                  0.2575          0.6638                  0.17300   \n",
       "4                  0.1625          0.2364                  0.07678   \n",
       "..                    ...             ...                      ...   \n",
       "564                0.2216          0.2060                  0.07115   \n",
       "565                0.1628          0.2572                  0.06637   \n",
       "566                0.1418          0.2218                  0.07820   \n",
       "567                0.2650          0.4087                  0.12400   \n",
       "568                0.0000          0.2871                  0.07039   \n",
       "\n",
       "     Unnamed: 32  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "..           ...  \n",
       "564          NaN  \n",
       "565          NaN  \n",
       "566          NaN  \n",
       "567          NaN  \n",
       "568          NaN  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23980cbe-9b2a-418d-80cf-2f5d6c00a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['diagnosis'] = data['diagnosis'].str.replace('M','0')\n",
    "data['diagnosis'] = data['diagnosis'].str.replace('B','1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1dfca8-d986-4dbf-b68d-fed68beefd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"Unnamed: 32\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80f74470-69ac-4301-bd1e-9e8eab675a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.drop(\"diagnosis\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913a5564-1799-4085-bcb7-07703ed6b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[\"diagnosis\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24e5bb69-cc7a-4e70-b331-29e0588b3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e583a0b-1de9-4e02-adc8-5926ccbd39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdccdbec-c1ae-4224-a1a0-c70c95926aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "standered=scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae93b731-3ed2-4a61-bf00-8c6b4601f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(standered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e63bc18-9762-49f3-94fa-20cd8289a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):       \n",
    "        \n",
    "        self.features=train\n",
    "        self.labels = target\n",
    "    def __getitem__(self, i):\n",
    "        feaures=self.features.iloc[i]\n",
    "        target=self.labels[i]\n",
    "        return torch.tensor(np.array(feaures)),torch.tensor(np.array(target))\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aba2551-393b-4a7e-a77e-565aeb48c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b59b1f6-69d4-4aee-aeef-6d00a64d51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,outNumber):\n",
    "        super(Net,self).__init__()\n",
    "        self.outNumber = outNumber\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.fc0 = nn.Linear(\n",
    "            in_features=30, out_features=100 ,bias=True)\n",
    "        self.depth = 100\n",
    "        self.linear_layers = []\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(\n",
    "            in_features=100, out_features=100, bias=True) for index in range(self.depth)])\n",
    "        self.fc = nn.Linear(in_features=100, out_features=self.outNumber, bias=True)\n",
    "        # self.activation = nn.sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.fc0(x)\n",
    "        residual = out\n",
    "        for index, layer in enumerate(self.linear_layers):\n",
    "            out = self.linear_layers[index](out)\n",
    "            out = f.leaky_relu(out)\n",
    "            # out = f.dropout(out, p=self.droup_out)\n",
    "            if index % 3 == 0:\n",
    "                out = out + residual\n",
    "                residual = out\n",
    "        out = self.fc(out)\n",
    "        out = torch.nn.functional.sigmoid(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6eff0bab-8c06-4556-bcca-288d52387392",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net(1)\n",
    "net=net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d5e451c4-eb78-479f-a58e-cb089369e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "274d6cc5-bf56-4661-9b6e-fac89855c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    accuracy = metrics.accuracy_score(y_true.cpu().detach().numpy(), y_prob.cpu().detach().numpy() > 0.5)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a38cd4cc-6fe4-4ea7-b04b-5ff1c5ac60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = MyDataset()\n",
    "#dataset_ = Subset(dataset_, np.arange(1000000))\n",
    "# Split into training and test\n",
    "train_size = int(0.3 * len(dataset_))\n",
    "test_size = len(dataset_) - train_size\n",
    "\n",
    "\n",
    "trainset, testset = random_split(dataset_, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False)\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1.0E-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a22878f5-6ec0-4c52-9532-b81bae0dfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pytorch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.optim as optim \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class train_:\n",
    "    def __init__(self,epochs,model,train_dl,val_dl,criterion,optimizer):\n",
    "        self.model = model\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "    def accuracy(self,y_prob,y_true):\n",
    "        y_prob = y_prob > 0.5\n",
    "        return (y_true == y_prob).sum().item() / y_true.size(0)    \n",
    "   # def accuracy(self,predictions, labels):# accuracy function from the resulted predictions\n",
    "    #    #accuracy = (torch.softmax(predictions, dim=1).argmax(dim=1) == labels).sum().float() / float( labels.size(0) )\n",
    "     #   accuracy = ((predictions== labels).sum().float() / float( labels.size(0) ))\n",
    "      #  return accuracy    \n",
    "    \n",
    "    def training(self):\n",
    "        Max_Accu_Val = .0 # start with zero accuracy to compare the results\n",
    "        lamda = torch.tensor(0.6).type(torch.float).requires_grad_() \n",
    "        lamda = torch.tensor(0.6,requires_grad=True, device=\"cuda\")\n",
    "        self.optimizer2 = optim.Adam([lamda], lr=1E-6)\n",
    "        error = []\n",
    "        acctest_=[]\n",
    "        acctrain_=[]\n",
    "        \n",
    "        self.model = self.model.double()\n",
    "        for e in range(self.epochs): # for every epoch\n",
    "            CounterTrain =0 # initialize the values with zero, this used to calculate the number of  training epochs\n",
    "            CounterVal =0  # this used as number of test epochs\n",
    "            train_loss = 0.0 # initialize the training loss with zero\n",
    "            TrainAccAll = 0.0 # the summed accuracy for the whole training epoch\n",
    "            VallAccAll = 0.0 # the summed accuracy of the whole test epoch\n",
    "            lossTrainAll = 0.0 # the summed loss for the whole training epoch\n",
    "            lossValAll = 0.0 # the summed loss for the whole test epoch\n",
    "            lossVarAll=0\n",
    "            acctest=0\n",
    "            countertest=0\n",
    "            lossTestAll=0.0\n",
    "            self.model.train() \n",
    "            \n",
    "            # signal a training process\n",
    "            for data, labels in self.train_dl: # for every input and output in train data loader\n",
    "                self.model.train() \n",
    "\n",
    "                if torch.cuda.is_available(): # check if cuda is a vailable\n",
    "                   data, labels = data.cuda(), labels.cuda() # feed the input and output to cuda\n",
    "               \n",
    "                self.optimizer.zero_grad() # zero the gradient, required by pytorch\n",
    "               \n",
    "                target = self.model(data.double()) # calculate the input\n",
    "               #outnorm = [float(i)/max(out) for i in out]\n",
    "\n",
    "                dataiter = iter(self.val_dl)\n",
    "                inputsTest, labelsTest  = next(dataiter)\n",
    "                labelsTest = labelsTest.to(\"cuda\")\n",
    "                inputsTest = inputsTest.to(\"cuda\")\n",
    "                self.model.eval() \n",
    "                targettest= self.model(inputsTest.double()) \n",
    "                \n",
    "                acctest += self.accuracy(targettest, labelsTest)\n",
    "                countertest+=1\n",
    "                \n",
    "                \n",
    "                l1 =1# sum(p.abs().sum() for p in self.model.parameters())\n",
    "                \n",
    "                lossOriginal = self.criterion(target.double(), labels.double())\n",
    "              \n",
    "                loss =lossOriginal+lamda*l1\n",
    "                x = (1/(labels-torch.sqrt(torch.abs(lossOriginal-lamda))+.0000001))-1\n",
    "                varout = torch.abs(torch.var(-(x*x-1)/(x+.0000001)))\n",
    "                loss.backward(retain_graph=True)# neural network backward calculations\n",
    "                self.optimizer2.zero_grad()\n",
    "                #var_=torch.abs(torch.var(out)-.1*torch.max(out))\n",
    "                varout.backward()\n",
    "                self.optimizer.step() \n",
    "                self.optimizer2.step()\n",
    "                # optimize\n",
    "                \n",
    "                train_loss = lossOriginal.item()# sotre loss values\n",
    "                TrainAcc = self.accuracy(target.double(),labels.double()) # calculate training accuracy\n",
    "                CounterTrain+=1 # update training counter\n",
    "                TrainAccAll +=TrainAcc # update training accuracy the whole epoch\n",
    "                lossTrainAll += train_loss # update training loss the whole epoch\n",
    "                #if CounterTrain%5==0:\n",
    "                if CounterTrain%1==0:\n",
    "                    print(\"lamda\",lamda)\n",
    "                    print(varout)\n",
    "                    print(\"acc Test:\", acctest/countertest)\n",
    "                    print(e,'Training acc',TrainAccAll /CounterTrain,'loss',lossTrainAll/CounterTrain,CounterTrain*50)\n",
    "                    #traced_cell = torch.jit.trace(self.model,data.double())\n",
    "                    #traced_cell.save('./atrial_Fib_model.zip')\n",
    "                    #print(\"Conerted!\")\n",
    "                \n",
    "                    error.append(lossTrainAll/CounterTrain)\n",
    "                    acctest_.append(acctest/countertest)\n",
    "                    acctrain_.append(TrainAccAll/CounterTrain)   \n",
    "        return acctest_,acctrain_\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b17fd0df-f057-472b-8330-5ff1b9d0e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=train_(1000,net,trainloader,testloader,criterion,optimizer)#with lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "07dac5f1-48a3-4fdb-b6de-9491b9cc2c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9536, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "0 Training acc 0.88 loss 0.6061690806430087 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9966, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "0 Training acc 0.9257142857142857 loss 0.6000922706854006 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9801, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "1 Training acc 0.9 loss 0.6053079071159355 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9564, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "1 Training acc 0.9214285714285715 loss 0.6002943742191222 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9877, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "2 Training acc 0.89 loss 0.6017446059875369 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9152, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "2 Training acc 0.9235714285714286 loss 0.6010716653046311 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9355, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "3 Training acc 0.92 loss 0.6024534266478601 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9951, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "3 Training acc 0.9171428571428571 loss 0.6009296115078506 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9632, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "4 Training acc 0.92 loss 0.5941427086577499 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9963, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "4 Training acc 0.9171428571428571 loss 0.6027184535713854 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9957, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "5 Training acc 0.91 loss 0.6057805064429607 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9241, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "5 Training acc 0.9192857142857143 loss 0.6002312726257489 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9537, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "6 Training acc 0.91 loss 0.6008697792724201 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9773, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "6 Training acc 0.9192857142857143 loss 0.6012880051829936 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9531, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "7 Training acc 0.94 loss 0.60054152192053 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9782, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "7 Training acc 0.9128571428571428 loss 0.6013613596786516 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9628, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "8 Training acc 0.97 loss 0.6007801859704754 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9651, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "8 Training acc 0.9064285714285714 loss 0.6013119000392868 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9473, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "9 Training acc 0.92 loss 0.5970974447274131 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9988, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "9 Training acc 0.9171428571428571 loss 0.602101511016935 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9228, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "10 Training acc 0.94 loss 0.5980961202620989 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0127, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "10 Training acc 0.9128571428571428 loss 0.6018875948521119 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9544, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "11 Training acc 0.9 loss 0.6012696065148935 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9762, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "11 Training acc 0.9214285714285715 loss 0.6012073129926754 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9680, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "12 Training acc 0.91 loss 0.6083650580279243 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9917, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "12 Training acc 0.9192857142857143 loss 0.5996855831704178 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9669, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "13 Training acc 0.91 loss 0.6029289627244332 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9640, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "13 Training acc 0.9192857142857143 loss 0.6008491563581562 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9643, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "14 Training acc 0.93 loss 0.59359816390105 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9979, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "14 Training acc 0.915 loss 0.6028474041346581 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9793, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "15 Training acc 0.92 loss 0.6009679971755064 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9346, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "15 Training acc 0.9171428571428571 loss 0.6012669594067541 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9016, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "16 Training acc 0.93 loss 0.5954296506526733 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0306, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "16 Training acc 0.915 loss 0.6024518069428118 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0074, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "17 Training acc 0.91 loss 0.59984220934848 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8076, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "17 Training acc 0.9192857142857143 loss 0.6015046058837226 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9573, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "18 Training acc 0.91 loss 0.6028029070467431 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9761, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "18 Training acc 0.9192857142857143 loss 0.6008676009358056 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9784, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "19 Training acc 0.9 loss 0.6044408710598377 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9540, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "19 Training acc 0.9214285714285715 loss 0.6005142458465975 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "20 Training acc 0.91 loss 0.6039229334838139 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9900, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "20 Training acc 0.9192857142857143 loss 0.600622861327415 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9270, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "21 Training acc 0.95 loss 0.5958049221949103 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0193, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "21 Training acc 0.9107142857142857 loss 0.6023603442123399 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9628, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "22 Training acc 0.88 loss 0.6056745340775449 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9841, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "22 Training acc 0.9257142857142857 loss 0.6002435315989096 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9733, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "23 Training acc 0.93 loss 0.5981877743653348 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9579, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "23 Training acc 0.915 loss 0.6018458638755106 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9906, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "24 Training acc 0.93 loss 0.5933402307321624 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9552, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "24 Training acc 0.915 loss 0.6028828598389269 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9782, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "25 Training acc 0.92 loss 0.6043518149340029 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "25 Training acc 0.9171428571428571 loss 0.6005214797016991 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9328, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "26 Training acc 0.93 loss 0.5990352167708094 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0023, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "26 Training acc 0.915 loss 0.6016587389396643 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9637, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "27 Training acc 0.9 loss 0.5987760926276334 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9705, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "27 Training acc 0.9214285714285715 loss 0.6017125364480156 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0112, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "28 Training acc 0.89 loss 0.6077989888427922 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8892, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "28 Training acc 0.9235714285714286 loss 0.5997773004320965 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9349, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "29 Training acc 0.92 loss 0.5979072337795408 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0056, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "29 Training acc 0.9171428571428571 loss 0.6018949255551151 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "30 Training acc 0.9 loss 0.6015879024751574 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "30 Training acc 0.9214285714285715 loss 0.6011044368690779 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "31 Training acc 0.92 loss 0.5974199975953789 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0070, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "31 Training acc 0.9171428571428571 loss 0.60199518807395 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "32 Training acc 0.92 loss 0.6006082824876215 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9887, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "32 Training acc 0.9171428571428571 loss 0.6013096561346558 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9877, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "33 Training acc 0.91 loss 0.6052375734562613 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9404, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "33 Training acc 0.9192857142857143 loss 0.6003149354573345 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9079, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "34 Training acc 0.94 loss 0.5993980842834904 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0146, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "34 Training acc 0.9128571428571428 loss 0.6015633591373943 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "35 Training acc 0.93 loss 0.5994423545708506 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9686, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "35 Training acc 0.915 loss 0.6015514509522175 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9357, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "36 Training acc 0.93 loss 0.6025708267975238 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9956, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "36 Training acc 0.915 loss 0.6008785669921489 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9829, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "37 Training acc 0.88 loss 0.6067294913256428 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9604, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "37 Training acc 0.9257142857142857 loss 0.5999848261402146 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "38 Training acc 0.93 loss 0.5982881630800617 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9717, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "38 Training acc 0.915 loss 0.6017913515803679 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9572, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "39 Training acc 0.9 loss 0.6027432710244729 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9761, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "39 Training acc 0.9214285714285715 loss 0.6008344496067749 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "40 Training acc 0.92 loss 0.5962040284994758 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0011, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "40 Training acc 0.9171428571428571 loss 0.602233599321417 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9280, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "41 Training acc 0.91 loss 0.5952805344145018 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0207, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "41 Training acc 0.9192857142857143 loss 0.6024296121994469 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9352, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "42 Training acc 0.91 loss 0.6022735656756346 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9949, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "42 Training acc 0.9192857142857143 loss 0.6009291170895656 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9253, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "43 Training acc 0.9 loss 0.6033382138531076 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0052, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "43 Training acc 0.9214285714285715 loss 0.6006987975261785 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9783, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "44 Training acc 0.92 loss 0.6087793009520154 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "44 Training acc 0.9171428571428571 loss 0.59953078624345 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9476, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "45 Training acc 0.93 loss 0.5969707797168878 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9989, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "45 Training acc 0.915 loss 0.6020591488344202 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9470, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "46 Training acc 0.94 loss 0.597291845929625 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9979, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "46 Training acc 0.9128571428571428 loss 0.601988529831137 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9853, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "47 Training acc 0.89 loss 0.6079622439838085 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9639, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "47 Training acc 0.9235714285714286 loss 0.5997000814088671 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9729, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "48 Training acc 0.93 loss 0.5983931086424328 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9571, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "48 Training acc 0.915 loss 0.6017486946917086 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9627, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "49 Training acc 0.93 loss 0.5993006044973299 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9688, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "49 Training acc 0.915 loss 0.6015525416695195 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9874, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "50 Training acc 0.94 loss 0.5949566181007162 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9506, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "50 Training acc 0.9128571428571428 loss 0.6024816185635897 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9936, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "51 Training acc 0.91 loss 0.6047084723770418 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9216, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "51 Training acc 0.9192857142857143 loss 0.6003900282403472 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "52 Training acc 0.92 loss 0.600984473536476 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9874, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "52 Training acc 0.9171428571428571 loss 0.601185893323444 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9566, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "53 Training acc 0.91 loss 0.602426901794941 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9754, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "53 Training acc 0.9192857142857143 loss 0.600874182651602 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9637, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "54 Training acc 0.94 loss 0.6012788595515379 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9634, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "54 Training acc 0.9128571428571428 loss 0.6011174496081246 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9219, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "55 Training acc 0.92 loss 0.5985973983748307 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0109, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "55 Training acc 0.9171428571428571 loss 0.6016893095021508 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9573, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "56 Training acc 0.92 loss 0.5972172606646857 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9872, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "56 Training acc 0.9171428571428571 loss 0.601982279361742 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0207, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "57 Training acc 0.89 loss 0.6101891781642285 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8727, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "57 Training acc 0.9235714285714286 loss 0.5992000311484406 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9333, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "58 Training acc 0.93 loss 0.6012650097200377 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9957, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "58 Training acc 0.915 loss 0.6011094512250295 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0064, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "59 Training acc 0.9 loss 0.6054280263234152 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8834, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "59 Training acc 0.9214285714285715 loss 0.6002143311338438 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "60 Training acc 0.91 loss 0.6000835635426472 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9990, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "60 Training acc 0.9192857142857143 loss 0.6013570207017 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9653, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "61 Training acc 0.92 loss 0.5979423287521644 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9725, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "61 Training acc 0.9171428571428571 loss 0.60181813019372 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9588, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "62 Training acc 0.88 loss 0.6089359897005023 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0047, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "62 Training acc 0.9257142857142857 loss 0.5994631282122109 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9322, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "63 Training acc 0.92 loss 0.5930092841345241 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0272, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "63 Training acc 0.9171428571428571 loss 0.602876225353415 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9799, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "64 Training acc 0.92 loss 0.6052512177025612 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9565, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "64 Training acc 0.9171428571428571 loss 0.600253325025148 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9802, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "65 Training acc 0.89 loss 0.6053620405500637 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9568, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "65 Training acc 0.9235714285714286 loss 0.6002289663976148 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0128, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "66 Training acc 0.89 loss 0.6045342756208956 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8344, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "66 Training acc 0.9235714285714286 loss 0.6004060473051878 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9215, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "67 Training acc 0.96 loss 0.5919330489232902 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "67 Training acc 0.9085714285714286 loss 0.6031048932282579 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9936, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "68 Training acc 0.91 loss 0.5983915834131257 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9043, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "68 Training acc 0.9192857142857143 loss 0.6017206373706666 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9899, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "69 Training acc 0.93 loss 0.6028444202468835 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9168, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "69 Training acc 0.915 loss 0.6007651588400136 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9593, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "70 Training acc 0.92 loss 0.6038462524044459 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9794, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "70 Training acc 0.9171428571428571 loss 0.6005487273055434 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0123, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "71 Training acc 0.89 loss 0.6083074035784368 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8908, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "71 Training acc 0.9235714285714286 loss 0.5995912238821183 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9531, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "72 Training acc 0.93 loss 0.5941057171774267 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0070, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "72 Training acc 0.915 loss 0.6026327195862069 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9206, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "73 Training acc 0.92 loss 0.6007172085644085 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0047, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "73 Training acc 0.9171428571428571 loss 0.6012144742591852 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9763, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "74 Training acc 0.94 loss 0.5966497528374284 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9618, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "74 Training acc 0.9128571428571428 loss 0.6020838594366842 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9545, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "75 Training acc 0.92 loss 0.6013319362149364 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9756, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "75 Training acc 0.9171428571428571 loss 0.601078185141132 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "76 Training acc 0.93 loss 0.5986383268502816 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9939, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "76 Training acc 0.915 loss 0.6016523693806997 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9604, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "77 Training acc 0.93 loss 0.6044201257371832 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "77 Training acc 0.915 loss 0.6004106175971509 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9859, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "78 Training acc 0.89 loss 0.6043486598405384 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "78 Training acc 0.9235714285714286 loss 0.6004229249480495 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9956, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "79 Training acc 0.92 loss 0.6092202177856347 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9515, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "79 Training acc 0.9171428571428571 loss 0.5993761562872354 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9667, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "80 Training acc 0.93 loss 0.5972048151648086 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9745, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "80 Training acc 0.915 loss 0.6019480867470008 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9701, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "81 Training acc 0.92 loss 0.6046250814187972 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9691, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "81 Training acc 0.9171428571428571 loss 0.6003557476927766 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9677, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "82 Training acc 0.92 loss 0.6000370028996563 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9666, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "82 Training acc 0.9171428571428571 loss 0.6013511008135299 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9532, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "83 Training acc 0.91 loss 0.5999379178934928 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9797, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "83 Training acc 0.9192857142857143 loss 0.6014493216586245 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9783, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "84 Training acc 0.91 loss 0.5997068557702967 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9380, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "84 Training acc 0.9192857142857143 loss 0.6015544788438669 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9586, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "85 Training acc 0.94 loss 0.5965636363984962 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9896, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "85 Training acc 0.9128571428571428 loss 0.602277651361228 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9455, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "86 Training acc 0.91 loss 0.6019614794526981 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9856, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "86 Training acc 0.9192857142857143 loss 0.6011573885765653 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9829, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "87 Training acc 0.93 loss 0.6028071278922906 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9333, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "87 Training acc 0.915 loss 0.6010065774998239 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0139, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "88 Training acc 0.88 loss 0.606881145488809 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8635, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "88 Training acc 0.9257142857142857 loss 0.6001569991508451 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9635, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "89 Training acc 0.95 loss 0.5988731160409718 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9710, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "89 Training acc 0.9107142857142857 loss 0.6018907438715806 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9347, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "90 Training acc 0.91 loss 0.6020464354149587 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9948, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "90 Training acc 0.9192857142857143 loss 0.6012262113219478 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9642, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "91 Training acc 0.94 loss 0.5984886307754976 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9722, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "91 Training acc 0.9128571428571428 loss 0.601999458930527 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9852, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "92 Training acc 0.93 loss 0.6039610875565625 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9360, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "92 Training acc 0.915 loss 0.6008357359974099 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9092, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "93 Training acc 0.93 loss 0.5986743945999037 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0177, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "93 Training acc 0.915 loss 0.6019750470013275 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9788, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "94 Training acc 0.9 loss 0.6006706027234087 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9362, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "94 Training acc 0.9214285714285715 loss 0.6015525555623467 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9724, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "95 Training acc 0.91 loss 0.6057646790344952 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9709, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "95 Training acc 0.9192857142857143 loss 0.6004639064898105 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9825, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "96 Training acc 0.92 loss 0.5974350388384505 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9450, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "96 Training acc 0.9171428571428571 loss 0.6022507563114239 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9835, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "97 Training acc 0.92 loss 0.6070369415240578 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9602, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "97 Training acc 0.9171428571428571 loss 0.600194639893151 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "98 Training acc 0.92 loss 0.6039357860571486 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9983, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "98 Training acc 0.9171428571428571 loss 0.6008596694943664 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "99 Training acc 0.92 loss 0.598176932022663 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9966, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "99 Training acc 0.9171428571428571 loss 0.602093885748429 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9730, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "100 Training acc 0.93 loss 0.6016982702307685 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "100 Training acc 0.915 loss 0.6013392238851467 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9583, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "101 Training acc 0.91 loss 0.5966984817654607 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9899, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "101 Training acc 0.9192857142857143 loss 0.6024095223630644 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9882, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "102 Training acc 0.9 loss 0.6055094732659373 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "102 Training acc 0.9214285714285715 loss 0.6005200793103473 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8705, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "103 Training acc 0.92 loss 0.5964237016464279 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "103 Training acc 0.9171428571428571 loss 0.6024653888590183 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9827, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "104 Training acc 0.91 loss 0.6066318676755423 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9591, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "104 Training acc 0.9192857142857143 loss 0.6002763978138927 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9569, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "105 Training acc 0.92 loss 0.6026097546541956 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9749, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "105 Training acc 0.9171428571428571 loss 0.6011362536013363 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9680, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "106 Training acc 0.9 loss 0.6034907167413185 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "106 Training acc 0.9214285714285715 loss 0.6009454471214282 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9288, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "107 Training acc 0.94 loss 0.5948168245039753 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0232, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "107 Training acc 0.9128571428571428 loss 0.6028019549097536 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9906, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "108 Training acc 0.88 loss 0.6032062502810043 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9166, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "108 Training acc 0.9257142857142857 loss 0.6010024480099937 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "109 Training acc 0.9 loss 0.603898848491331 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "109 Training acc 0.9214285714285715 loss 0.6008516688438841 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "110 Training acc 0.94 loss 0.6007987955786812 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9983, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "110 Training acc 0.9128571428571428 loss 0.601513556443591 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9750, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "111 Training acc 0.91 loss 0.6070828659153995 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9746, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "111 Training acc 0.9192857142857143 loss 0.6001642992823284 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9848, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "112 Training acc 0.91 loss 0.6077051399464893 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "112 Training acc 0.9192857142857143 loss 0.6000278994016408 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9623, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "113 Training acc 0.91 loss 0.5946246814400628 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9957, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "113 Training acc 0.9192857142857143 loss 0.6028284692326851 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9430, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "114 Training acc 0.91 loss 0.6005735654380878 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9897, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "114 Training acc 0.9192857142857143 loss 0.6015513635474242 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9146, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "115 Training acc 0.91 loss 0.6043604875651746 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0129, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "115 Training acc 0.9192857142857143 loss 0.600736797324372 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9617, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "116 Training acc 0.9 loss 0.6051038842095962 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "116 Training acc 0.9214285714285715 loss 0.600574623223985 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9919, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "117 Training acc 0.92 loss 0.6073761418415164 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "117 Training acc 0.9171428571428571 loss 0.6000847669329534 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9535, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "118 Training acc 0.91 loss 0.5992193582557248 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9826, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "118 Training acc 0.9192857142857143 loss 0.6018299858238525 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0054, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "119 Training acc 0.89 loss 0.6049282362754668 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8811, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "119 Training acc 0.9235714285714286 loss 0.6006042656727161 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9317, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "120 Training acc 0.91 loss 0.5997751888656347 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0011, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "120 Training acc 0.9192857142857143 loss 0.6017057262090401 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "121 Training acc 0.93 loss 0.5982539550755533 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9727, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "121 Training acc 0.915 loss 0.6020291238427761 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9677, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "122 Training acc 0.91 loss 0.6033353645712578 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9644, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "122 Training acc 0.9192857142857143 loss 0.6009376139265931 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9719, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "123 Training acc 0.92 loss 0.6010895353348258 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9508, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "123 Training acc 0.9171428571428571 loss 0.601416010398585 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9692, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "124 Training acc 0.92 loss 0.5958785197433031 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9794, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "124 Training acc 0.9171428571428571 loss 0.6025292984357344 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9264, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "125 Training acc 0.93 loss 0.5961283253159432 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0191, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "125 Training acc 0.915 loss 0.6024727845320601 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9081, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "126 Training acc 0.93 loss 0.5992848769463026 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0158, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "126 Training acc 0.915 loss 0.6017934803817111 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "127 Training acc 0.91 loss 0.6038239048539025 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9891, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "127 Training acc 0.9192857142857143 loss 0.6008179979550439 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9467, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "128 Training acc 0.91 loss 0.5974641701919954 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9984, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "128 Training acc 0.9192857142857143 loss 0.6021778162848264 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9567, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "129 Training acc 0.95 loss 0.5975232923794151 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9873, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "129 Training acc 0.9107142857142857 loss 0.6021625969350237 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "130 Training acc 0.91 loss 0.5997160082278972 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9811, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "130 Training acc 0.9192857142857143 loss 0.6016902239932007 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9811, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "131 Training acc 0.91 loss 0.5999840418224166 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9378, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "131 Training acc 0.9192857142857143 loss 0.6016259020758412 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9801, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "132 Training acc 0.89 loss 0.6013488467984213 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9341, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "132 Training acc 0.9235714285714286 loss 0.6013007103240071 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9440, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "133 Training acc 0.92 loss 0.6010996171246985 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9878, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "133 Training acc 0.9171428571428571 loss 0.601327207060605 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9657, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "134 Training acc 0.93 loss 0.6023324248405667 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9624, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "134 Training acc 0.915 loss 0.6010408406807561 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9210, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "135 Training acc 0.93 loss 0.5990754236297903 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0100, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "135 Training acc 0.915 loss 0.601718915865006 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9369, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "136 Training acc 0.92 loss 0.6031855691601669 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9969, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "136 Training acc 0.9171428571428571 loss 0.6008226022084809 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9885, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "137 Training acc 0.92 loss 0.602151840781596 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9153, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "137 Training acc 0.9171428571428571 loss 0.6010308337490713 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9323, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "138 Training acc 0.91 loss 0.6007203827493663 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9977, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "138 Training acc 0.9192857142857143 loss 0.6013242280214521 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9639, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "139 Training acc 0.9 loss 0.6013873061565328 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9634, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "139 Training acc 0.9214285714285715 loss 0.6011692797207389 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9771, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "140 Training acc 0.94 loss 0.5918583218597069 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9901, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "140 Training acc 0.9128571428571428 loss 0.6032005548282443 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0147, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "141 Training acc 0.87 loss 0.6151187303626843 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "141 Training acc 0.9278571428571429 loss 0.5982066064762952 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9894, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "142 Training acc 0.88 loss 0.6060889846124315 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "142 Training acc 0.9257142857142857 loss 0.6001334369817468 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9806, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "143 Training acc 0.89 loss 0.6055631902880244 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9572, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "143 Training acc 0.9235714285714286 loss 0.6002391008882443 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9786, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "144 Training acc 0.93 loss 0.5994684793274765 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9384, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "144 Training acc 0.915 loss 0.6015392349311424 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "145 Training acc 0.94 loss 0.5952322381287783 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0132, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "145 Training acc 0.9128571428571428 loss 0.6024418657087742 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0023, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "146 Training acc 0.86 loss 0.6125341809237599 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9605, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "146 Training acc 0.9299999999999999 loss 0.5987294967779885 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9747, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "147 Training acc 0.96 loss 0.5974425964342518 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9598, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "147 Training acc 0.9085714285714286 loss 0.6019591189715464 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9561, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "148 Training acc 0.9 loss 0.6022045841799928 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9749, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "148 Training acc 0.9214285714285715 loss 0.6009349078405515 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9698, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "149 Training acc 0.9 loss 0.6044278118624824 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9683, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "149 Training acc 0.9214285714285715 loss 0.6004544113692503 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9890, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "150 Training acc 0.92 loss 0.6023873601551484 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9156, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "150 Training acc 0.9171428571428571 loss 0.6008880142491271 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0113, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "151 Training acc 0.9 loss 0.610439626104742 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9174, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "151 Training acc 0.9214285714285715 loss 0.5991590667389248 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9561, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "152 Training acc 0.91 loss 0.5925375743364809 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0117, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "152 Training acc 0.9192857142857143 loss 0.6029920295143152 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9904, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "153 Training acc 0.91 loss 0.6031252085894628 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9175, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "153 Training acc 0.9192857142857143 loss 0.6007204222509832 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9203, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "154 Training acc 0.89 loss 0.6005285144609608 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0053, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "154 Training acc 0.9235714285714286 loss 0.6012736727433213 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9314, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "155 Training acc 0.93 loss 0.5934470366186941 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0260, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "155 Training acc 0.915 loss 0.6027877851537595 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9526, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "156 Training acc 0.91 loss 0.605708183429907 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9954, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "156 Training acc 0.9192857142857143 loss 0.6001570987696858 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9655, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "157 Training acc 0.92 loss 0.597834417743156 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9728, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "157 Training acc 0.9171428571428571 loss 0.6018412789554721 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9559, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "158 Training acc 0.93 loss 0.5979765534958432 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "158 Training acc 0.915 loss 0.6018080412896354 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9665, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "159 Training acc 0.96 loss 0.5924357007773544 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0008, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "159 Training acc 0.9085714285714286 loss 0.6029927112534794 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8943, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "160 Training acc 0.91 loss 0.6001410374859162 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0163, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "160 Training acc 0.9192857142857143 loss 0.6013390180931708 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9508, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "161 Training acc 0.93 loss 0.5953034253168114 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0035, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "161 Training acc 0.915 loss 0.6023736572161315 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9608, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "162 Training acc 0.95 loss 0.6046133135033792 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9815, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "162 Training acc 0.9107142857142857 loss 0.6003764157333618 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9931, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "163 Training acc 0.92 loss 0.6044843819495103 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9211, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "163 Training acc 0.9171428571428571 loss 0.6004017562923886 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9880, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "164 Training acc 0.91 loss 0.6054020653777497 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "164 Training acc 0.9192857142857143 loss 0.6002025904408026 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9560, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "165 Training acc 0.92 loss 0.602123028841638 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "165 Training acc 0.9171428571428571 loss 0.6009022938120885 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9304, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "166 Training acc 0.94 loss 0.5940053127866043 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0242, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "166 Training acc 0.9128571428571428 loss 0.6026393586708423 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9247, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "167 Training acc 0.93 loss 0.6029746939134879 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0044, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "167 Training acc 0.915 loss 0.600714897937128 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9559, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "168 Training acc 0.93 loss 0.6020773494707271 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "168 Training acc 0.915 loss 0.6009046312931846 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0115, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "169 Training acc 0.89 loss 0.6079313938864358 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8899, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "169 Training acc 0.9235714285714286 loss 0.5996472614392756 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9604, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "170 Training acc 0.92 loss 0.5956003284224741 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9916, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "170 Training acc 0.9171428571428571 loss 0.6022869517032406 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9750, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "171 Training acc 0.94 loss 0.6027259256122346 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "171 Training acc 0.9128571428571428 loss 0.6007574607391589 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9825, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "172 Training acc 0.92 loss 0.6026358568181444 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9338, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "172 Training acc 0.9171428571428571 loss 0.6007741596053802 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0034, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "173 Training acc 0.89 loss 0.6096234185924353 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9349, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "173 Training acc 0.9235714285714286 loss 0.5992742362867189 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9526, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "174 Training acc 0.9 loss 0.6056642679544163 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9955, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "174 Training acc 0.9214285714285715 loss 0.6001199512422171 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9134, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "175 Training acc 0.94 loss 0.5963193298798927 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0232, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "175 Training acc 0.9128571428571428 loss 0.6021202168763475 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "176 Training acc 0.91 loss 0.6063042511070121 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9973, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "176 Training acc 0.9192857142857143 loss 0.5999784146814584 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0009, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "177 Training acc 0.9 loss 0.6053302250033298 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9042, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "177 Training acc 0.9214285714285715 loss 0.6001847838602272 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0052, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "178 Training acc 0.89 loss 0.6074375034851072 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9097, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "178 Training acc 0.9235714285714286 loss 0.5997304924400886 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9384, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "179 Training acc 0.92 loss 0.5960157277491649 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0105, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "179 Training acc 0.9171428571428571 loss 0.6021752926240131 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9799, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "180 Training acc 0.93 loss 0.601247839394658 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9333, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "180 Training acc 0.915 loss 0.6010518734596831 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9274, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "181 Training acc 0.9 loss 0.6044298436207359 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0087, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "181 Training acc 0.9214285714285715 loss 0.6003667371166197 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0003, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "182 Training acc 0.89 loss 0.608077375878434 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "182 Training acc 0.9235714285714286 loss 0.5995813079137138 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9597, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "183 Training acc 0.92 loss 0.5959750471462906 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9904, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "183 Training acc 0.9171428571428571 loss 0.6021717383987548 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0176, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "184 Training acc 0.92 loss 0.6086757960163166 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8691, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "184 Training acc 0.9171428571428571 loss 0.5994474531717384 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0053, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "185 Training acc 0.91 loss 0.6105622056241251 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9376, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "185 Training acc 0.9192857142857143 loss 0.599039946890517 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0082, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "186 Training acc 0.93 loss 0.6022741914157285 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8294, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "186 Training acc 0.915 loss 0.6008131299798825 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9789, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "187 Training acc 0.92 loss 0.6047112314932214 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9553, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "187 Training acc 0.9171428571428571 loss 0.6002879055573288 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "188 Training acc 0.94 loss 0.5996998920406613 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9798, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "188 Training acc 0.9128571428571428 loss 0.6013586260117447 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9541, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "189 Training acc 0.92 loss 0.5989123090645526 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9820, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "189 Training acc 0.9171428571428571 loss 0.6015246399427405 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9995, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "190 Training acc 0.93 loss 0.6046001304584518 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9025, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "190 Training acc 0.915 loss 0.6003030163176066 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8798, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "191 Training acc 0.94 loss 0.5911169324125582 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0462, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "191 Training acc 0.9128571428571428 loss 0.603188839007506 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "192 Training acc 0.93 loss 0.5997743486889587 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9796, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "192 Training acc 0.915 loss 0.6013311566017014 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9093, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "193 Training acc 0.9 loss 0.5985745662195161 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0163, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "193 Training acc 0.9214285714285715 loss 0.6015850674037164 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "194 Training acc 0.94 loss 0.6034291951341806 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9985, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "194 Training acc 0.9128571428571428 loss 0.6005417317591574 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "195 Training acc 0.91 loss 0.5962092838441049 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0098, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "195 Training acc 0.9192857142857143 loss 0.6020856946578986 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9263, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "196 Training acc 0.95 loss 0.5961856242396449 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0174, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "196 Training acc 0.9107142857142857 loss 0.6020880359651535 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9546, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "197 Training acc 0.88 loss 0.6067160513930712 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9988, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "197 Training acc 0.9257142857142857 loss 0.5998287256008423 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "198 Training acc 0.94 loss 0.5978076320712983 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9423, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "198 Training acc 0.9128571428571428 loss 0.6017347821436265 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9859, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "199 Training acc 0.88 loss 0.6007947611514902 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9169, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "199 Training acc 0.9257142857142857 loss 0.601091915583265 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9818, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "200 Training acc 0.91 loss 0.5977666110372815 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9424, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "200 Training acc 0.9192857142857143 loss 0.6017371894503665 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "201 Training acc 0.96 loss 0.5996975226590591 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9672, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "201 Training acc 0.9085714285714286 loss 0.6013200401485375 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9872, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "202 Training acc 0.9 loss 0.6089291550404474 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9672, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "202 Training acc 0.9214285714285715 loss 0.5993381899924708 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9249, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "203 Training acc 0.93 loss 0.596937403454266 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0151, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "203 Training acc 0.915 loss 0.6019042344393919 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9448, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "204 Training acc 0.94 loss 0.5926262859118293 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0202, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "204 Training acc 0.9128571428571428 loss 0.6028248549526241 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9943, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "205 Training acc 0.9 loss 0.6050564559332497 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9231, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "205 Training acc 0.9214285714285715 loss 0.6001579226362754 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9670, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "206 Training acc 0.91 loss 0.6029951317494571 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9650, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "206 Training acc 0.9192857142857143 loss 0.6005964728650143 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9327, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "207 Training acc 0.93 loss 0.5990833458176471 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0013, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "207 Training acc 0.915 loss 0.6014316402097385 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9454, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "208 Training acc 0.94 loss 0.5923391198207953 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0210, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "208 Training acc 0.9128571428571428 loss 0.6028741363180317 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0130, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "209 Training acc 0.91 loss 0.6086375604588204 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8921, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "209 Training acc 0.9192857142857143 loss 0.5993789723400329 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9069, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "210 Training acc 0.93 loss 0.5924968509750828 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "210 Training acc 0.915 loss 0.6028345287929506 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "211 Training acc 0.93 loss 0.5900603423840025 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0277, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "211 Training acc 0.915 loss 0.6033540456915125 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9816, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "212 Training acc 0.93 loss 0.5978749853251492 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "212 Training acc 0.915 loss 0.6016772491043865 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0055, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "213 Training acc 0.91 loss 0.6106336239042434 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "213 Training acc 0.9192857142857143 loss 0.5989405085960797 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9589, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "214 Training acc 0.94 loss 0.5964059026888591 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9888, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "214 Training acc 0.9128571428571428 loss 0.6019861651099663 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9574, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "215 Training acc 0.9 loss 0.5971857359783431 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9866, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "215 Training acc 0.9214285714285715 loss 0.6018165200221431 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9531, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "216 Training acc 0.91 loss 0.6005354061799668 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9772, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "216 Training acc 0.9192857142857143 loss 0.601095919737838 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9828, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "217 Training acc 0.92 loss 0.6027796341181726 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9347, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "217 Training acc 0.9171428571428571 loss 0.6006112744242031 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "218 Training acc 0.9 loss 0.6001671747557985 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9657, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "218 Training acc 0.9214285714285715 loss 0.601167324644831 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9583, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "219 Training acc 0.93 loss 0.5966977875611893 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9879, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "219 Training acc 0.915 loss 0.6019076547225657 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "220 Training acc 0.91 loss 0.6002854354145722 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9512, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "220 Training acc 0.9192857142857143 loss 0.6011355818582653 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9037, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "221 Training acc 0.93 loss 0.5942637965038072 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0329, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "221 Training acc 0.915 loss 0.6024220795571968 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9451, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "222 Training acc 0.92 loss 0.6017507450212466 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9856, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "222 Training acc 0.9171428571428571 loss 0.600814034709569 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "223 Training acc 0.9 loss 0.6048346009039197 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9703, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "223 Training acc 0.9214285714285715 loss 0.6001483527129161 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0048, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "224 Training acc 0.9 loss 0.6103131233614538 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9374, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "224 Training acc 0.9214285714285715 loss 0.5989695499422364 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8971, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "225 Training acc 0.91 loss 0.5979508585030667 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0219, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "225 Training acc 0.9192857142857143 loss 0.6016140914425573 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9818, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "226 Training acc 0.93 loss 0.6061924806395816 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9599, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "226 Training acc 0.915 loss 0.5998444959604023 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9549, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "227 Training acc 0.93 loss 0.6015496644584836 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9750, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "227 Training acc 0.915 loss 0.6008349693897028 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9939, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "228 Training acc 0.92 loss 0.6048616876181167 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9228, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "228 Training acc 0.9171428571428571 loss 0.6001227257099301 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9984, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "229 Training acc 0.91 loss 0.6071333789441772 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9289, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "229 Training acc 0.9192857142857143 loss 0.59963332805562 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9863, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "230 Training acc 0.92 loss 0.6084559266203574 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9662, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "230 Training acc 0.9171428571428571 loss 0.5993473590183567 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9382, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "231 Training acc 0.91 loss 0.5961299042973656 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0095, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "231 Training acc 0.9192857142857143 loss 0.601986478014827 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9590, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "232 Training acc 0.91 loss 0.6036778900801385 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9797, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "232 Training acc 0.9192857142857143 loss 0.6003668317369732 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9813, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "233 Training acc 0.92 loss 0.6019756231675245 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9328, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "233 Training acc 0.9171428571428571 loss 0.6007291952245626 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9806, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "234 Training acc 0.91 loss 0.601655033940551 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "234 Training acc 0.9192857142857143 loss 0.6007952376990877 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "235 Training acc 0.92 loss 0.6003013951465159 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9776, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "235 Training acc 0.9171428571428571 loss 0.6010796712440976 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9653, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "236 Training acc 0.88 loss 0.6021212753614943 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9631, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "236 Training acc 0.9257142857142857 loss 0.6006829967851193 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9481, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "237 Training acc 0.94 loss 0.5967141331773451 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9985, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "237 Training acc 0.9128571428571428 loss 0.6018355784591706 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9280, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "238 Training acc 0.92 loss 0.5952842937071742 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0195, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "238 Training acc 0.9171428571428571 loss 0.6021367133214844 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9711, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "239 Training acc 0.9 loss 0.6000929863119765 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9514, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "239 Training acc 0.9214285714285715 loss 0.6011021235422844 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9804, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "240 Training acc 0.93 loss 0.5945432263034153 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9667, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "240 Training acc 0.915 loss 0.602290509573987 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9426, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "241 Training acc 0.92 loss 0.5997818606947386 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9897, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "241 Training acc 0.9171428571428571 loss 0.6011664460199385 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9938, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "242 Training acc 0.92 loss 0.6083078058078735 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "242 Training acc 0.9171428571428571 loss 0.5993372254188942 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9723, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "243 Training acc 0.9 loss 0.6013457041533523 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "243 Training acc 0.9214285714285715 loss 0.6008261142994531 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9307, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "244 Training acc 0.94 loss 0.5938013110160903 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0238, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "244 Training acc 0.9128571428571428 loss 0.6024397281294275 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9318, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "245 Training acc 0.98 loss 0.5932362872530347 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0254, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "245 Training acc 0.9042857142857144 loss 0.6025576977975661 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "246 Training acc 0.92 loss 0.6009735771094957 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9864, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "246 Training acc 0.9171428571428571 loss 0.6008966712538785 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9878, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "247 Training acc 0.92 loss 0.6053228080972689 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "247 Training acc 0.9171428571428571 loss 0.5999604431631279 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9751, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "248 Training acc 0.93 loss 0.5928740936423398 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9857, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "248 Training acc 0.915 loss 0.6026239652074921 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9827, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "249 Training acc 0.94 loss 0.6026989157530485 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9349, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "249 Training acc 0.9128571428571428 loss 0.6005150210750114 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9556, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "250 Training acc 0.89 loss 0.6019039986909381 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9752, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "250 Training acc 0.9235714285714286 loss 0.6006813744108143 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9851, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "251 Training acc 0.89 loss 0.6078409075796978 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9648, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "251 Training acc 0.9235714285714286 loss 0.5994054011150762 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9790, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "252 Training acc 0.9 loss 0.6047904670055868 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9564, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "252 Training acc 0.9214285714285715 loss 0.6000552889637962 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9162, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "253 Training acc 0.94 loss 0.5948094924844343 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0266, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "253 Training acc 0.9128571428571428 loss 0.6021906866247674 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9918, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "254 Training acc 0.93 loss 0.6038070040031289 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9204, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "254 Training acc 0.915 loss 0.6002597538983334 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "255 Training acc 0.9 loss 0.6011045670159828 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9951, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "255 Training acc 0.9214285714285715 loss 0.6008352368410461 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9812, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "256 Training acc 0.95 loss 0.601935098009192 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "256 Training acc 0.9107142857142857 loss 0.6006534714181357 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9076, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "257 Training acc 0.92 loss 0.6003774165417768 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0103, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "257 Training acc 0.9171428571428571 loss 0.6009826929946354 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9200, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "258 Training acc 0.92 loss 0.6003001516673362 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0047, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "258 Training acc 0.9171428571428571 loss 0.6009946480219854 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0042, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "259 Training acc 0.93 loss 0.6020791030387224 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "259 Training acc 0.915 loss 0.6006085836067946 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9828, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "260 Training acc 0.92 loss 0.6027804990377391 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9353, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "260 Training acc 0.9171428571428571 loss 0.6004532516592933 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9091, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "261 Training acc 0.91 loss 0.6013389205297827 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0080, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "261 Training acc 0.9192857142857143 loss 0.6007570074270554 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "262 Training acc 0.91 loss 0.5933773986875324 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0171, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "262 Training acc 0.9192857142857143 loss 0.6024592691834125 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9859, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "263 Training acc 0.91 loss 0.6007752073258081 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9161, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "263 Training acc 0.9192857142857143 loss 0.6008703511690119 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9207, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "264 Training acc 0.92 loss 0.5999503471835769 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0055, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "264 Training acc 0.9171428571428571 loss 0.6010419297098516 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "265 Training acc 0.93 loss 0.6003799179019643 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9877, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "265 Training acc 0.915 loss 0.6009392952491586 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9988, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "266 Training acc 0.88 loss 0.607309403889689 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9299, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "266 Training acc 0.9257142857142857 loss 0.5994444593419694 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9336, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "267 Training acc 0.93 loss 0.6014070863308248 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9980, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "267 Training acc 0.915 loss 0.6007009386299993 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9583, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "268 Training acc 0.91 loss 0.6033146619901187 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9795, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "268 Training acc 0.9192857142857143 loss 0.6002518493343996 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9976, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "269 Training acc 0.91 loss 0.6036628089985686 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9014, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "269 Training acc 0.9192857142857143 loss 0.6001342322136316 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9546, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "270 Training acc 0.9 loss 0.6013841677218127 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "270 Training acc 0.9214285714285715 loss 0.6005842557792654 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "271 Training acc 0.89 loss 0.601421756437643 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0147, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "271 Training acc 0.9235714285714286 loss 0.600544460416562 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9571, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "272 Training acc 0.91 loss 0.6027012132137716 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9783, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "272 Training acc 0.9192857142857143 loss 0.6002456990991352 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9762, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "273 Training acc 0.94 loss 0.6033659671569143 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9536, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "273 Training acc 0.9128571428571428 loss 0.6000815483114049 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9091, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "274 Training acc 0.93 loss 0.5987097365909351 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0139, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "274 Training acc 0.915 loss 0.6010613818608148 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "275 Training acc 0.95 loss 0.58644981782509 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0548, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "275 Training acc 0.9107142857142857 loss 0.6036734603442859 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "276 Training acc 0.92 loss 0.5961619228602476 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9986, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "276 Training acc 0.9171428571428571 loss 0.6015798852367737 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0134, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "277 Training acc 0.88 loss 0.608854078496566 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8944, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "277 Training acc 0.9257142857142857 loss 0.5988497595022088 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9708, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "278 Training acc 0.93 loss 0.5950844102322501 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9780, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "278 Training acc 0.915 loss 0.6017906279541168 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9219, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "279 Training acc 0.93 loss 0.5985784008888099 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0082, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "279 Training acc 0.915 loss 0.6010337235113858 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "280 Training acc 0.92 loss 0.5955563598599646 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0094, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "280 Training acc 0.9171428571428571 loss 0.6016740988391229 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9897, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "281 Training acc 0.89 loss 0.6062732867273011 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9459, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "281 Training acc 0.9235714285714286 loss 0.5993714426706042 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9663, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "282 Training acc 0.91 loss 0.6026380054394415 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9661, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "282 Training acc 0.9192857142857143 loss 0.6001442539548275 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0064, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "283 Training acc 0.91 loss 0.60805844731373 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9137, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "283 Training acc 0.9192857142857143 loss 0.5989774053785656 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "284 Training acc 0.94 loss 0.5998350715446101 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "284 Training acc 0.9128571428571428 loss 0.6007344655089495 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9213, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "285 Training acc 0.9 loss 0.598922228593663 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0071, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "285 Training acc 0.9214285714285715 loss 0.6009251106772946 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9691, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "286 Training acc 0.91 loss 0.6000399030407161 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9642, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "286 Training acc 0.9192857142857143 loss 0.6007151267638228 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9941, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "287 Training acc 0.88 loss 0.6084740208530813 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9512, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "287 Training acc 0.9257142857142857 loss 0.599079921920532 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "288 Training acc 0.89 loss 0.5993572652346985 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9906, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "288 Training acc 0.9235714285714286 loss 0.601172743284616 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9727, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "289 Training acc 0.91 loss 0.6107495378401198 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9999, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "289 Training acc 0.9192857142857143 loss 0.5988485913555697 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9815, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "290 Training acc 0.91 loss 0.602105508057397 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9329, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "290 Training acc 0.9192857142857143 loss 0.6007866920095581 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9396, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "291 Training acc 0.91 loss 0.5953812068350401 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0123, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "291 Training acc 0.9192857142857143 loss 0.6023009880921706 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9880, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "292 Training acc 0.92 loss 0.6054007221450263 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9411, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "292 Training acc 0.9171428571428571 loss 0.600213508813028 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9894, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "293 Training acc 0.89 loss 0.6100178385520928 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9696, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "293 Training acc 0.9235714285714286 loss 0.599270410971525 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9339, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "294 Training acc 0.95 loss 0.6015736965030122 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9952, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "294 Training acc 0.9107142857142857 loss 0.6011156690512989 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9814, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "295 Training acc 0.9 loss 0.6059800475444154 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9581, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "295 Training acc 0.9214285714285715 loss 0.6002007685052996 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9825, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "296 Training acc 0.93 loss 0.593499391213779 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9711, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "296 Training acc 0.915 loss 0.6028983752895365 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9562, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "297 Training acc 0.95 loss 0.5924749905962486 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0124, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "297 Training acc 0.9107142857142857 loss 0.6031355798088015 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8741, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "298 Training acc 0.91 loss 0.5943687954506546 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0374, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "298 Training acc 0.9192857142857143 loss 0.6027433792183856 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9776, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "299 Training acc 0.9 loss 0.6084434261535651 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9790, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "299 Training acc 0.9214285714285715 loss 0.599738271620238 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9682, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "300 Training acc 0.9 loss 0.6036271896233101 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9656, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "300 Training acc 0.9214285714285715 loss 0.6007779429886229 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9503, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "301 Training acc 0.94 loss 0.5955697066289145 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0035, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "301 Training acc 0.9128571428571428 loss 0.6025101501540971 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9937, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "302 Training acc 0.91 loss 0.6047664797756532 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9211, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "302 Training acc 0.9192857142857143 loss 0.6005438849633005 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0009, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "303 Training acc 0.93 loss 0.6052963658198838 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9033, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "303 Training acc 0.915 loss 0.6004328955405259 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9874, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "304 Training acc 0.9 loss 0.6015895051686865 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9160, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "304 Training acc 0.9214285714285715 loss 0.6012286188485063 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9830, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "305 Training acc 0.9 loss 0.5971510717932187 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9452, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "305 Training acc 0.9214285714285715 loss 0.6021795620587378 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9130, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "306 Training acc 0.92 loss 0.596526491985026 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0235, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "306 Training acc 0.9171428571428571 loss 0.6023125910839022 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9257, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "307 Training acc 0.93 loss 0.5965260788802281 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0176, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "307 Training acc 0.915 loss 0.6023114536197669 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9671, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "308 Training acc 0.9 loss 0.6078860206743594 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9900, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "308 Training acc 0.9214285714285715 loss 0.5998755493331605 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9708, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "309 Training acc 0.91 loss 0.5995408384034017 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9546, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "309 Training acc 0.9192857142857143 loss 0.6016616867475157 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9879, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "310 Training acc 0.93 loss 0.6018146504323196 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9154, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "310 Training acc 0.915 loss 0.6011721473284092 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "311 Training acc 0.94 loss 0.5932524204899089 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9992, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "311 Training acc 0.9128571428571428 loss 0.6030035562254419 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9096, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "312 Training acc 0.9 loss 0.6015875925399705 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0088, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "312 Training acc 0.9214285714285715 loss 0.601214048813218 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9636, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "313 Training acc 0.94 loss 0.5987995637307708 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9708, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "313 Training acc 0.9128571428571428 loss 0.6018071969674452 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "314 Training acc 0.92 loss 0.5992576985798267 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9929, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "314 Training acc 0.9171428571428571 loss 0.6017048976117854 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8825, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "315 Training acc 0.93 loss 0.5981646154014808 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0253, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "315 Training acc 0.915 loss 0.60193510700568 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9596, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "316 Training acc 0.94 loss 0.6039957811251757 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9791, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "316 Training acc 0.9128571428571428 loss 0.6006816272398043 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9800, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "317 Training acc 0.92 loss 0.6013092294172984 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9340, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "317 Training acc 0.9171428571428571 loss 0.6012529238000912 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9568, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "318 Training acc 0.9 loss 0.6025219725387692 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9752, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "318 Training acc 0.9214285714285715 loss 0.6009882238221969 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "319 Training acc 0.9 loss 0.6039435048535148 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9898, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "319 Training acc 0.9214285714285715 loss 0.6006784226047461 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8837, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "320 Training acc 0.94 loss 0.5974601562987111 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0273, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "320 Training acc 0.9128571428571428 loss 0.6020630641960529 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9540, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "321 Training acc 0.91 loss 0.601064228064326 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9769, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "321 Training acc 0.9192857142857143 loss 0.6012866209607769 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9564, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "322 Training acc 0.93 loss 0.6023283500014727 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "322 Training acc 0.915 loss 0.601010465173603 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9209, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "323 Training acc 0.91 loss 0.5991265351898872 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0098, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "323 Training acc 0.9192857142857143 loss 0.6016912942218083 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9772, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "324 Training acc 0.94 loss 0.5961931811555455 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9636, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "324 Training acc 0.9128571428571428 loss 0.6023153914579144 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9827, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "325 Training acc 0.92 loss 0.5972935954449157 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9446, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "325 Training acc 0.9171428571428571 loss 0.6020751293111319 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9530, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "326 Training acc 0.91 loss 0.6001150546646209 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9794, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "326 Training acc 0.9192857142857143 loss 0.6014665421244879 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9761, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "327 Training acc 0.9 loss 0.6033074944051008 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9508, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "327 Training acc 0.9214285714285715 loss 0.6007795857282988 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "328 Training acc 0.93 loss 0.5966343178219168 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0094, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "328 Training acc 0.915 loss 0.602206413473203 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9730, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "329 Training acc 0.9 loss 0.6016720331174249 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "329 Training acc 0.9214285714285715 loss 0.6011240013266672 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9236, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "330 Training acc 0.89 loss 0.6023841612119948 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0024, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "330 Training acc 0.9235714285714286 loss 0.6009669592247581 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "331 Training acc 0.93 loss 0.5996950695962331 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "331 Training acc 0.915 loss 0.6015389099576547 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9846, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "332 Training acc 0.91 loss 0.6076044047965227 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9626, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "332 Training acc 0.9192857142857143 loss 0.5998397183899409 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "333 Training acc 0.91 loss 0.600303237582597 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9896, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "333 Training acc 0.9192857142857143 loss 0.6014000677982252 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9925, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "334 Training acc 0.92 loss 0.6041935974290353 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9200, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "334 Training acc 0.9171428571428571 loss 0.6005616188570836 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9398, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "335 Training acc 0.9 loss 0.6047625349272789 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0016, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "335 Training acc 0.9214285714285715 loss 0.6004347432083372 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9626, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "336 Training acc 0.95 loss 0.5993887852556445 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9688, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "336 Training acc 0.9107142857142857 loss 0.6015821212452241 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9339, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "337 Training acc 0.94 loss 0.5984276534243671 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0041, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "337 Training acc 0.9128571428571428 loss 0.6017843409400914 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9929, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "338 Training acc 0.91 loss 0.6012848237714894 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8972, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "338 Training acc 0.9192857142857143 loss 0.6011682370330453 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9694, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "339 Training acc 0.89 loss 0.6042441577051195 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9677, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "339 Training acc 0.9235714285714286 loss 0.6005289813335324 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9555, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "340 Training acc 0.89 loss 0.6018635619345274 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "340 Training acc 0.9235714285714286 loss 0.601034187790098 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0110, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "341 Training acc 0.89 loss 0.6076870775076577 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8889, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "341 Training acc 0.9235714285714286 loss 0.5997811755043392 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "342 Training acc 0.95 loss 0.5997164092259318 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9911, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "342 Training acc 0.9107142857142857 loss 0.6014844612999686 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9139, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "343 Training acc 0.91 loss 0.6039461519548259 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0128, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "343 Training acc 0.9192857142857143 loss 0.6005735019925909 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9440, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "344 Training acc 0.93 loss 0.6011213477059334 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9872, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "344 Training acc 0.915 loss 0.6011741588892975 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0145, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "345 Training acc 0.93 loss 0.6071944244387801 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8651, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "345 Training acc 0.915 loss 0.5998676189366046 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9509, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "346 Training acc 0.9 loss 0.6048105526858858 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9928, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "346 Training acc 0.9214285714285715 loss 0.6003726315764446 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "347 Training acc 0.92 loss 0.600620047091463 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9652, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "347 Training acc 0.9171428571428571 loss 0.6012656711918495 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9654, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "348 Training acc 0.94 loss 0.6021647258973052 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9624, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "348 Training acc 0.9128571428571428 loss 0.6009289807731231 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9543, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "349 Training acc 0.92 loss 0.5987767724649787 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9828, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "349 Training acc 0.9171428571428571 loss 0.6016496431896785 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0063, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "350 Training acc 0.94 loss 0.6053771176241757 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8832, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "350 Training acc 0.9128571428571428 loss 0.6002303954986339 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9334, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "351 Training acc 0.92 loss 0.5987183744498198 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0030, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "351 Training acc 0.9171428571428571 loss 0.6016522149953799 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9893, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "352 Training acc 0.91 loss 0.5974501151212183 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9261, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "352 Training acc 0.9192857142857143 loss 0.6019198278261453 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9743, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "353 Training acc 0.93 loss 0.5976461288836127 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9591, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "353 Training acc 0.915 loss 0.6018736401564195 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9615, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "354 Training acc 0.94 loss 0.6049931053125335 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9826, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "354 Training acc 0.9128571428571428 loss 0.6002952194654698 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9654, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "355 Training acc 0.95 loss 0.5978757192270839 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9726, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "355 Training acc 0.9107142857142857 loss 0.6018160515957729 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9652, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "356 Training acc 0.91 loss 0.6020599301485733 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9623, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "356 Training acc 0.9192857142857143 loss 0.6009154471520533 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9458, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "357 Training acc 0.93 loss 0.5921210008060903 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0221, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "357 Training acc 0.915 loss 0.6030408346771191 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "358 Training acc 0.91 loss 0.6038588817084181 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9370, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "358 Training acc 0.9192857142857143 loss 0.6005212534317053 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9782, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "359 Training acc 0.9 loss 0.6043709612266602 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9542, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "359 Training acc 0.9214285714285715 loss 0.6004070991572282 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9913, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "360 Training acc 0.9 loss 0.6070409116496731 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "360 Training acc 0.9214285714285715 loss 0.5998309429074934 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9883, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "361 Training acc 0.92 loss 0.6020320982582196 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9151, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "361 Training acc 0.9171428571428571 loss 0.6009000943784966 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8605, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "362 Training acc 0.94 loss 0.5929946980392026 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0398, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "362 Training acc 0.9128571428571428 loss 0.6028323583599597 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9783, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "363 Training acc 0.93 loss 0.6002954551809565 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9359, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "363 Training acc 0.915 loss 0.6012642076137638 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9560, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "364 Training acc 0.93 loss 0.5978777089798649 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9851, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "364 Training acc 0.915 loss 0.6017777808003197 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9801, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "365 Training acc 0.94 loss 0.605329349137582 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9570, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "365 Training acc 0.9128571428571428 loss 0.6001762992132477 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9786, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "366 Training acc 0.93 loss 0.6005628841328691 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9351, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "366 Training acc 0.915 loss 0.6011931282350508 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9898, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "367 Training acc 0.91 loss 0.602813810266644 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9169, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "367 Training acc 0.9192857142857143 loss 0.6007055662648331 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9919, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "368 Training acc 0.91 loss 0.5992482646362032 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9019, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "368 Training acc 0.9192857142857143 loss 0.6014645790720342 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9723, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "369 Training acc 0.91 loss 0.6012960674405593 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "369 Training acc 0.9192857142857143 loss 0.6010210481161278 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9461, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "370 Training acc 0.93 loss 0.6000327738475908 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9898, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "370 Training acc 0.915 loss 0.6012960903091751 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9838, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "371 Training acc 0.93 loss 0.603288318437676 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9354, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "371 Training acc 0.915 loss 0.6006634483547866 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "372 Training acc 0.9 loss 0.6052770644401322 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0032, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "372 Training acc 0.9214285714285715 loss 0.6002841855203396 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9477, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "373 Training acc 0.92 loss 0.6031232434803141 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9878, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "373 Training acc 0.9171428571428571 loss 0.6007841270144969 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9211, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "374 Training acc 0.94 loss 0.5921569100281441 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0362, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "374 Training acc 0.9128571428571428 loss 0.6031641245282489 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "375 Training acc 0.91 loss 0.5961840666699293 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0016, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "375 Training acc 0.9192857142857143 loss 0.602326607504329 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9654, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "376 Training acc 0.92 loss 0.6021765706423737 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9632, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "376 Training acc 0.9171428571428571 loss 0.6010619094130185 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9606, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "377 Training acc 0.92 loss 0.5955115153666739 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9926, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "377 Training acc 0.9171428571428571 loss 0.602497328866056 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "378 Training acc 0.92 loss 0.5944578438944396 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0160, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "378 Training acc 0.9171428571428571 loss 0.6027257413271917 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "379 Training acc 0.94 loss 0.6009532354867718 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9773, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "379 Training acc 0.9128571428571428 loss 0.6013355003371978 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0045, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "380 Training acc 0.93 loss 0.6070941505908961 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9080, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "380 Training acc 0.915 loss 0.6000192223997194 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9797, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "381 Training acc 0.94 loss 0.5988551274565813 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9405, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "381 Training acc 0.9128571428571428 loss 0.6017828931356823 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9803, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "382 Training acc 0.89 loss 0.6097991596102579 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9829, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "382 Training acc 0.9235714285714286 loss 0.5994356513483535 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9957, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "383 Training acc 0.89 loss 0.605777034065954 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9239, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "383 Training acc 0.9235714285714286 loss 0.6002953170153464 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9867, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "384 Training acc 0.94 loss 0.5913731746416384 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9772, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "384 Training acc 0.9128571428571428 loss 0.603379556367589 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9829, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "385 Training acc 0.9 loss 0.6111101289475396 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9866, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "385 Training acc 0.9214285714285715 loss 0.5991477484479626 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9560, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "386 Training acc 0.91 loss 0.5978709225196817 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9859, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "386 Training acc 0.9192857142857143 loss 0.60198199035206 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9334, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "387 Training acc 0.92 loss 0.5987300584098335 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0035, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "387 Training acc 0.9171428571428571 loss 0.60179536759856 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9589, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "388 Training acc 0.9 loss 0.6036622690530168 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9782, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "388 Training acc 0.9214285714285715 loss 0.6007355518666994 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9579, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "389 Training acc 0.88 loss 0.6084517586662037 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0027, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "389 Training acc 0.9257142857142857 loss 0.5997057787591424 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0149, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "390 Training acc 0.9 loss 0.6095661020605547 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8935, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "390 Training acc 0.9214285714285715 loss 0.599463555813809 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9457, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "391 Training acc 0.91 loss 0.6020302792187914 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9858, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "391 Training acc 0.9192857142857143 loss 0.6010746025689585 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9542, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "392 Training acc 0.9 loss 0.5988433882206446 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9831, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "392 Training acc 0.9214285714285715 loss 0.6017549519488845 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8816, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "393 Training acc 0.93 loss 0.601362164382361 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0159, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "393 Training acc 0.915 loss 0.6012129804021304 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0077, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "394 Training acc 0.9 loss 0.6086698537867385 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9123, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "394 Training acc 0.9214285714285715 loss 0.5996430835881925 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0165, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "395 Training acc 0.89 loss 0.6129959288299653 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9238, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "395 Training acc 0.9235714285714286 loss 0.598712062588785 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8942, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "396 Training acc 0.93 loss 0.600171828336349 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0168, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "396 Training acc 0.915 loss 0.6014558721254478 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "397 Training acc 0.95 loss 0.5943983128681293 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0160, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "397 Training acc 0.9107142857142857 loss 0.6026893785162922 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9957, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "398 Training acc 0.92 loss 0.6027055837726778 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8969, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "398 Training acc 0.9171428571428571 loss 0.6009061383854668 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9685, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "399 Training acc 0.93 loss 0.5914215709314241 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0042, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "399 Training acc 0.915 loss 0.6033196091128648 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9688, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "400 Training acc 0.91 loss 0.6039424459857042 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9667, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "400 Training acc 0.9192857142857143 loss 0.6006327663620332 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9659, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "401 Training acc 0.92 loss 0.5927302865084669 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0004, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "401 Training acc 0.9171428571428571 loss 0.6030315182790371 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0111, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "402 Training acc 0.92 loss 0.6077402435652438 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8889, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "402 Training acc 0.9171428571428571 loss 0.5998116193918004 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9867, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "403 Training acc 0.92 loss 0.5988139949842467 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9228, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "403 Training acc 0.9171428571428571 loss 0.6017204003770671 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8984, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "404 Training acc 0.93 loss 0.5971815210195066 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0254, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "404 Training acc 0.915 loss 0.6020666074736114 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8888, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "405 Training acc 0.96 loss 0.5945846843143103 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0356, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "405 Training acc 0.9085714285714286 loss 0.6026194382931453 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9561, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "406 Training acc 0.91 loss 0.5978498009237981 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9857, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "406 Training acc 0.9192857142857143 loss 0.6019164364970333 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "407 Training acc 0.9 loss 0.6015549690960421 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9331, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "407 Training acc 0.9214285714285715 loss 0.6011188921574535 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9254, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "408 Training acc 0.95 loss 0.5966603879802217 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0168, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "408 Training acc 0.9107142857142857 loss 0.6021627035870207 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9806, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "409 Training acc 0.93 loss 0.5983642712493574 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "409 Training acc 0.915 loss 0.6017933816852452 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9594, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "410 Training acc 0.91 loss 0.6039265660882662 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9793, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "410 Training acc 0.9192857142857143 loss 0.6005966335259902 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9998, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "411 Training acc 0.92 loss 0.6047571200501338 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9023, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "411 Training acc 0.9171428571428571 loss 0.6004140961814772 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9818, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "412 Training acc 0.92 loss 0.6022577657071397 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "412 Training acc 0.9171428571428571 loss 0.6009451959205048 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9301, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "413 Training acc 0.9 loss 0.6058538871502934 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0124, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "413 Training acc 0.9214285714285715 loss 0.6001699201878108 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9722, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "414 Training acc 0.91 loss 0.6012394952453272 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "414 Training acc 0.9192857142857143 loss 0.6011542286742445 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9623, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "415 Training acc 0.93 loss 0.5945851415220551 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9948, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "415 Training acc 0.915 loss 0.6025750076061096 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9616, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "416 Training acc 0.89 loss 0.605054534129422 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9826, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "416 Training acc 0.9235714285714286 loss 0.6003264457692654 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9736, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "417 Training acc 0.93 loss 0.602008719556854 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "417 Training acc 0.915 loss 0.6009739586160028 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0015, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "418 Training acc 0.9 loss 0.6086459362591495 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "418 Training acc 0.9214285714285715 loss 0.5995409910366041 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9924, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "419 Training acc 0.91 loss 0.6041354381500964 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9201, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "419 Training acc 0.9192857142857143 loss 0.6004963857354493 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9900, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "420 Training acc 0.91 loss 0.602915438213244 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9170, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "420 Training acc 0.9192857142857143 loss 0.600747927995072 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9725, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "421 Training acc 0.89 loss 0.6106163009338762 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9986, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "421 Training acc 0.9235714285714286 loss 0.5990889466024067 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9760, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "422 Training acc 0.92 loss 0.6032099369791375 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9510, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "422 Training acc 0.9171428571428571 loss 0.6006685589101234 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9921, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "423 Training acc 0.91 loss 0.5960311218567957 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9298, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "423 Training acc 0.9192857142857143 loss 0.6022000998951177 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9792, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "424 Training acc 0.91 loss 0.6009078480950713 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "424 Training acc 0.9192857142857143 loss 0.601148976724242 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9433, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "425 Training acc 0.9 loss 0.6007202062362279 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9880, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "425 Training acc 0.9214285714285715 loss 0.6011821888813264 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "426 Training acc 0.92 loss 0.5994025768659165 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9916, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "426 Training acc 0.9171428571428571 loss 0.6014571291118715 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9983, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "427 Training acc 0.9 loss 0.6105476165155643 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9553, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "427 Training acc 0.9214285714285715 loss 0.5990613102139175 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9707, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "428 Training acc 0.93 loss 0.6004091719292753 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9513, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "428 Training acc 0.915 loss 0.601227341462828 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0016, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "429 Training acc 0.89 loss 0.6056722507930187 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9052, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "429 Training acc 0.9235714285714286 loss 0.6000922221303828 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0015, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "430 Training acc 0.92 loss 0.6029419603149008 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8774, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "430 Training acc 0.9171428571428571 loss 0.6006708512606836 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9749, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "431 Training acc 0.92 loss 0.5929869286490903 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9862, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "431 Training acc 0.9171428571428571 loss 0.6027980472419532 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0095, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "432 Training acc 0.9 loss 0.606930625651942 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8875, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "432 Training acc 0.9214285714285715 loss 0.5998048148744966 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9759, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "433 Training acc 0.93 loss 0.592451827099893 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9877, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "433 Training acc 0.915 loss 0.602902033651117 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9982, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "434 Training acc 0.91 loss 0.6069977339429649 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9281, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "434 Training acc 0.9192857142857143 loss 0.5997801687552123 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9741, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "435 Training acc 0.92 loss 0.6022598873460936 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "435 Training acc 0.9171428571428571 loss 0.6007903586124919 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9703, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "436 Training acc 0.92 loss 0.6046867462225467 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9696, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "436 Training acc 0.9171428571428571 loss 0.6002652101029234 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9856, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "437 Training acc 0.92 loss 0.600645548269296 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9173, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "437 Training acc 0.9171428571428571 loss 0.6011266433928476 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9323, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "438 Training acc 0.94 loss 0.6006719323049019 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9969, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "438 Training acc 0.9128571428571428 loss 0.6011154680784767 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0190, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "439 Training acc 0.88 loss 0.6093860076249944 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8711, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "439 Training acc 0.9257142857142857 loss 0.5992414490571498 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9169, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "440 Training acc 0.92 loss 0.6056002970652378 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0183, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "440 Training acc 0.9171428571428571 loss 0.6000464840000233 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9721, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "441 Training acc 0.91 loss 0.5988071151596207 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9554, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "441 Training acc 0.9192857142857143 loss 0.6014962557857125 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9386, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "442 Training acc 0.93 loss 0.5958969485026713 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0106, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "442 Training acc 0.915 loss 0.60211487288441 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9772, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "443 Training acc 0.91 loss 0.6082131662523566 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9796, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "443 Training acc 0.9192857142857143 loss 0.5994706510275163 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9251, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "444 Training acc 0.89 loss 0.6032141315883932 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0056, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "444 Training acc 0.9235714285714286 loss 0.6005361561433229 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9539, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "445 Training acc 0.94 loss 0.5989955273142341 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9815, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "445 Training acc 0.9128571428571428 loss 0.6014352501159413 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9554, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "446 Training acc 0.91 loss 0.6017856573155269 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9749, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "446 Training acc 0.9192857142857143 loss 0.6008326590513067 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9915, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "447 Training acc 0.91 loss 0.6071656698403649 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "447 Training acc 0.9192857142857143 loss 0.599673137025932 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "448 Training acc 0.92 loss 0.6007896913601745 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9873, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "448 Training acc 0.9171428571428571 loss 0.6010328770589775 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "449 Training acc 0.91 loss 0.5997652358046291 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9793, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "449 Training acc 0.9192857142857143 loss 0.6012454524161503 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "450 Training acc 0.91 loss 0.5957829273431525 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0014, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "450 Training acc 0.9192857142857143 loss 0.6020919002058795 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9341, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "451 Training acc 0.97 loss 0.5920233220140042 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0294, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "451 Training acc 0.9064285714285714 loss 0.6028911977647993 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9756, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "452 Training acc 0.92 loss 0.6030253660005483 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9512, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "452 Training acc 0.9171428571428571 loss 0.6005273507953074 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9828, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "453 Training acc 0.92 loss 0.6027428695646537 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9347, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "453 Training acc 0.9171428571428571 loss 0.6005818487850233 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "454 Training acc 0.96 loss 0.5905463463282178 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0260, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "454 Training acc 0.9085714285714286 loss 0.6031900562603616 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9796, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "455 Training acc 0.91 loss 0.5949487772532182 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9657, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "455 Training acc 0.9192857142857143 loss 0.6022419444446931 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0020, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "456 Training acc 0.92 loss 0.6058777226281722 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9063, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "456 Training acc 0.9171428571428571 loss 0.5998951691462496 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9546, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "457 Training acc 0.91 loss 0.6067075394552148 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9992, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "457 Training acc 0.9192857142857143 loss 0.5997122089683459 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "458 Training acc 0.92 loss 0.6022152822124918 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9334, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "458 Training acc 0.9171428571428571 loss 0.6006701111542505 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "459 Training acc 0.93 loss 0.5990702477410196 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "459 Training acc 0.915 loss 0.6013394166944075 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9837, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "460 Training acc 0.94 loss 0.5928776142183477 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9714, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "460 Training acc 0.9128571428571428 loss 0.6026625803449754 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "461 Training acc 0.93 loss 0.6002428709883582 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9652, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "461 Training acc 0.915 loss 0.6010801680444615 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9631, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "462 Training acc 0.93 loss 0.6058063960281392 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9858, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "462 Training acc 0.915 loss 0.5998831085010861 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9679, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "463 Training acc 0.92 loss 0.5965489628914494 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9754, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "463 Training acc 0.9171428571428571 loss 0.6018617748237124 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "464 Training acc 0.93 loss 0.6002628244373795 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9510, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "464 Training acc 0.915 loss 0.6010614118933733 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9820, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "465 Training acc 0.92 loss 0.6062802926215407 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9604, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "465 Training acc 0.9171428571428571 loss 0.5997664643944063 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "466 Training acc 0.89 loss 0.6062530628434413 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0149, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "466 Training acc 0.9235714285714286 loss 0.5997665844094637 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9819, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "467 Training acc 0.93 loss 0.5977357652475637 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9419, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "467 Training acc 0.915 loss 0.6015867356364766 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9860, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "468 Training acc 0.91 loss 0.6043612076469466 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "468 Training acc 0.9192857142857143 loss 0.6001623922166802 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9448, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "469 Training acc 0.91 loss 0.598456534841226 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9934, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "469 Training acc 0.9192857142857143 loss 0.6014231416997778 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9642, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "470 Training acc 0.92 loss 0.5999741617370014 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9658, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "470 Training acc 0.9171428571428571 loss 0.6010875551629606 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9536, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "471 Training acc 0.91 loss 0.6008337875121843 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9758, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "471 Training acc 0.9192857142857143 loss 0.6008684218896616 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9586, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "472 Training acc 0.93 loss 0.59653870123246 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9876, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "472 Training acc 0.915 loss 0.6017587245170729 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9535, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "473 Training acc 0.92 loss 0.6007998889932924 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9756, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "473 Training acc 0.9171428571428571 loss 0.6008198335851704 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9459, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "474 Training acc 0.91 loss 0.6021349392834976 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9870, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "474 Training acc 0.9192857142857143 loss 0.6005110143024674 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9443, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "475 Training acc 0.92 loss 0.5987506983402938 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9920, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "475 Training acc 0.9171428571428571 loss 0.6012165815508598 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8867, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "476 Training acc 0.93 loss 0.5957830370437261 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0298, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "476 Training acc 0.915 loss 0.6018362824707055 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9579, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "477 Training acc 0.91 loss 0.5968956676308319 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9862, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "477 Training acc 0.9192857142857143 loss 0.6015836148340812 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9776, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "478 Training acc 0.89 loss 0.6040676243734842 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9551, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "478 Training acc 0.9235714285714286 loss 0.6000341369999633 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9901, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "479 Training acc 0.89 loss 0.6064275734531832 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "479 Training acc 0.9235714285714286 loss 0.5995170807007815 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9324, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "480 Training acc 0.92 loss 0.6007493927421368 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9953, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "480 Training acc 0.9171428571428571 loss 0.6007243130087152 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9813, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "481 Training acc 0.92 loss 0.6020086338059486 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9339, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "481 Training acc 0.9171428571428571 loss 0.6004446092728113 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9890, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "482 Training acc 0.88 loss 0.6058784898714745 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9443, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "482 Training acc 0.9257142857142857 loss 0.5996063604398363 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "483 Training acc 0.92 loss 0.5951891989122496 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9782, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "483 Training acc 0.9171428571428571 loss 0.601889300098148 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9626, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "484 Training acc 0.91 loss 0.6006394463979828 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9631, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "484 Training acc 0.9192857142857143 loss 0.6007144996229647 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9202, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "485 Training acc 0.92 loss 0.6004709412073125 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0033, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "485 Training acc 0.9171428571428571 loss 0.6007428310002454 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0036, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "486 Training acc 0.93 loss 0.6040268274137729 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8818, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "486 Training acc 0.915 loss 0.5999722036092474 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9928, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "487 Training acc 0.92 loss 0.6012541969761834 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8961, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "487 Training acc 0.9171428571428571 loss 0.6005586302576966 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9119, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "488 Training acc 0.92 loss 0.5971726561452385 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0186, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "488 Training acc 0.9171428571428571 loss 0.6014247384370832 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9561, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "489 Training acc 0.9 loss 0.602172753367729 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9769, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "489 Training acc 0.9214285714285715 loss 0.6003454011801989 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "490 Training acc 0.9 loss 0.6055492980502529 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9595, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "490 Training acc 0.9214285714285715 loss 0.5996143685737467 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9359, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "491 Training acc 0.91 loss 0.597356566574094 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0045, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "491 Training acc 0.9192857142857143 loss 0.6013633159147191 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9958, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "492 Training acc 0.93 loss 0.5942215599907668 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "492 Training acc 0.915 loss 0.6020295852563486 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0017, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "493 Training acc 0.93 loss 0.6008164749401519 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8531, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "493 Training acc 0.915 loss 0.6006106804212094 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9633, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "494 Training acc 0.94 loss 0.6010604013644255 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "494 Training acc 0.9128571428571428 loss 0.6005514397940706 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9025, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "495 Training acc 0.94 loss 0.5949551610050291 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0292, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "495 Training acc 0.9128571428571428 loss 0.6018790900285831 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9527, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "496 Training acc 0.94 loss 0.5942980884678858 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0043, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "496 Training acc 0.9128571428571428 loss 0.602048988670014 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9974, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "497 Training acc 0.92 loss 0.5991180849659001 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8797, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "497 Training acc 0.9171428571428571 loss 0.6010379575462645 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9210, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "498 Training acc 0.93 loss 0.5990591157380891 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0074, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "498 Training acc 0.915 loss 0.6010660705087585 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9533, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "499 Training acc 0.91 loss 0.5993527877439304 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9792, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "499 Training acc 0.9192857142857143 loss 0.6010160071925024 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9318, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "500 Training acc 0.92 loss 0.600316120833488 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9965, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "500 Training acc 0.9171428571428571 loss 0.6008182746740187 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8911, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "501 Training acc 0.96 loss 0.593338752040598 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0370, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "501 Training acc 0.9085714285714286 loss 0.6023187354086654 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9451, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "502 Training acc 0.9 loss 0.6016995346245808 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9861, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "502 Training acc 0.9214285714285715 loss 0.6005312179729954 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9556, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "503 Training acc 0.92 loss 0.6018930728573971 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9758, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "503 Training acc 0.9171428571428571 loss 0.6004912301377934 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9582, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "504 Training acc 0.92 loss 0.6032538005280723 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9795, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "504 Training acc 0.9171428571428571 loss 0.6001999857170127 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9546, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "505 Training acc 0.91 loss 0.6067138860061713 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0002, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "505 Training acc 0.9192857142857143 loss 0.5994580509688392 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "506 Training acc 0.91 loss 0.6037539809933948 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9918, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "506 Training acc 0.9192857142857143 loss 0.6000911146896888 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9968, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "507 Training acc 0.91 loss 0.606331081676127 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9277, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "507 Training acc 0.9192857142857143 loss 0.5995370211643876 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "508 Training acc 0.91 loss 0.5983980557297904 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9928, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "508 Training acc 0.9192857142857143 loss 0.6012350392430946 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9200, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "509 Training acc 0.96 loss 0.599813393882262 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0054, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "509 Training acc 0.9085714285714286 loss 0.6009295941805242 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9560, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "510 Training acc 0.93 loss 0.5978789152184928 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9833, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "510 Training acc 0.915 loss 0.6013406804895265 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0103, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "511 Training acc 0.92 loss 0.601902545844502 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8041, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "511 Training acc 0.9171428571428571 loss 0.6004750073402901 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9708, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "512 Training acc 0.92 loss 0.5995643519113135 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9520, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "512 Training acc 0.9171428571428571 loss 0.6009713893917855 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "513 Training acc 0.89 loss 0.6029687769852345 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9897, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "513 Training acc 0.9235714285714286 loss 0.6002374862749636 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "514 Training acc 0.93 loss 0.5981291561943703 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9700, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "514 Training acc 0.915 loss 0.6012700555686266 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9632, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "515 Training acc 0.94 loss 0.5941281363759299 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9939, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "515 Training acc 0.9128571428571428 loss 0.6021233175880714 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9548, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "516 Training acc 0.92 loss 0.6015110644677136 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9750, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "516 Training acc 0.9171428571428571 loss 0.6005370497219943 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0078, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "517 Training acc 0.91 loss 0.6020609727853594 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8303, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "517 Training acc 0.9192857142857143 loss 0.6004139366241639 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0021, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "518 Training acc 0.93 loss 0.605921334000309 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9075, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "518 Training acc 0.915 loss 0.599581375960986 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9700, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "519 Training acc 0.92 loss 0.5954720883810499 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9774, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "519 Training acc 0.9171428571428571 loss 0.601815537562977 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9797, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "520 Training acc 0.89 loss 0.6051325457206148 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9582, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "520 Training acc 0.9235714285714286 loss 0.5997409189505527 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9840, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "521 Training acc 0.9 loss 0.6033532960354251 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9376, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "521 Training acc 0.9214285714285715 loss 0.6001174908016809 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "522 Training acc 0.9 loss 0.6002616936586789 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9640, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "522 Training acc 0.9214285714285715 loss 0.6007751478468335 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9915, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "523 Training acc 0.92 loss 0.596356654631804 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9270, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "523 Training acc 0.9171428571428571 loss 0.6016070738950963 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9577, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "524 Training acc 0.92 loss 0.6030011462807693 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9791, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "524 Training acc 0.9171428571428571 loss 0.6001780284787475 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9718, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "525 Training acc 0.92 loss 0.6010322167457594 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "525 Training acc 0.9171428571428571 loss 0.6005944312670388 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9564, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "526 Training acc 0.94 loss 0.6023127322194252 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9773, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "526 Training acc 0.9128571428571428 loss 0.6003153429736211 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9768, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "527 Training acc 0.89 loss 0.6080079433015597 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9807, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "527 Training acc 0.9235714285714286 loss 0.5990898741272154 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9894, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "528 Training acc 0.91 loss 0.5974005229301771 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9241, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "528 Training acc 0.9192857142857143 loss 0.601358520160467 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9712, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "529 Training acc 0.9 loss 0.6007237994999907 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "529 Training acc 0.9214285714285715 loss 0.6006422337035484 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9818, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "530 Training acc 0.93 loss 0.5938103630209406 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9675, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "530 Training acc 0.915 loss 0.6021181456283788 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9668, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "531 Training acc 0.92 loss 0.5971271373478786 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9725, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "531 Training acc 0.9171428571428571 loss 0.6014021749515461 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9945, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "532 Training acc 0.92 loss 0.6051928589654477 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9251, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "532 Training acc 0.9171428571428571 loss 0.5996683768585603 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9558, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "533 Training acc 0.92 loss 0.592679675672199 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0088, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "533 Training acc 0.9171428571428571 loss 0.6023444526843645 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9659, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "534 Training acc 0.91 loss 0.5976069017662883 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9711, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "534 Training acc 0.9192857142857143 loss 0.6012837813271072 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9571, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "535 Training acc 0.94 loss 0.5972909219929879 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9845, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "535 Training acc 0.9128571428571428 loss 0.6013467821821554 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9126, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "536 Training acc 0.91 loss 0.5967544178785423 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0196, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "536 Training acc 0.9192857142857143 loss 0.6014569162069505 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9567, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "537 Training acc 0.91 loss 0.5975045576676168 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9839, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "537 Training acc 0.9192857142857143 loss 0.6012913349405569 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9452, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "538 Training acc 0.94 loss 0.5982483554242267 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9926, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "538 Training acc 0.9128571428571428 loss 0.6011271006084427 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "539 Training acc 0.9 loss 0.6005722573674052 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9629, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "539 Training acc 0.9214285714285715 loss 0.6006240462559591 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9751, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "540 Training acc 0.9 loss 0.607148716994147 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9785, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "540 Training acc 0.9214285714285715 loss 0.5992082925503103 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9639, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "541 Training acc 0.93 loss 0.5986583235843552 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9680, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "541 Training acc 0.915 loss 0.6010213140028033 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9883, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "542 Training acc 0.92 loss 0.6055292093356416 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "542 Training acc 0.9171428571428571 loss 0.599543163551648 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9852, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "543 Training acc 0.91 loss 0.5997082999263327 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9177, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "543 Training acc 0.9192857142857143 loss 0.6007848411604579 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9469, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "544 Training acc 0.95 loss 0.5915526472704861 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0212, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "544 Training acc 0.9107142857142857 loss 0.6025270800033864 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9376, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "545 Training acc 0.93 loss 0.5964422897939519 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0068, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "545 Training acc 0.915 loss 0.6014741240051567 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9092, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "546 Training acc 0.91 loss 0.5986561698912753 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0138, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "546 Training acc 0.9192857142857143 loss 0.6009946062867092 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9336, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "547 Training acc 0.89 loss 0.5986118481170567 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "547 Training acc 0.9235714285714286 loss 0.6009991845544848 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "548 Training acc 0.93 loss 0.5965874112952977 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9971, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "548 Training acc 0.915 loss 0.6014281916225162 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9765, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "549 Training acc 0.92 loss 0.6034891472239473 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9543, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "549 Training acc 0.9171428571428571 loss 0.5999443171729452 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9549, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "550 Training acc 0.95 loss 0.598499649850654 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9808, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "550 Training acc 0.9107142857142857 loss 0.6010080800154023 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9710, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "551 Training acc 0.94 loss 0.605059105250558 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9729, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "551 Training acc 0.9128571428571428 loss 0.5995976244752539 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9003, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "552 Training acc 0.92 loss 0.596131550666814 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0252, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "552 Training acc 0.9171428571428571 loss 0.6015052574908113 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9954, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "553 Training acc 0.89 loss 0.6130099019779017 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "553 Training acc 0.9235714285714286 loss 0.597883988622127 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9670, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "554 Training acc 0.92 loss 0.5970016558731986 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9724, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "554 Training acc 0.9171428571428571 loss 0.6013091812787537 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0031, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "555 Training acc 0.91 loss 0.6129223370108263 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9645, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "555 Training acc 0.9192857142857143 loss 0.5978929392240175 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9692, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "556 Training acc 0.94 loss 0.59590469315415 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9754, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "556 Training acc 0.9128571428571428 loss 0.6015347593169376 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "557 Training acc 0.93 loss 0.5996857892111346 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "557 Training acc 0.915 loss 0.6007202540474064 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9590, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "558 Training acc 0.93 loss 0.596318623649607 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9868, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "558 Training acc 0.915 loss 0.6014371357000599 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9913, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "559 Training acc 0.89 loss 0.6035498354416219 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9213, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "559 Training acc 0.9235714285714286 loss 0.5998825710133594 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9858, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "560 Training acc 0.93 loss 0.6042735918884702 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9408, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "560 Training acc 0.915 loss 0.5997223029522274 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9645, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "561 Training acc 0.91 loss 0.601691101958576 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9638, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "561 Training acc 0.9192857142857143 loss 0.6002707846214835 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9804, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "562 Training acc 0.97 loss 0.5984793220074813 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "562 Training acc 0.9064285714285714 loss 0.6009541088381328 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9375, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "563 Training acc 0.9 loss 0.5965065145021333 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0063, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "563 Training acc 0.9214285714285715 loss 0.6013725048780985 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9854, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "564 Training acc 0.92 loss 0.5959607278624851 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "564 Training acc 0.9171428571428571 loss 0.6014852327536568 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9091, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "565 Training acc 0.91 loss 0.5987120443043918 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0133, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "565 Training acc 0.9192857142857143 loss 0.6008913053921094 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9909, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "566 Training acc 0.89 loss 0.6107721003101032 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9749, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "566 Training acc 0.9235714285714286 loss 0.59830233410543 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9555, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "567 Training acc 0.94 loss 0.5981361943009104 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9815, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "567 Training acc 0.9128571428571428 loss 0.6010052007780962 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "568 Training acc 0.93 loss 0.6025860275003827 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9988, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "568 Training acc 0.915 loss 0.6000470469851252 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0099, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "569 Training acc 0.93 loss 0.607124135435238 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8905, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "569 Training acc 0.915 loss 0.5990696785986234 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9770, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "570 Training acc 0.93 loss 0.5919128704080835 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9864, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "570 Training acc 0.915 loss 0.602324391598758 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9753, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "571 Training acc 0.91 loss 0.6028436658658247 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9530, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "571 Training acc 0.9192857142857143 loss 0.5999776532469729 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0081, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "572 Training acc 0.95 loss 0.5937626835261917 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8924, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "572 Training acc 0.9107142857142857 loss 0.601918725477094 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9999, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "573 Training acc 0.93 loss 0.592139389943068 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9374, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "573 Training acc 0.915 loss 0.6022622028667486 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0063, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "574 Training acc 0.92 loss 0.6031522964467969 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8580, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "574 Training acc 0.9171428571428571 loss 0.5998975358460349 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9739, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "575 Training acc 0.91 loss 0.6021267913059464 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9511, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "575 Training acc 0.9192857142857143 loss 0.6001125765653634 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9670, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "576 Training acc 0.96 loss 0.5970471419883576 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9719, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "576 Training acc 0.9085714285714286 loss 0.6011961673064239 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9730, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "577 Training acc 0.92 loss 0.6016917361938788 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "577 Training acc 0.9171428571428571 loss 0.6001962936806492 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9533, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "578 Training acc 0.93 loss 0.600633088869082 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "578 Training acc 0.915 loss 0.6004180208593726 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "579 Training acc 0.9 loss 0.6003345678695959 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9628, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "579 Training acc 0.9214285714285715 loss 0.6004766155850336 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9858, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "580 Training acc 0.9 loss 0.6042789515803138 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9412, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "580 Training acc 0.9214285714285715 loss 0.5996249131876763 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9745, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "581 Training acc 0.94 loss 0.597542185324906 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9562, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "581 Training acc 0.9128571428571428 loss 0.6010623511926226 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9547, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "582 Training acc 0.92 loss 0.6067927875140962 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0020, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "582 Training acc 0.9171428571428571 loss 0.599074339603411 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9323, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "583 Training acc 0.94 loss 0.5929696676042956 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0238, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "583 Training acc 0.9128571428571428 loss 0.6020309434921086 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9848, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "584 Training acc 0.92 loss 0.5962384506144924 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9437, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "584 Training acc 0.9171428571428571 loss 0.6013254295567397 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9833, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "585 Training acc 0.9 loss 0.6030285980938123 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "585 Training acc 0.9214285714285715 loss 0.5998651493087386 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9560, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "586 Training acc 0.94 loss 0.6000317997669993 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9759, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "586 Training acc 0.9128571428571428 loss 0.6005158685225152 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9868, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "587 Training acc 0.94 loss 0.5987551266497998 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9198, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "587 Training acc 0.9128571428571428 loss 0.6008621157486269 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9550, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "588 Training acc 0.91 loss 0.5983948907294633 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "588 Training acc 0.9192857142857143 loss 0.6009935147625886 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9827, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "589 Training acc 0.93 loss 0.6027141860658171 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "589 Training acc 0.915 loss 0.600111406385385 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8952, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "590 Training acc 0.93 loss 0.5990092401757764 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0170, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "590 Training acc 0.915 loss 0.6009390167491456 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9638, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "591 Training acc 0.92 loss 0.6013360223521148 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "591 Training acc 0.9171428571428571 loss 0.6004680750496512 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "592 Training acc 0.93 loss 0.5997745298355129 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9651, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "592 Training acc 0.915 loss 0.6008226225268983 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "593 Training acc 0.92 loss 0.595924521012141 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9994, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "593 Training acc 0.9171428571428571 loss 0.6016628300237865 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8762, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "594 Training acc 0.93 loss 0.5931419274134934 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0380, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "594 Training acc 0.915 loss 0.6022703839485599 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9225, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "595 Training acc 0.88 loss 0.5982344213124435 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0096, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "595 Training acc 0.9257142857142857 loss 0.6011878041267718 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "596 Training acc 0.92 loss 0.6026106149620181 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9980, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "596 Training acc 0.9171428571428571 loss 0.6002555864266594 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9784, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "597 Training acc 0.93 loss 0.6004299661635749 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9337, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "597 Training acc 0.915 loss 0.600726164168422 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9975, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "598 Training acc 0.92 loss 0.603619603837441 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9016, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "598 Training acc 0.9171428571428571 loss 0.6000432521026189 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0022, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "599 Training acc 0.91 loss 0.6124402838168541 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "599 Training acc 0.9192857142857143 loss 0.5981517761819413 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9791, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "600 Training acc 0.9 loss 0.6047830031208548 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9574, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "600 Training acc 0.9214285714285715 loss 0.5997906488845155 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9793, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "601 Training acc 0.91 loss 0.5990630382210548 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "601 Training acc 0.9192857142857143 loss 0.6010141206941522 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9851, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "602 Training acc 0.93 loss 0.6001858455979462 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9168, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "602 Training acc 0.915 loss 0.6007713936362211 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8945, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "603 Training acc 0.91 loss 0.5994128663050227 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0162, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "603 Training acc 0.9192857142857143 loss 0.6009335409757732 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8692, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "604 Training acc 0.92 loss 0.5971692226658365 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0261, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "604 Training acc 0.9171428571428571 loss 0.6014103743705457 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9859, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "605 Training acc 0.93 loss 0.5956968527002399 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9463, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "605 Training acc 0.915 loss 0.6017224928537757 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "606 Training acc 0.93 loss 0.6061383862704621 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9612, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "606 Training acc 0.915 loss 0.5994805876395262 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9341, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "607 Training acc 0.89 loss 0.5983203266402384 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0018, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "607 Training acc 0.9235714285714286 loss 0.6011508445417291 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9230, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "608 Training acc 0.88 loss 0.6020598366331852 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0041, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "608 Training acc 0.9257142857142857 loss 0.6003449243216099 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "609 Training acc 0.93 loss 0.5981569519003845 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9396, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "609 Training acc 0.915 loss 0.60117634034889 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9794, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "610 Training acc 0.92 loss 0.6009831289602114 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "610 Training acc 0.9171428571428571 loss 0.6005662895758892 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "611 Training acc 0.93 loss 0.5950358249089228 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0112, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "611 Training acc 0.915 loss 0.6018351547078629 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9981, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "612 Training acc 0.89 loss 0.6012642609023403 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8753, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "612 Training acc 0.9235714285714286 loss 0.6004962130071745 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9707, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "613 Training acc 0.93 loss 0.6004007916969684 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "613 Training acc 0.915 loss 0.6006751936455572 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "614 Training acc 0.91 loss 0.5982076452260414 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9928, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "614 Training acc 0.9192857142857143 loss 0.6011384002205367 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0097, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "615 Training acc 0.91 loss 0.6070440089819159 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8897, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "615 Training acc 0.9192857142857143 loss 0.5992385149532072 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9930, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "616 Training acc 0.93 loss 0.5986632849526912 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9014, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "616 Training acc 0.915 loss 0.6010279314671192 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8942, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "617 Training acc 0.91 loss 0.5996783895309856 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0151, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "617 Training acc 0.9192857142857143 loss 0.6008045501795691 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9792, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "618 Training acc 0.94 loss 0.5991074654721578 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9368, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "618 Training acc 0.9128571428571428 loss 0.6009207767154174 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9274, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "619 Training acc 0.91 loss 0.6044200768225143 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0111, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "619 Training acc 0.9192857142857143 loss 0.5997762940930671 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9207, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "620 Training acc 0.92 loss 0.6007875364514752 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0022, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "620 Training acc 0.9171428571428571 loss 0.600548159093608 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9764, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "621 Training acc 0.9 loss 0.6034033501138215 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9540, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "621 Training acc 0.9214285714285715 loss 0.5999802504098433 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9080, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "622 Training acc 0.91 loss 0.5919209417595468 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "622 Training acc 0.9192857142857143 loss 0.6024334707558455 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9725, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "623 Training acc 0.92 loss 0.5985839420449284 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9539, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "623 Training acc 0.9171428571428571 loss 0.6009996733333136 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "624 Training acc 0.93 loss 0.6036549999829552 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9923, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "624 Training acc 0.915 loss 0.5999066314646679 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0031, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "625 Training acc 0.89 loss 0.6037460653040904 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8816, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "625 Training acc 0.9235714285714286 loss 0.5998803527852166 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9911, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "626 Training acc 0.92 loss 0.603441336362826 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9209, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "626 Training acc 0.9171428571428571 loss 0.5999394728827303 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "627 Training acc 0.92 loss 0.5998230329068177 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9771, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "627 Training acc 0.9171428571428571 loss 0.6007091613249793 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9363, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "628 Training acc 0.94 loss 0.5971564098344417 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0046, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "628 Training acc 0.9128571428571428 loss 0.6012740052749364 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9977, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "629 Training acc 0.91 loss 0.6037042709837338 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9024, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "629 Training acc 0.9192857142857143 loss 0.5998647299486146 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9948, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "630 Training acc 0.9 loss 0.6053342527115029 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9260, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "630 Training acc 0.9214285714285715 loss 0.5995086616542453 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9321, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "631 Training acc 0.92 loss 0.6005289113125544 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9950, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "631 Training acc 0.9171428571428571 loss 0.6005316836404029 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9677, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "632 Training acc 0.92 loss 0.6033423788599194 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9683, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "632 Training acc 0.9171428571428571 loss 0.599921497612624 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0017, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "633 Training acc 0.91 loss 0.6030704765474606 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8801, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "633 Training acc 0.9192857142857143 loss 0.5999728934530117 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9772, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "634 Training acc 0.89 loss 0.6038448167779107 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9555, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "634 Training acc 0.9235714285714286 loss 0.5997999570500607 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9710, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "635 Training acc 0.92 loss 0.5949722381493515 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9779, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "635 Training acc 0.9171428571428571 loss 0.6016952018635597 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9062, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "636 Training acc 0.92 loss 0.5928792061268534 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0346, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "636 Training acc 0.9171428571428571 loss 0.6021382174173615 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9963, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "637 Training acc 0.9 loss 0.6030248012836947 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9009, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "637 Training acc 0.9214285714285715 loss 0.5999587698994686 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9708, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "638 Training acc 0.93 loss 0.5950434150750524 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9776, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "638 Training acc 0.915 loss 0.6016634153028981 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9601, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "639 Training acc 0.95 loss 0.5957635236190926 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9882, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "639 Training acc 0.9107142857142857 loss 0.6015041388169717 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9874, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "640 Training acc 0.92 loss 0.6015923540466271 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9164, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "640 Training acc 0.9171428571428571 loss 0.6002500568130986 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0059, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "641 Training acc 0.91 loss 0.6077854792666623 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9133, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "641 Training acc 0.9192857142857143 loss 0.5989169326884725 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9707, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "642 Training acc 0.91 loss 0.5998826753100849 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "642 Training acc 0.9192857142857143 loss 0.6006041350053901 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "643 Training acc 0.94 loss 0.5922105587108185 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9711, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "643 Training acc 0.9128571428571428 loss 0.6022398119731931 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0071, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "644 Training acc 0.91 loss 0.6114330876202841 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "644 Training acc 0.9192857142857143 loss 0.5981130683958606 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9165, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "645 Training acc 0.91 loss 0.594606648157931 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0250, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "645 Training acc 0.9192857142857143 loss 0.6017103554969842 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9477, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "646 Training acc 0.91 loss 0.5969083565776063 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9956, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "646 Training acc 0.9192857142857143 loss 0.6012107501255342 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9214, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "647 Training acc 0.9 loss 0.5988563853163352 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0067, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "647 Training acc 0.9214285714285715 loss 0.6007866063653289 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9650, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "648 Training acc 0.9 loss 0.601930289551107 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9648, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "648 Training acc 0.9214285714285715 loss 0.6001210987202559 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9637, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "649 Training acc 0.95 loss 0.593872694020675 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9933, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "649 Training acc 0.9107142857142857 loss 0.6018418294652348 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0040, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "650 Training acc 0.92 loss 0.6041848920395896 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8832, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "650 Training acc 0.9171428571428571 loss 0.5996262870039526 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9378, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "651 Training acc 0.95 loss 0.5963543470747912 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0063, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.92\n",
      "651 Training acc 0.9107142857142857 loss 0.6012983717125306 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9661, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "652 Training acc 0.93 loss 0.6024957973576884 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9664, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "652 Training acc 0.915 loss 0.5999770300120835 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9540, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "653 Training acc 0.96 loss 0.5935883607203291 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0050, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "653 Training acc 0.9085714285714286 loss 0.601880120011433 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9536, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "654 Training acc 0.9 loss 0.599149607169046 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9783, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "654 Training acc 0.9214285714285715 loss 0.6006836721963567 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9869, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "655 Training acc 0.94 loss 0.5951868592861524 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9465, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "655 Training acc 0.9128571428571428 loss 0.6015277818490368 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9868, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "656 Training acc 0.89 loss 0.6047665269886335 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9426, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "656 Training acc 0.9235714285714286 loss 0.5994694511784635 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9686, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "657 Training acc 0.93 loss 0.5961791639136598 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9740, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "657 Training acc 0.915 loss 0.6013040266844106 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9619, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "658 Training acc 0.91 loss 0.5947972325594738 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9905, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "658 Training acc 0.9192857142857143 loss 0.6015952110088246 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9201, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "659 Training acc 0.92 loss 0.5996325943324282 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0043, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "659 Training acc 0.9171428571428571 loss 0.6005539895583204 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9108, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "660 Training acc 0.92 loss 0.5977348064289604 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0155, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "660 Training acc 0.9171428571428571 loss 0.6009552805876982 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9707, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "661 Training acc 0.94 loss 0.5950927134653047 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9769, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "661 Training acc 0.9128571428571428 loss 0.6015161439831511 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "662 Training acc 0.91 loss 0.5986166561518498 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9904, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "662 Training acc 0.9192857142857143 loss 0.6007556538886794 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8899, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "663 Training acc 0.95 loss 0.5939482600136315 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0333, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "663 Training acc 0.9107142857142857 loss 0.6017504357031259 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9598, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "664 Training acc 0.93 loss 0.59590689569845 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9872, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "664 Training acc 0.915 loss 0.6013254071887797 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9823, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "665 Training acc 0.92 loss 0.5974959632440998 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9401, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "665 Training acc 0.9171428571428571 loss 0.6009798643961969 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9213, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "666 Training acc 0.9 loss 0.601090259781308 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0028, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "666 Training acc 0.9214285714285715 loss 0.6002038826654414 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9852, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "667 Training acc 0.89 loss 0.6039575623463881 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "667 Training acc 0.9235714285714286 loss 0.5995828892069727 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "668 Training acc 0.92 loss 0.6009652046817686 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9751, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "668 Training acc 0.9171428571428571 loss 0.600218011167429 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9366, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "669 Training acc 0.92 loss 0.5969805769756528 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0041, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "669 Training acc 0.9171428571428571 loss 0.601066328573711 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9972, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "670 Training acc 0.92 loss 0.5992297239984926 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8780, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "670 Training acc 0.9171428571428571 loss 0.6005794598093072 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9647, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "671 Training acc 0.91 loss 0.5982188124139504 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9680, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "671 Training acc 0.9192857142857143 loss 0.600790845999162 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9622, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "672 Training acc 0.91 loss 0.5998806451393253 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9635, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "672 Training acc 0.9192857142857143 loss 0.600428993081422 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9894, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "673 Training acc 0.93 loss 0.5974296718511317 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9225, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "673 Training acc 0.915 loss 0.6009454393193615 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "674 Training acc 0.93 loss 0.5996813023468345 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9764, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "674 Training acc 0.915 loss 0.600455076879326 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9443, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "675 Training acc 0.96 loss 0.5929049908203377 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0158, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "675 Training acc 0.9085714285714286 loss 0.6018993960139927 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9707, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "676 Training acc 0.93 loss 0.6003902245215231 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "676 Training acc 0.915 loss 0.6002875743365028 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9363, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "677 Training acc 0.91 loss 0.6028689648836812 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "677 Training acc 0.9192857142857143 loss 0.5997488579959824 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0063, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "678 Training acc 0.9 loss 0.6031131840526913 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8586, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "678 Training acc 0.9214285714285715 loss 0.5996886393961998 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9859, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "679 Training acc 0.92 loss 0.5991955571627515 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9177, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "679 Training acc 0.9171428571428571 loss 0.6005212368052983 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9790, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "680 Training acc 0.94 loss 0.6047617528120932 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9591, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "680 Training acc 0.9128571428571428 loss 0.5993219254822462 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9586, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "681 Training acc 0.93 loss 0.5964994486167556 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9851, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "681 Training acc 0.915 loss 0.6010859911171418 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "682 Training acc 0.91 loss 0.597085662896409 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0035, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "682 Training acc 0.9192857142857143 loss 0.6009548385627709 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9556, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "683 Training acc 0.93 loss 0.5981127335608865 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9805, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "683 Training acc 0.915 loss 0.6007292228990998 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "684 Training acc 0.94 loss 0.6001565437614944 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "684 Training acc 0.9128571428571428 loss 0.6002860111098653 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9457, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "685 Training acc 0.93 loss 0.5979666405299332 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9917, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "685 Training acc 0.915 loss 0.6007501416082957 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9126, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "686 Training acc 0.91 loss 0.5967396576953518 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0178, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "686 Training acc 0.9192857142857143 loss 0.6010078843390096 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9833, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "687 Training acc 0.91 loss 0.6030210134877766 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "687 Training acc 0.9192857142857143 loss 0.5996565348619126 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0207, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "688 Training acc 0.91 loss 0.6123696864518924 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9052, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "688 Training acc 0.9192857142857143 loss 0.5976477198388197 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9929, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "689 Training acc 0.92 loss 0.6012870449554377 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8976, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "689 Training acc 0.9171428571428571 loss 0.6000170461468588 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9715, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "690 Training acc 0.9 loss 0.5999462448129621 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "690 Training acc 0.9214285714285715 loss 0.6002949252214913 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9623, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "691 Training acc 0.92 loss 0.6001121179613835 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9624, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "691 Training acc 0.9171428571428571 loss 0.6002384703937315 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0092, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "692 Training acc 0.9 loss 0.6045624356152525 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "692 Training acc 0.9214285714285715 loss 0.5992700778927869 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0076, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "693 Training acc 0.89 loss 0.608607181319637 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9168, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "693 Training acc 0.9235714285714286 loss 0.5983888338882226 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9932, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "694 Training acc 0.9 loss 0.608003558924846 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9525, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "694 Training acc 0.9214285714285715 loss 0.5985061195126027 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9724, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "695 Training acc 0.92 loss 0.6057539252075468 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9767, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "695 Training acc 0.9171428571428571 loss 0.5989764235988443 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9826, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "696 Training acc 0.93 loss 0.5973519919932919 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9395, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "696 Training acc 0.915 loss 0.6007667951116367 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9580, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "697 Training acc 0.91 loss 0.603154578528765 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9820, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "697 Training acc 0.9192857142857143 loss 0.5995145106533284 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "698 Training acc 0.93 loss 0.5959861619900244 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9968, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "698 Training acc 0.915 loss 0.6010420321898942 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9428, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "699 Training acc 0.94 loss 0.6004295992491526 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9856, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "699 Training acc 0.9128571428571428 loss 0.6000822514952908 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9439, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "700 Training acc 0.91 loss 0.5989346174795733 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9884, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "700 Training acc 0.9192857142857143 loss 0.600394628157314 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9452, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "701 Training acc 0.95 loss 0.5924168468497504 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0163, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "701 Training acc 0.9107142857142857 loss 0.6017840207846357 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9867, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "702 Training acc 0.93 loss 0.6012118908229342 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9170, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "702 Training acc 0.915 loss 0.5998925759991132 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9909, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "703 Training acc 0.88 loss 0.6068385118072128 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "703 Training acc 0.9257142857142857 loss 0.5986792312720852 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9134, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "704 Training acc 0.92 loss 0.5963153130839075 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0183, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "704 Training acc 0.9171428571428571 loss 0.6009268481533657 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9853, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "705 Training acc 0.94 loss 0.5959999926832434 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "705 Training acc 0.9128571428571428 loss 0.6009887663113697 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9722, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "706 Training acc 0.94 loss 0.5943494928735603 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9777, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "706 Training acc 0.9128571428571428 loss 0.6013363908083629 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9615, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "707 Training acc 0.93 loss 0.5950122026230373 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9884, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "707 Training acc 0.915 loss 0.601188639423818 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9961, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "708 Training acc 0.91 loss 0.6028854633971527 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9023, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "708 Training acc 0.9192857142857143 loss 0.5994955854235555 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "709 Training acc 0.94 loss 0.6020069924598177 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9900, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "709 Training acc 0.9128571428571428 loss 0.5996768519303963 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9699, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "710 Training acc 0.92 loss 0.5955256539086361 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9743, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "710 Training acc 0.9171428571428571 loss 0.6010600473000978 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9950, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "711 Training acc 0.91 loss 0.6023378875857099 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9009, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "711 Training acc 0.9192857142857143 loss 0.5995947453570767 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9864, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "712 Training acc 0.93 loss 0.5914855299383427 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9712, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "712 Training acc 0.915 loss 0.6019144598443746 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9628, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "713 Training acc 0.97 loss 0.59433714855583 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9902, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "713 Training acc 0.9064285714285714 loss 0.6012983093832258 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9091, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "714 Training acc 0.94 loss 0.591289313889446 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0372, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "714 Training acc 0.9128571428571428 loss 0.6019460766782678 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9529, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "715 Training acc 0.9 loss 0.6004113271902508 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9750, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "715 Training acc 0.9214285714285715 loss 0.5999860312592648 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0028, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "716 Training acc 0.89 loss 0.603600661526392 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8834, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "716 Training acc 0.9235714285714286 loss 0.5992966932775984 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9357, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "717 Training acc 0.92 loss 0.5974493384542305 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0013, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "717 Training acc 0.9171428571428571 loss 0.6006089044501431 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "718 Training acc 0.9 loss 0.5997916491033152 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9625, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "718 Training acc 0.9214285714285715 loss 0.6001018236489258 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9592, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "719 Training acc 0.91 loss 0.6038167961907992 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9844, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "719 Training acc 0.9192857142857143 loss 0.5992327385983328 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9079, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "720 Training acc 0.93 loss 0.6005790172601906 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0087, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "720 Training acc 0.915 loss 0.5999200069294942 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0043, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "721 Training acc 0.94 loss 0.6043540413860654 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8854, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "721 Training acc 0.9128571428571428 loss 0.5991048540579693 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9656, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "722 Training acc 0.9 loss 0.6022811430547548 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9677, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "722 Training acc 0.9214285714285715 loss 0.5995419592702846 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9873, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "723 Training acc 0.89 loss 0.6050582337167201 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9452, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "723 Training acc 0.9235714285714286 loss 0.5989406515907294 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9700, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "724 Training acc 0.9 loss 0.5954730066743917 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9741, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "724 Training acc 0.9214285714285715 loss 0.6009889525909484 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0098, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "725 Training acc 0.89 loss 0.6030370892958534 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8354, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "725 Training acc 0.9235714285714286 loss 0.5993626761997279 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "726 Training acc 0.93 loss 0.6003491469505697 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9751, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "726 Training acc 0.915 loss 0.5999325608544106 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "727 Training acc 0.91 loss 0.5972310241617337 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9925, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "727 Training acc 0.9192857142857143 loss 0.6005955664887026 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9641, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "728 Training acc 0.9 loss 0.6063095317896201 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9917, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "728 Training acc 0.9214285714285715 loss 0.5986450085078618 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9714, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "729 Training acc 0.95 loss 0.6052385329724888 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9761, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "729 Training acc 0.9107142857142857 loss 0.5988689865552916 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9508, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "730 Training acc 0.92 loss 0.5953025279247822 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9980, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "730 Training acc 0.9171428571428571 loss 0.6009923326264516 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0041, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "731 Training acc 0.9 loss 0.6042752593882189 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8854, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "731 Training acc 0.9214285714285715 loss 0.5990648283667934 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9960, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "732 Training acc 0.93 loss 0.6028520903500101 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9027, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "732 Training acc 0.915 loss 0.5993636785048422 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9337, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "733 Training acc 0.93 loss 0.5985497603537618 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9978, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "733 Training acc 0.915 loss 0.6002792254494236 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "734 Training acc 0.91 loss 0.597051953992399 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9929, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "734 Training acc 0.9192857142857143 loss 0.6005947677852537 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9239, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "735 Training acc 0.93 loss 0.5974707210248328 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0084, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "735 Training acc 0.915 loss 0.6004997744905256 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8852, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "736 Training acc 0.94 loss 0.596631353798328 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0233, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "736 Training acc 0.9128571428571428 loss 0.6006737991359765 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9780, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "737 Training acc 0.88 loss 0.6042402710743867 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9592, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "737 Training acc 0.9257142857142857 loss 0.5990382247410038 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0032, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "738 Training acc 0.9 loss 0.6064675269959467 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9122, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "738 Training acc 0.9214285714285715 loss 0.598555028041803 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9869, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "739 Training acc 0.92 loss 0.6048549096212468 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9450, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "739 Training acc 0.9171428571428571 loss 0.5988938172945806 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9835, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "740 Training acc 0.93 loss 0.6030862657856512 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9402, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "740 Training acc 0.915 loss 0.5992668100604348 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0119, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "741 Training acc 0.91 loss 0.6080935451080001 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8954, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "741 Training acc 0.9192857142857143 loss 0.5981881660227228 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9981, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "742 Training acc 0.94 loss 0.6039089309654807 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9056, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "742 Training acc 0.9128571428571428 loss 0.5990783902192989 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9954, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "743 Training acc 0.91 loss 0.6025299540617595 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9021, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "743 Training acc 0.9192857142857143 loss 0.5993683850663005 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9563, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "744 Training acc 0.91 loss 0.59241290751523 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0060, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "744 Training acc 0.9192857142857143 loss 0.6015305880071273 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9637, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "745 Training acc 0.91 loss 0.5938614549692948 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9908, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "745 Training acc 0.9192857142857143 loss 0.601215643088824 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0028, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "746 Training acc 0.91 loss 0.606240063865799 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9118, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "746 Training acc 0.9192857142857143 loss 0.5985581379657459 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9663, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "747 Training acc 0.92 loss 0.602619488355314 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9692, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "747 Training acc 0.9171428571428571 loss 0.599327538196036 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9981, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "748 Training acc 0.88 loss 0.6039012459753387 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9057, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "748 Training acc 0.9257142857142857 loss 0.599047286827 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9605, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "749 Training acc 0.92 loss 0.604467681449759 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9870, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "749 Training acc 0.9171428571428571 loss 0.5989200852852421 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9806, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "750 Training acc 0.92 loss 0.6016284667081359 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9365, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "750 Training acc 0.9171428571428571 loss 0.5995228160208617 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "751 Training acc 0.93 loss 0.5992064296044889 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9865, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "751 Training acc 0.915 loss 0.6000364928079438 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9953, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "752 Training acc 0.89 loss 0.6055725919484084 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9296, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "752 Training acc 0.9235714285714286 loss 0.5986674401240346 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "753 Training acc 0.94 loss 0.5925300429796554 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "753 Training acc 0.9128571428571428 loss 0.601455919255018 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9715, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "754 Training acc 0.9 loss 0.6008889047513459 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9505, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "754 Training acc 0.9214285714285715 loss 0.5996601809445615 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9269, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "755 Training acc 0.94 loss 0.5958308265621277 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0127, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "755 Training acc 0.9128571428571428 loss 0.600737938585365 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9227, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "756 Training acc 0.93 loss 0.5981387317492801 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0060, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "756 Training acc 0.915 loss 0.6002383594284697 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9637, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "757 Training acc 0.93 loss 0.5938639496291833 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9906, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "757 Training acc 0.915 loss 0.6011496140366904 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9213, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "758 Training acc 0.91 loss 0.5920266931894672 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0297, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "758 Training acc 0.9192857142857143 loss 0.6015374226748359 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9543, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "759 Training acc 0.91 loss 0.5934478505863463 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0027, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "759 Training acc 0.9192857142857143 loss 0.6012274751040552 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9734, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "760 Training acc 0.9 loss 0.601903349037257 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9533, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "760 Training acc 0.9214285714285715 loss 0.5994098928723214 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "761 Training acc 0.92 loss 0.5980000944669569 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9896, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "761 Training acc 0.9171428571428571 loss 0.600239764101881 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9633, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "762 Training acc 0.89 loss 0.6010542430694824 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9652, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "762 Training acc 0.9235714285714286 loss 0.5995798339870253 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "763 Training acc 0.92 loss 0.6024103933661258 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9923, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "763 Training acc 0.9171428571428571 loss 0.5992822183039636 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0164, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "764 Training acc 0.92 loss 0.6062758226527258 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8439, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "764 Training acc 0.9171428571428571 loss 0.5984474377132626 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9709, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "765 Training acc 0.91 loss 0.600526723579456 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "765 Training acc 0.9192857142857143 loss 0.5996719037771531 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9836, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "766 Training acc 0.92 loss 0.5968567765982317 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9393, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "766 Training acc 0.9171428571428571 loss 0.6004525609843017 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9774, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "767 Training acc 0.9 loss 0.603915545440547 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9590, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "767 Training acc 0.9214285714285715 loss 0.5989340534703982 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9278, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "768 Training acc 0.92 loss 0.5953823647320897 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0136, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "768 Training acc 0.9171428571428571 loss 0.6007563876539439 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "769 Training acc 0.91 loss 0.60163093512522 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9903, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "769 Training acc 0.9192857142857143 loss 0.5994116069865372 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9886, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "770 Training acc 0.92 loss 0.594339070142868 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9460, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "770 Training acc 0.9171428571428571 loss 0.6009684470313064 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9790, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "771 Training acc 0.97 loss 0.5908656398344634 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9861, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "771 Training acc 0.9064285714285714 loss 0.6017074240453284 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9101, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "772 Training acc 0.88 loss 0.6018861793587961 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0136, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "772 Training acc 0.9257142857142857 loss 0.5993403887072384 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9917, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "773 Training acc 0.9 loss 0.6072443353051834 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9523, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "773 Training acc 0.9214285714285715 loss 0.5981854701478324 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9858, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "774 Training acc 0.91 loss 0.5992518374145314 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9154, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "774 Training acc 0.9192857142857143 loss 0.5998924149193701 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9686, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "775 Training acc 0.96 loss 0.5913399483478111 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9974, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "775 Training acc 0.9085714285714286 loss 0.6015825842335926 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "776 Training acc 0.91 loss 0.5997682943881707 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "776 Training acc 0.9192857142857143 loss 0.599771019526917 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9529, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "777 Training acc 0.93 loss 0.5996180576694217 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9776, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "777 Training acc 0.915 loss 0.5997972008505352 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9897, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "778 Training acc 0.93 loss 0.5972797830801851 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9202, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "778 Training acc 0.915 loss 0.6002313987789121 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9394, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "779 Training acc 0.93 loss 0.6045018213426417 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0085, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "779 Training acc 0.915 loss 0.5986041293436013 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9721, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "780 Training acc 0.9 loss 0.6056214712430941 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9791, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "780 Training acc 0.9214285714285715 loss 0.5983003262512836 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9375, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "781 Training acc 0.94 loss 0.5964875604894556 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0016, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "781 Training acc 0.9128571428571428 loss 0.600203275784446 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9377, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "782 Training acc 0.94 loss 0.5963984484869059 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0017, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "782 Training acc 0.9128571428571428 loss 0.6001753593945411 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9837, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "783 Training acc 0.9 loss 0.6071388035661879 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9695, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "783 Training acc 0.9214285714285715 loss 0.5978381908084367 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9856, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "784 Training acc 0.9 loss 0.6041757604218527 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9454, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "784 Training acc 0.9214285714285715 loss 0.5984406715742419 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0049, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "785 Training acc 0.92 loss 0.6000999236978538 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "785 Training acc 0.9171428571428571 loss 0.5992869664642952 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9161, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "786 Training acc 0.95 loss 0.5948529909451662 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0191, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "786 Training acc 0.9107142857142857 loss 0.6003945308906532 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9960, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "787 Training acc 0.86 loss 0.6094124493672358 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9601, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "787 Training acc 0.9299999999999999 loss 0.5972601379484948 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9846, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "788 Training acc 0.92 loss 0.603674817750871 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9444, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "788 Training acc 0.9171428571428571 loss 0.5984763305690056 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9943, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "789 Training acc 0.9 loss 0.6020069509879944 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9031, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "789 Training acc 0.9214285714285715 loss 0.5988218747811743 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9635, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "790 Training acc 0.92 loss 0.6011356651315508 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9676, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "790 Training acc 0.9171428571428571 loss 0.5989966484280635 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9775, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "791 Training acc 0.92 loss 0.5916393386135894 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9818, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "791 Training acc 0.9171428571428571 loss 0.6010227200447598 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9815, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "792 Training acc 0.92 loss 0.5979476081058973 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "792 Training acc 0.9171428571428571 loss 0.599663040815088 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9592, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "793 Training acc 0.91 loss 0.5962171155007995 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9814, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "793 Training acc 0.9192857142857143 loss 0.6000260357717679 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9534, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "794 Training acc 0.92 loss 0.6007163677861689 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9791, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "794 Training acc 0.9171428571428571 loss 0.5990543622165074 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "795 Training acc 0.9 loss 0.6003436125423944 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9781, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "795 Training acc 0.9214285714285715 loss 0.5991260311333826 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9771, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "796 Training acc 0.9 loss 0.6037701684689676 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9608, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "796 Training acc 0.9214285714285715 loss 0.5983847536169208 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9873, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "797 Training acc 0.9 loss 0.6050530247937447 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9484, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "797 Training acc 0.9214285714285715 loss 0.5981027440758255 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0108, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "798 Training acc 0.92 loss 0.6101881998016586 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9250, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "798 Training acc 0.9171428571428571 loss 0.5969950484944964 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9127, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "799 Training acc 0.9 loss 0.5967075341988213 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0132, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "799 Training acc 0.9214285714285715 loss 0.5998767789145635 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9781, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "800 Training acc 0.89 loss 0.6086604372785864 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9891, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "800 Training acc 0.9235714285714286 loss 0.5973094410693676 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9215, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "801 Training acc 0.9 loss 0.6012344763513745 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0082, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "801 Training acc 0.9214285714285715 loss 0.5988936691890172 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9716, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "802 Training acc 0.9 loss 0.6053597099467483 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9798, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "802 Training acc 0.9214285714285715 loss 0.5980032397115018 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "803 Training acc 0.94 loss 0.5942267953136827 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9977, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "803 Training acc 0.9128571428571428 loss 0.6003831651653397 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "804 Training acc 0.94 loss 0.6029055159368111 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0056, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "804 Training acc 0.9128571428571428 loss 0.5985179999437773 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9991, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "805 Training acc 0.91 loss 0.6017682721080475 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8821, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "805 Training acc 0.9192857142857143 loss 0.5987551438449226 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9599, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "806 Training acc 0.89 loss 0.6041599492849768 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9891, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "806 Training acc 0.9235714285714286 loss 0.5982366313973949 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0183, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "807 Training acc 0.91 loss 0.6058081780191978 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8189, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "807 Training acc 0.9192857142857143 loss 0.597877255266932 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9913, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "808 Training acc 0.92 loss 0.5964389009811809 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9204, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "808 Training acc 0.9171428571428571 loss 0.5998790522150538 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9555, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "809 Training acc 0.9 loss 0.601860658808026 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9827, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "809 Training acc 0.9214285714285715 loss 0.5987115633531503 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9631, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "810 Training acc 0.9 loss 0.6009662729422596 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9677, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "810 Training acc 0.9214285714285715 loss 0.5988965998636706 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0005, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "811 Training acc 0.91 loss 0.6051242514647964 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9118, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "811 Training acc 0.9192857142857143 loss 0.597999737471953 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9802, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "812 Training acc 0.92 loss 0.598621128771405 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "812 Training acc 0.9171428571428571 loss 0.5993874074712974 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9347, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "813 Training acc 0.93 loss 0.5979923392526433 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9959, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "813 Training acc 0.915 loss 0.5995186587730743 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9715, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "814 Training acc 0.94 loss 0.5991080929357143 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "814 Training acc 0.9128571428571428 loss 0.5992766251258337 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9335, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "815 Training acc 0.92 loss 0.5986482754922898 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9951, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "815 Training acc 0.9171428571428571 loss 0.5993728290735838 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9537, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "816 Training acc 0.92 loss 0.5991356461849783 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9752, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "816 Training acc 0.9171428571428571 loss 0.5992711128867487 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9589, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "817 Training acc 0.93 loss 0.5963570839055932 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9806, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "817 Training acc 0.915 loss 0.5998713081758105 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0115, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "818 Training acc 0.9 loss 0.6038977174410064 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8405, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "818 Training acc 0.9214285714285715 loss 0.5982585411479711 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9329, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "819 Training acc 0.93 loss 0.6000572421736629 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9976, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "819 Training acc 0.915 loss 0.5990851245685631 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9960, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "820 Training acc 0.91 loss 0.594102151117855 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9266, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "820 Training acc 0.9192857142857143 loss 0.6003858250357238 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9719, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "821 Training acc 0.89 loss 0.6055137022178115 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9802, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "821 Training acc 0.9235714285714286 loss 0.5979576711775658 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9966, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "822 Training acc 0.87 loss 0.606212666244398 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9339, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "822 Training acc 0.9278571428571429 loss 0.5978206856569475 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9516, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "823 Training acc 0.93 loss 0.5948754445773929 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9959, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "823 Training acc 0.915 loss 0.6002584487523126 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9432, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "824 Training acc 0.92 loss 0.6000801840276909 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9883, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "824 Training acc 0.9171428571428571 loss 0.5991518161013178 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9713, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "825 Training acc 0.89 loss 0.605198192406869 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9792, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "825 Training acc 0.9235714285714286 loss 0.5980708648866588 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "826 Training acc 0.94 loss 0.5997036570370532 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9871, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "826 Training acc 0.9128571428571428 loss 0.5992589778618527 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9754, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "827 Training acc 0.93 loss 0.6029185295174349 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9584, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "827 Training acc 0.915 loss 0.5985794347621806 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9729, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "828 Training acc 0.92 loss 0.5983831468333868 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "828 Training acc 0.9171428571428571 loss 0.5995579916298598 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "829 Training acc 0.92 loss 0.6035100022493809 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9978, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "829 Training acc 0.9171428571428571 loss 0.5984638999285183 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8947, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "830 Training acc 0.91 loss 0.6006753122108324 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0164, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "830 Training acc 0.9192857142857143 loss 0.5990733257359371 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9585, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "831 Training acc 0.91 loss 0.5966012528045294 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9804, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "831 Training acc 0.9192857142857143 loss 0.5999471253954816 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "832 Training acc 0.91 loss 0.606155230682189 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9674, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "832 Training acc 0.9192857142857143 loss 0.5978994413171548 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9815, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "833 Training acc 0.92 loss 0.602104250409482 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9403, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "833 Training acc 0.9171428571428571 loss 0.5987654623039953 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0051, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "834 Training acc 0.91 loss 0.6047461853729452 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8893, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "834 Training acc 0.9192857142857143 loss 0.5981963610880419 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9782, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "835 Training acc 0.91 loss 0.5997584059986709 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9341, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "835 Training acc 0.9192857142857143 loss 0.5992620449380848 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9841, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "836 Training acc 0.92 loss 0.5965813299023693 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "836 Training acc 0.9171428571428571 loss 0.5999405871596414 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "837 Training acc 0.93 loss 0.5961334085419461 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9391, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "837 Training acc 0.915 loss 0.6000340221091331 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9517, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "838 Training acc 0.9 loss 0.6052148700380857 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0028, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "838 Training acc 0.9214285714285715 loss 0.598084350941203 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9575, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "839 Training acc 0.92 loss 0.5970790613365842 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9790, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "839 Training acc 0.9171428571428571 loss 0.5998231463975374 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9531, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "840 Training acc 0.9 loss 0.6000930714174216 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9774, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "840 Training acc 0.9214285714285715 loss 0.5991739751715266 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9044, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "841 Training acc 0.93 loss 0.5938534527575242 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0258, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "841 Training acc 0.915 loss 0.6005130866883619 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9933, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "842 Training acc 0.91 loss 0.6045710914292033 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9294, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "842 Training acc 0.9192857142857143 loss 0.5982173781413203 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0031, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "843 Training acc 0.94 loss 0.5905705407058909 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "843 Training acc 0.9128571428571428 loss 0.601216844679834 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0064, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "844 Training acc 0.9 loss 0.6031557635264924 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8627, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "844 Training acc 0.9214285714285715 loss 0.5985185088474542 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9355, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "845 Training acc 0.93 loss 0.5912796029422597 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0233, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "845 Training acc 0.915 loss 0.601060224817409 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9714, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "846 Training acc 0.9 loss 0.5992056972719838 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "846 Training acc 0.9214285714285715 loss 0.59935938325368 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0140, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "847 Training acc 0.91 loss 0.6050976712870897 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "847 Training acc 0.9192857142857143 loss 0.5980947296509089 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9650, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "848 Training acc 0.97 loss 0.5879085156610896 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0162, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "848 Training acc 0.9064285714285714 loss 0.6017743522264649 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9858, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "849 Training acc 0.93 loss 0.6043010703513676 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "849 Training acc 0.915 loss 0.5982590697154753 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "850 Training acc 0.92 loss 0.5997144974520928 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "850 Training acc 0.9171428571428571 loss 0.5992369885151904 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9474, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "851 Training acc 0.92 loss 0.5970815355857308 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9897, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "851 Training acc 0.9171428571428571 loss 0.5997977642319336 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9466, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "852 Training acc 0.88 loss 0.6025355568237015 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9952, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "852 Training acc 0.9257142857142857 loss 0.5986255799171974 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9148, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "853 Training acc 0.91 loss 0.6044335112432471 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0234, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "853 Training acc 0.9192857142857143 loss 0.5982139525077554 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9811, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "854 Training acc 0.92 loss 0.598131988478494 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9336, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "854 Training acc 0.9171428571428571 loss 0.599559296538014 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9985, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "855 Training acc 0.94 loss 0.592840743574595 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9301, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "855 Training acc 0.9128571428571428 loss 0.6006884345256172 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9786, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "856 Training acc 0.92 loss 0.6045569926228735 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9632, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "856 Training acc 0.9171428571428571 loss 0.5981724568256772 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9324, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "857 Training acc 0.94 loss 0.6007629812758529 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9994, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "857 Training acc 0.9128571428571428 loss 0.5989793511331862 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9885, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "858 Training acc 0.91 loss 0.602141649413746 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9231, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "858 Training acc 0.9192857142857143 loss 0.5986771359539578 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0037, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "859 Training acc 0.94 loss 0.5933118994509713 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9094, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "859 Training acc 0.9128571428571428 loss 0.6005632199853033 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9245, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "860 Training acc 0.91 loss 0.5971535436182275 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0059, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "860 Training acc 0.9192857142857143 loss 0.5997350158548105 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9658, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "861 Training acc 0.93 loss 0.5927922112255827 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9908, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "861 Training acc 0.915 loss 0.6006637124438303 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9974, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "862 Training acc 0.9 loss 0.6035766497647951 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9077, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "862 Training acc 0.9214285714285715 loss 0.5983462598869707 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9741, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "863 Training acc 0.9 loss 0.6022447234051406 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9570, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "863 Training acc 0.9214285714285715 loss 0.5986250683775773 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "864 Training acc 0.92 loss 0.596604905862197 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9999, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "864 Training acc 0.9171428571428571 loss 0.599826916883647 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9639, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "865 Training acc 0.91 loss 0.6013843392437199 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9689, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "865 Training acc 0.9192857142857143 loss 0.5987965435963519 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9943, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "866 Training acc 0.91 loss 0.6050793729789187 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9311, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "866 Training acc 0.9192857142857143 loss 0.5979978625983227 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9820, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "867 Training acc 0.91 loss 0.6023678413757317 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "867 Training acc 0.9192857142857143 loss 0.5985720517685078 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9940, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "868 Training acc 0.89 loss 0.6049282421981859 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "868 Training acc 0.9235714285714286 loss 0.5980165310174965 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9972, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "869 Training acc 0.9 loss 0.6065338451833893 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9351, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "869 Training acc 0.9214285714285715 loss 0.5976654269650266 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9834, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "870 Training acc 0.92 loss 0.606969965202583 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9703, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "870 Training acc 0.9171428571428571 loss 0.5975653072579628 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9752, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "871 Training acc 0.92 loss 0.5928029186214829 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9778, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "871 Training acc 0.9171428571428571 loss 0.6005951682086728 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "872 Training acc 0.92 loss 0.6017497263046668 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9700, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "872 Training acc 0.9171428571428571 loss 0.5986720243188525 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9740, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "873 Training acc 0.9 loss 0.5978201927135508 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "873 Training acc 0.9214285714285715 loss 0.5995079204337701 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9753, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "874 Training acc 0.92 loss 0.597119926313079 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9516, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "874 Training acc 0.9171428571428571 loss 0.599652563238814 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "875 Training acc 0.91 loss 0.6002272985034898 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9784, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "875 Training acc 0.9192857142857143 loss 0.598980768222492 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "876 Training acc 0.89 loss 0.5981945167455384 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9952, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "876 Training acc 0.9235714285714286 loss 0.5994111919096299 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9618, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "877 Training acc 0.92 loss 0.594884761077958 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9844, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "877 Training acc 0.9171428571428571 loss 0.60011581398775 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9828, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "878 Training acc 0.93 loss 0.5933218738748941 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9619, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "878 Training acc 0.915 loss 0.6004459454480306 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "879 Training acc 0.91 loss 0.5996721103394016 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9769, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "879 Training acc 0.9192857142857143 loss 0.5990800141776143 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9535, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "880 Training acc 0.92 loss 0.599242595171841 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9758, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "880 Training acc 0.9171428571428571 loss 0.5991681034482692 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9929, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "881 Training acc 0.92 loss 0.6012757795692057 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9021, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "881 Training acc 0.9171428571428571 loss 0.5987297826738852 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "882 Training acc 0.92 loss 0.5934200184316866 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9885, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "882 Training acc 0.9171428571428571 loss 0.6004100893905825 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9215, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "883 Training acc 0.92 loss 0.5987827992639444 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0021, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "883 Training acc 0.9171428571428571 loss 0.5992579828179263 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9835, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "884 Training acc 0.9 loss 0.607041733400562 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9707, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "884 Training acc 0.9214285714285715 loss 0.5974852232601482 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9626, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "885 Training acc 0.91 loss 0.6006452520867367 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9672, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "885 Training acc 0.9192857142857143 loss 0.5988517498119496 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9809, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "886 Training acc 0.88 loss 0.6057321551200796 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9671, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "886 Training acc 0.9257142857142857 loss 0.5977570876591818 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9734, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "887 Training acc 0.9 loss 0.5981097390018372 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "887 Training acc 0.9214285714285715 loss 0.5993858905469618 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9622, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "888 Training acc 0.94 loss 0.6003945406773659 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9666, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "888 Training acc 0.9128571428571428 loss 0.5988919441856932 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9342, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "889 Training acc 0.95 loss 0.5919585876011699 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0204, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "889 Training acc 0.9107142857142857 loss 0.6006944081856889 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9643, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "890 Training acc 0.9 loss 0.6015643997178286 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9698, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "890 Training acc 0.9214285714285715 loss 0.5986309436847327 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9918, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "891 Training acc 0.91 loss 0.600702527652222 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9008, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "891 Training acc 0.9192857142857143 loss 0.5988097477955305 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9201, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "892 Training acc 0.91 loss 0.5996514299566618 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0045, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "892 Training acc 0.9192857142857143 loss 0.5990291317987995 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9004, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "893 Training acc 0.95 loss 0.5960932572799073 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0183, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "893 Training acc 0.9107142857142857 loss 0.5997876704641187 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9761, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "894 Training acc 0.95 loss 0.5875243174220833 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0053, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "894 Training acc 0.9107142857142857 loss 0.6016202785555851 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9639, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "895 Training acc 0.93 loss 0.6062435917885803 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9958, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "895 Training acc 0.915 loss 0.5976042052491681 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9254, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "896 Training acc 0.93 loss 0.5966579594824839 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0066, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "896 Training acc 0.915 loss 0.59965262409673 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9791, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "897 Training acc 0.93 loss 0.6008191569436367 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "897 Training acc 0.915 loss 0.5987557932196418 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8699, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "898 Training acc 0.94 loss 0.5876165195435897 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0459, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "898 Training acc 0.9128571428571428 loss 0.6015784706822083 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9949, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "899 Training acc 0.89 loss 0.6053884829633135 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "899 Training acc 0.9235714285714286 loss 0.59776522167325 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9864, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "900 Training acc 0.92 loss 0.595430688959416 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "900 Training acc 0.9171428571428571 loss 0.5998930458217544 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "901 Training acc 0.91 loss 0.5990817587002257 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9757, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "901 Training acc 0.9192857142857143 loss 0.5991052905371986 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9746, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "902 Training acc 0.94 loss 0.5974847865282016 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9501, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "902 Training acc 0.9128571428571428 loss 0.5994431864457541 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9923, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "903 Training acc 0.92 loss 0.5959426974313026 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9209, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "903 Training acc 0.9171428571428571 loss 0.5997698292149368 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9542, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "904 Training acc 0.92 loss 0.598834174228231 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9751, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "904 Training acc 0.9171428571428571 loss 0.59914553737149 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9718, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "905 Training acc 0.94 loss 0.5945526916110254 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9723, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "905 Training acc 0.9128571428571428 loss 0.6000594012753111 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9538, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "906 Training acc 0.94 loss 0.6009225704799783 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9809, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "906 Training acc 0.9128571428571428 loss 0.5986905337476569 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9649, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "907 Training acc 0.94 loss 0.5981044288810514 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9626, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "907 Training acc 0.9128571428571428 loss 0.5992896394478622 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "908 Training acc 0.92 loss 0.5965848884579752 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9899, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "908 Training acc 0.9171428571428571 loss 0.5996108193262886 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9537, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "909 Training acc 0.92 loss 0.599133330474017 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9760, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "909 Training acc 0.9171428571428571 loss 0.5990602182167262 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9612, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "910 Training acc 0.92 loss 0.5951944853922914 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9830, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "910 Training acc 0.9171428571428571 loss 0.599900557364917 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "911 Training acc 0.94 loss 0.5933891792060481 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9881, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "911 Training acc 0.9128571428571428 loss 0.6002837997683621 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9595, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "912 Training acc 0.92 loss 0.5907267052986177 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0067, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "912 Training acc 0.9171428571428571 loss 0.6008501812763775 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9266, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "913 Training acc 0.93 loss 0.5960319337938685 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0081, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "913 Training acc 0.915 loss 0.5997085175295256 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9552, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "914 Training acc 0.9 loss 0.6016913264064166 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9832, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "914 Training acc 0.9214285714285715 loss 0.598490051774232 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9784, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "915 Training acc 0.92 loss 0.6001164227207108 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9363, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "915 Training acc 0.9171428571428571 loss 0.5988222245557104 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9883, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "916 Training acc 0.92 loss 0.5979660539202911 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9155, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "916 Training acc 0.9171428571428571 loss 0.5992819983661279 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9936, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "917 Training acc 0.91 loss 0.6016317494806372 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9036, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "917 Training acc 0.9192857142857143 loss 0.5984946376363206 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9435, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "918 Training acc 0.89 loss 0.6008620863169303 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9917, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "918 Training acc 0.9235714285714286 loss 0.5986559363939437 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9563, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "919 Training acc 0.92 loss 0.5977288160883597 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9758, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "919 Training acc 0.9171428571428571 loss 0.5993236927737414 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9771, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "920 Training acc 0.89 loss 0.5962270937143461 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9533, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "920 Training acc 0.9235714285714286 loss 0.5996420357263688 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9213, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "921 Training acc 0.91 loss 0.5989043054820603 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0029, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "921 Training acc 0.9192857142857143 loss 0.5990643080195976 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9462, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "922 Training acc 0.94 loss 0.597700485998207 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9866, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "922 Training acc 0.9128571428571428 loss 0.5993193403994217 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "923 Training acc 0.94 loss 0.5943837529338335 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0052, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "923 Training acc 0.9128571428571428 loss 0.6000271747574072 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9912, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "924 Training acc 0.9 loss 0.6034880587453862 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9279, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "924 Training acc 0.9214285714285715 loss 0.5980723040461049 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9768, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "925 Training acc 0.96 loss 0.5871841850614571 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0057, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "925 Training acc 0.9085714285714286 loss 0.601560970431533 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9673, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "926 Training acc 0.9 loss 0.6031433979384424 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9749, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "926 Training acc 0.9214285714285715 loss 0.5981361937920153 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "927 Training acc 0.94 loss 0.5942486161433886 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9963, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "927 Training acc 0.9128571428571428 loss 0.6000365876518248 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "928 Training acc 0.92 loss 0.603840055798528 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0004, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "928 Training acc 0.9171428571428571 loss 0.5979756834703326 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9457, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "929 Training acc 0.92 loss 0.6020535969821876 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9953, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "929 Training acc 0.9171428571428571 loss 0.5983522857287977 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9368, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "930 Training acc 0.92 loss 0.59684476782054 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9980, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "930 Training acc 0.9171428571428571 loss 0.5994622941302161 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9920, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "931 Training acc 0.92 loss 0.6007972074171076 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9017, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "931 Training acc 0.9171428571428571 loss 0.5986096395551277 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9627, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "932 Training acc 0.88 loss 0.6007352963454506 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9683, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "932 Training acc 0.9257142857142857 loss 0.5986154563925901 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9804, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "933 Training acc 0.89 loss 0.5984741772409593 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "933 Training acc 0.9235714285714286 loss 0.5990940049770315 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0026, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "934 Training acc 0.91 loss 0.6035010573133971 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8878, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "934 Training acc 0.9192857142857143 loss 0.5980120696701219 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9336, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "935 Training acc 0.93 loss 0.5985701304751236 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9949, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "935 Training acc 0.915 loss 0.599063722586954 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9838, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "936 Training acc 0.91 loss 0.5967428977741894 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "936 Training acc 0.9192857142857143 loss 0.5994524959140084 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9671, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "937 Training acc 0.91 loss 0.6030232941955377 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "937 Training acc 0.9192857142857143 loss 0.5981030613678793 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9579, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "938 Training acc 0.91 loss 0.6031243277177952 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9877, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "938 Training acc 0.9192857142857143 loss 0.5980764881649339 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9791, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "939 Training acc 0.92 loss 0.5952130049430383 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9557, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "939 Training acc 0.9171428571428571 loss 0.5997667940769336 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9998, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "940 Training acc 0.92 loss 0.5952705832953116 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9029, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "940 Training acc 0.9171428571428571 loss 0.5997497225901909 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9503, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "941 Training acc 0.91 loss 0.6044712633732066 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0025, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "941 Training acc 0.9192857142857143 loss 0.597772747619188 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9773, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "942 Training acc 0.9 loss 0.6039006923130968 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9630, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "942 Training acc 0.9214285714285715 loss 0.5978876931329491 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9770, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "943 Training acc 0.92 loss 0.5962452382149066 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "943 Training acc 0.9171428571428571 loss 0.5995221619408891 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9710, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "944 Training acc 0.93 loss 0.5949381242947513 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9705, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "944 Training acc 0.915 loss 0.599796771859306 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0244, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "945 Training acc 0.89 loss 0.6072082322154048 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7658, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "945 Training acc 0.9235714285714286 loss 0.5971611344573844 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9147, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "946 Training acc 0.93 loss 0.5956133708929846 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0145, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "946 Training acc 0.915 loss 0.5996386419591044 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9725, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "947 Training acc 0.91 loss 0.6014033092160406 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9563, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "947 Training acc 0.9192857142857143 loss 0.5983912290373076 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9722, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "948 Training acc 0.9 loss 0.6012435460790753 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9558, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "948 Training acc 0.9214285714285715 loss 0.5984182464096985 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9732, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "949 Training acc 0.92 loss 0.6017770671848587 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9573, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "949 Training acc 0.9171428571428571 loss 0.5982965059788516 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9703, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "950 Training acc 0.93 loss 0.5953213213844744 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9693, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "950 Training acc 0.915 loss 0.5996731124320515 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9366, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "951 Training acc 0.95 loss 0.5970006709032815 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9971, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "951 Training acc 0.9107142857142857 loss 0.5993069921199847 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9629, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "952 Training acc 0.9 loss 0.599190984853998 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9645, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "952 Training acc 0.9214285714285715 loss 0.5988314038035849 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9919, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "953 Training acc 0.91 loss 0.6007324244228985 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9020, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "953 Training acc 0.9192857142857143 loss 0.5984962751141084 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9345, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "954 Training acc 0.93 loss 0.6019167128066297 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0047, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "954 Training acc 0.915 loss 0.5982367225575542 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0060, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "955 Training acc 0.9 loss 0.605182533315133 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8925, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "955 Training acc 0.9214285714285715 loss 0.5975312871719219 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9794, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "956 Training acc 0.91 loss 0.6009936265147631 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9395, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "956 Training acc 0.9192857142857143 loss 0.5984227496657608 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9539, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "957 Training acc 0.93 loss 0.5990180073230067 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9766, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "957 Training acc 0.915 loss 0.5988400361899227 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9767, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "958 Training acc 0.91 loss 0.5964016288053253 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9520, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "958 Training acc 0.9192857142857143 loss 0.599397330969696 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9540, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "959 Training acc 0.91 loss 0.5989355895634584 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9764, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "959 Training acc 0.9192857142857143 loss 0.5988507275244643 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9809, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "960 Training acc 0.91 loss 0.5982461370751492 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "960 Training acc 0.9192857142857143 loss 0.5989963668101406 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9621, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "961 Training acc 0.9 loss 0.6001425966723795 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9673, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "961 Training acc 0.9214285714285715 loss 0.5985887992757506 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9668, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "962 Training acc 0.92 loss 0.602873186741463 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9748, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "962 Training acc 0.9171428571428571 loss 0.5980039493489127 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9381, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "963 Training acc 0.94 loss 0.5961619291599911 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9992, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "963 Training acc 0.9128571428571428 loss 0.599440891096837 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0018, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "964 Training acc 0.88 loss 0.5969059996805862 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8775, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "964 Training acc 0.9257142857142857 loss 0.5992801828521623 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9922, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "965 Training acc 0.92 loss 0.599074135558083 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8979, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "965 Training acc 0.9171428571428571 loss 0.5988125106276854 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9850, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "966 Training acc 0.92 loss 0.596113997447603 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9369, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "966 Training acc 0.9171428571428571 loss 0.5994451403372605 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9769, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "967 Training acc 0.91 loss 0.60366624084135 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9628, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "967 Training acc 0.9192857142857143 loss 0.5978241150509941 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9351, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "968 Training acc 0.95 loss 0.5977978146264522 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9948, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "968 Training acc 0.9107142857142857 loss 0.5990775316891686 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9605, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "969 Training acc 0.95 loss 0.5955082390028962 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "969 Training acc 0.9107142857142857 loss 0.5995645513623507 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9636, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "970 Training acc 0.92 loss 0.6011988322939604 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9703, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "970 Training acc 0.9171428571428571 loss 0.5983408411497362 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "971 Training acc 0.92 loss 0.5958975947559163 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9907, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "971 Training acc 0.9171428571428571 loss 0.5994708507033257 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9323, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "972 Training acc 0.95 loss 0.5929286710543947 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0160, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "972 Training acc 0.9107142857142857 loss 0.6001018675715026 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9645, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "973 Training acc 0.92 loss 0.601680125245175 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9717, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "973 Training acc 0.9171428571428571 loss 0.598220490536042 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9729, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "974 Training acc 0.92 loss 0.5984066529292366 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9487, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "974 Training acc 0.9171428571428571 loss 0.598915563909854 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0203, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "975 Training acc 0.92 loss 0.5917897698145391 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "975 Training acc 0.9171428571428571 loss 0.6003294774654914 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9226, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "976 Training acc 0.91 loss 0.5981571122727972 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0021, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "976 Training acc 0.9192857142857143 loss 0.5989609950591959 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9537, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "977 Training acc 0.91 loss 0.5937790334762879 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9966, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "977 Training acc 0.9192857142857143 loss 0.5998954117356617 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9371, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "978 Training acc 0.92 loss 0.596710636819757 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9975, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "978 Training acc 0.9242857142857144 loss 0.599263185462638 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9685, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "979 Training acc 0.92 loss 0.603740651819103 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9775, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "979 Training acc 0.9242857142857144 loss 0.5977515900155104 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9295, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "980 Training acc 0.92 loss 0.5944460488618643 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0114, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "980 Training acc 0.9242857142857144 loss 0.5997374074050175 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "981 Training acc 0.91 loss 0.5984049144851722 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9861, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "981 Training acc 0.9264285714285714 loss 0.598884066956325 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9571, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "982 Training acc 0.94 loss 0.5973166853810162 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9758, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "982 Training acc 0.9199999999999999 loss 0.5991133235617089 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9869, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "983 Training acc 0.92 loss 0.6013298119680021 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9232, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "983 Training acc 0.9242857142857144 loss 0.5982491280101654 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9253, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "984 Training acc 0.91 loss 0.596726848888969 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0048, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "984 Training acc 0.9264285714285714 loss 0.5992300616331303 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "985 Training acc 0.92 loss 0.5983942939372331 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9861, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "985 Training acc 0.9242857142857144 loss 0.5988679077757864 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9783, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "986 Training acc 0.92 loss 0.6001431211676705 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9376, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "986 Training acc 0.9242857142857144 loss 0.5984900482202093 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9804, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "987 Training acc 0.93 loss 0.6015257903122567 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9413, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "987 Training acc 0.9221428571428572 loss 0.5981924523290147 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9550, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "988 Training acc 0.91 loss 0.6016070335487796 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9843, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "988 Training acc 0.9264285714285714 loss 0.5981721935017911 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9784, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "989 Training acc 0.93 loss 0.6044454636954081 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9653, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "989 Training acc 0.9221428571428572 loss 0.5975602443570656 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0065, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "990 Training acc 0.89 loss 0.5967659402927427 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8550, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "990 Training acc 0.9307142857142857 loss 0.5992024463348523 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9981, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "991 Training acc 0.92 loss 0.6012574140695349 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8830, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "991 Training acc 0.9242857142857144 loss 0.5982359239478782 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9036, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "992 Training acc 0.92 loss 0.5942991967296002 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0217, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "992 Training acc 0.9242857142857144 loss 0.5997220252116469 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9132, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "993 Training acc 0.93 loss 0.5964315914494871 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0113, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "993 Training acc 0.9221428571428572 loss 0.5992602675912643 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0043, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "994 Training acc 0.94 loss 0.5930237787319481 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9078, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "994 Training acc 0.9199999999999999 loss 0.5999859795994495 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9177, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "995 Training acc 0.93 loss 0.5939922346321554 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0183, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "995 Training acc 0.9221428571428572 loss 0.5997726857505947 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9907, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "996 Training acc 0.92 loss 0.6032464514780311 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9285, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "996 Training acc 0.9242857142857144 loss 0.5977832592472707 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9113, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "997 Training acc 0.96 loss 0.590070792114828 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0341, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "997 Training acc 0.9157142857142857 loss 0.6005997080496632 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9533, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "998 Training acc 0.93 loss 0.6006303146186073 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "998 Training acc 0.9221428571428572 loss 0.5983308609809941 100\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9856, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "999 Training acc 0.93 loss 0.599381828915688 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9184, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.93\n",
      "999 Training acc 0.9221428571428572 loss 0.5985915749554276 100\n"
     ]
    }
   ],
   "source": [
    "test,tatin=tr.training()#without lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "81e70fe1-d416-4e31-974f-f95f11f7172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testno1,tatinon1=test,tatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6287c623-7994-4001-af6d-028403b4f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.extend(testno1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "63125dce-ca9c-465b-885f-60b766190005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d226a182-f543-4b3b-b02d-6ffa123accf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d648420a-30d8-4b5b-aaf5-2a3773b63d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Accuarcy with lambda\")\n",
    "plt.plot(test[0:-1:10],label=\"test accuracy\",linestyle='dashed')\n",
    "plt.plot(tatin[0:-1:10],label=\"train accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea7b538f-ba79-488a-b6f8-539e10302ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Accuarcy without lambda\")\n",
    "plt.plot(np.array(test)*100,label=\"test accuracy\",linestyle='dashed')\n",
    "plt.plot(np.array(tatin)*100,label=\"train accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64be5d-531d-4ceb-86f8-cd1568f98255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
