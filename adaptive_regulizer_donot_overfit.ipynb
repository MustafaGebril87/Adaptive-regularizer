{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161d6a9a-44a0-48e1-b412-941a4c94dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940685ea-1863-4c08-9008-b26b0365d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2822266d-1ff2-43f9-9eea-05591d9fae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x174aae880b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split,Subset\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee276a1-b683-4930-810c-b699240f4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e4c1aab5-2c89-4b17-96d3-091576c96c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv(os.path.join(\"dont-overfit-ii\",\"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b5999ea0-df6c-4672-ac3b-5c9ee384d2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.067</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.958</td>\n",
       "      <td>-0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.271</td>\n",
       "      <td>1.716</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.731</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>1.904</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-1.932</td>\n",
       "      <td>-0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>1.390</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>0.317</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.311</td>\n",
       "      <td>0.799</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>1.544</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.127</td>\n",
       "      <td>0.189</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.596</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>0.805</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.751</td>\n",
       "      <td>1.442</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1.557</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.950</td>\n",
       "      <td>...</td>\n",
       "      <td>1.492</td>\n",
       "      <td>1.430</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>0.797</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.032</td>\n",
       "      <td>-0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>-2.050</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-1.267</td>\n",
       "      <td>-1.021</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.327</td>\n",
       "      <td>-1.834</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>-2.764</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-2.010</td>\n",
       "      <td>0.438</td>\n",
       "      <td>1.972</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>-0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-1.487</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>0.141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.411</td>\n",
       "      <td>2.366</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-2.117</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.333</td>\n",
       "      <td>-2.863</td>\n",
       "      <td>0.203</td>\n",
       "      <td>1.898</td>\n",
       "      <td>0.434</td>\n",
       "      <td>1.207</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-2.621</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-1.690</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target      0      1      2      3      4      5      6      7  ...  \\\n",
       "0      0     1.0 -1.067 -1.114 -0.616  0.376  1.090  0.467 -0.422  0.460  ...   \n",
       "1      1     0.0 -0.831  0.271  1.716  1.096  1.731 -0.197  1.904 -0.265  ...   \n",
       "2      2     0.0  0.099  1.390 -0.732 -1.065  0.005 -0.081 -1.450  0.317  ...   \n",
       "3      3     1.0 -0.989 -0.916 -1.343  0.145  0.543  0.636  1.127  0.189  ...   \n",
       "4      4     0.0  0.811 -1.509  0.522 -0.360 -0.220 -0.959  0.334 -0.566  ...   \n",
       "..   ...     ...    ...    ...    ...    ...    ...    ...    ...    ...  ...   \n",
       "245  245     1.0 -0.068 -0.184 -1.153  0.610  0.414  1.557 -0.234  0.950  ...   \n",
       "246  246     0.0 -0.234 -1.373 -2.050 -0.408 -0.255  0.784  0.986 -0.891  ...   \n",
       "247  247     0.0 -2.327 -1.834 -0.762  0.660 -0.858 -2.764 -0.539 -0.065  ...   \n",
       "248  248     1.0 -0.451 -0.204 -0.762  0.261  0.022 -1.487 -1.122  0.141  ...   \n",
       "249  249     0.0  0.725  1.064  1.333 -2.863  0.203  1.898  0.434  1.207  ...   \n",
       "\n",
       "       290    291    292    293    294    295    296    297    298    299  \n",
       "0    0.220 -0.339  0.254 -0.179  0.352  0.125  0.347  0.436  0.958 -0.824  \n",
       "1   -0.765 -0.735 -1.158  2.554  0.856 -1.506  0.462 -0.029 -1.932 -0.343  \n",
       "2   -1.311  0.799 -1.001  1.544  0.575 -0.309 -0.339 -0.148 -0.646  0.725  \n",
       "3   -1.370  1.093  0.596 -0.589 -0.649 -0.163 -0.958 -1.081  0.805  3.401  \n",
       "4   -0.178  0.718 -1.017  1.249 -0.596 -0.445  1.751  1.442 -0.393 -0.643  \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "245  1.492  1.430 -0.333 -0.200 -1.073  0.797  1.980  1.191  1.032 -0.402  \n",
       "246 -0.996  0.678  1.395  0.714  0.215 -0.537 -1.267 -1.021  0.747  0.128  \n",
       "247 -1.237 -0.620  0.670 -2.010  0.438  1.972 -0.379  0.676 -1.220 -0.855  \n",
       "248  0.729  0.411  2.366 -0.021  0.160  0.045  0.208 -2.117 -0.546 -0.093  \n",
       "249 -1.028  1.081  0.607  0.550 -2.621 -0.143 -0.544 -1.690 -0.198  0.643  \n",
       "\n",
       "[250 rows x 302 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7e20be6c-72be-44ae-b3ac-8d8f700447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"id\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7e25c7aa-dcfd-41d5-8321-90705d77c40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.067</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>-0.616</td>\n",
       "      <td>0.376</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.422</td>\n",
       "      <td>0.460</td>\n",
       "      <td>-0.443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.958</td>\n",
       "      <td>-0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.831</td>\n",
       "      <td>0.271</td>\n",
       "      <td>1.716</td>\n",
       "      <td>1.096</td>\n",
       "      <td>1.731</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>1.904</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>0.557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765</td>\n",
       "      <td>-0.735</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>2.554</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-1.506</td>\n",
       "      <td>0.462</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-1.932</td>\n",
       "      <td>-0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099</td>\n",
       "      <td>1.390</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-1.065</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>0.317</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.311</td>\n",
       "      <td>0.799</td>\n",
       "      <td>-1.001</td>\n",
       "      <td>1.544</td>\n",
       "      <td>0.575</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.339</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.989</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>-1.343</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.127</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.370</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.596</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.163</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-1.081</td>\n",
       "      <td>0.805</td>\n",
       "      <td>3.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.811</td>\n",
       "      <td>-1.509</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-0.360</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>-0.959</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.566</td>\n",
       "      <td>-0.656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>0.718</td>\n",
       "      <td>-1.017</td>\n",
       "      <td>1.249</td>\n",
       "      <td>-0.596</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>1.751</td>\n",
       "      <td>1.442</td>\n",
       "      <td>-0.393</td>\n",
       "      <td>-0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>-1.153</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1.557</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.896</td>\n",
       "      <td>...</td>\n",
       "      <td>1.492</td>\n",
       "      <td>1.430</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>0.797</td>\n",
       "      <td>1.980</td>\n",
       "      <td>1.191</td>\n",
       "      <td>1.032</td>\n",
       "      <td>-0.402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.234</td>\n",
       "      <td>-1.373</td>\n",
       "      <td>-2.050</td>\n",
       "      <td>-0.408</td>\n",
       "      <td>-0.255</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.986</td>\n",
       "      <td>-0.891</td>\n",
       "      <td>-0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996</td>\n",
       "      <td>0.678</td>\n",
       "      <td>1.395</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.215</td>\n",
       "      <td>-0.537</td>\n",
       "      <td>-1.267</td>\n",
       "      <td>-1.021</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.327</td>\n",
       "      <td>-1.834</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.660</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>-2.764</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.549</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.237</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-2.010</td>\n",
       "      <td>0.438</td>\n",
       "      <td>1.972</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>0.676</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>-0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.451</td>\n",
       "      <td>-0.204</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-1.487</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.411</td>\n",
       "      <td>2.366</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.208</td>\n",
       "      <td>-2.117</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.064</td>\n",
       "      <td>1.333</td>\n",
       "      <td>-2.863</td>\n",
       "      <td>0.203</td>\n",
       "      <td>1.898</td>\n",
       "      <td>0.434</td>\n",
       "      <td>1.207</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.028</td>\n",
       "      <td>1.081</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-2.621</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>-1.690</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target      0      1      2      3      4      5      6      7      8  \\\n",
       "0       1.0 -1.067 -1.114 -0.616  0.376  1.090  0.467 -0.422  0.460 -0.443   \n",
       "1       0.0 -0.831  0.271  1.716  1.096  1.731 -0.197  1.904 -0.265  0.557   \n",
       "2       0.0  0.099  1.390 -0.732 -1.065  0.005 -0.081 -1.450  0.317 -0.624   \n",
       "3       1.0 -0.989 -0.916 -1.343  0.145  0.543  0.636  1.127  0.189 -0.118   \n",
       "4       0.0  0.811 -1.509  0.522 -0.360 -0.220 -0.959  0.334 -0.566 -0.656   \n",
       "..      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "245     1.0 -0.068 -0.184 -1.153  0.610  0.414  1.557 -0.234  0.950  0.896   \n",
       "246     0.0 -0.234 -1.373 -2.050 -0.408 -0.255  0.784  0.986 -0.891 -0.268   \n",
       "247     0.0 -2.327 -1.834 -0.762  0.660 -0.858 -2.764 -0.539 -0.065  0.549   \n",
       "248     1.0 -0.451 -0.204 -0.762  0.261  0.022 -1.487 -1.122  0.141  0.369   \n",
       "249     0.0  0.725  1.064  1.333 -2.863  0.203  1.898  0.434  1.207 -0.015   \n",
       "\n",
       "     ...    290    291    292    293    294    295    296    297    298    299  \n",
       "0    ...  0.220 -0.339  0.254 -0.179  0.352  0.125  0.347  0.436  0.958 -0.824  \n",
       "1    ... -0.765 -0.735 -1.158  2.554  0.856 -1.506  0.462 -0.029 -1.932 -0.343  \n",
       "2    ... -1.311  0.799 -1.001  1.544  0.575 -0.309 -0.339 -0.148 -0.646  0.725  \n",
       "3    ... -1.370  1.093  0.596 -0.589 -0.649 -0.163 -0.958 -1.081  0.805  3.401  \n",
       "4    ... -0.178  0.718 -1.017  1.249 -0.596 -0.445  1.751  1.442 -0.393 -0.643  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "245  ...  1.492  1.430 -0.333 -0.200 -1.073  0.797  1.980  1.191  1.032 -0.402  \n",
       "246  ... -0.996  0.678  1.395  0.714  0.215 -0.537 -1.267 -1.021  0.747  0.128  \n",
       "247  ... -1.237 -0.620  0.670 -2.010  0.438  1.972 -0.379  0.676 -1.220 -0.855  \n",
       "248  ...  0.729  0.411  2.366 -0.021  0.160  0.045  0.208 -2.117 -0.546 -0.093  \n",
       "249  ... -1.028  1.081  0.607  0.550 -2.621 -0.143 -0.544 -1.690 -0.198  0.643  \n",
       "\n",
       "[250 rows x 301 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80f74470-69ac-4301-bd1e-9e8eab675a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.drop(\"target\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "913a5564-1799-4085-bcb7-07703ed6b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "24e5bb69-cc7a-4e70-b331-29e0588b3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9e583a0b-1de9-4e02-adc8-5926ccbd39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bdccdbec-c1ae-4224-a1a0-c70c95926aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "standered=scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ae93b731-3ed2-4a61-bf00-8c6b4601f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(standered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8e63bc18-9762-49f3-94fa-20cd8289a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):       \n",
    "        \n",
    "        self.features=train\n",
    "        self.labels = target\n",
    "    def __getitem__(self, i):\n",
    "        feaures=self.features.iloc[i]\n",
    "        target=self.labels[i]\n",
    "        return torch.tensor(np.array(feaures)),torch.tensor(np.array(target))\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5aba2551-393b-4a7e-a77e-565aeb48c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9b59b1f6-69d4-4aee-aeef-6d00a64d51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,outNumber):\n",
    "        super(Net,self).__init__()\n",
    "        self.outNumber = outNumber\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.fc0 = nn.Linear(\n",
    "            in_features=30, out_features=100 ,bias=True)\n",
    "        self.depth = 100\n",
    "        self.linear_layers = []\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(\n",
    "            in_features=100, out_features=100, bias=True) for index in range(self.depth)])\n",
    "        self.fc = nn.Linear(in_features=100, out_features=self.outNumber, bias=True)\n",
    "        # self.activation = nn.sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.fc0(x)\n",
    "        residual = out\n",
    "        for index, layer in enumerate(self.linear_layers):\n",
    "            out = self.linear_layers[index](out)\n",
    "            out = f.leaky_relu(out)\n",
    "            # out = f.dropout(out, p=self.droup_out)\n",
    "            if index % 3 == 0:\n",
    "                out = out + residual\n",
    "                residual = out\n",
    "        out = self.fc(out)\n",
    "        out = torch.nn.functional.sigmoid(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6eff0bab-8c06-4556-bcca-288d52387392",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net(1)\n",
    "net=net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "d5e451c4-eb78-479f-a58e-cb089369e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "274d6cc5-bf56-4661-9b6e-fac89855c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    accuracy = metrics.accuracy_score(y_true.cpu().detach().numpy(), y_prob.cpu().detach().numpy() > 0.5)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a38cd4cc-6fe4-4ea7-b04b-5ff1c5ac60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = MyDataset()\n",
    "#dataset_ = Subset(dataset_, np.arange(1000000))\n",
    "# Split into training and test\n",
    "train_size = int(0.3 * len(dataset_))\n",
    "test_size = len(dataset_) - train_size\n",
    "\n",
    "\n",
    "trainset, testset = random_split(dataset_, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False)\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1.0E-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a22878f5-6ec0-4c52-9532-b81bae0dfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pytorch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.optim as optim \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class train_:\n",
    "    def __init__(self,epochs,model,train_dl,val_dl,criterion,optimizer):\n",
    "        self.model = model\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "    def accuracy(self,y_prob,y_true):\n",
    "        y_prob = y_prob > 0.5\n",
    "        return (y_true == y_prob).sum().item() / y_true.size(0)    \n",
    "   # def accuracy(self,predictions, labels):# accuracy function from the resulted predictions\n",
    "    #    #accuracy = (torch.softmax(predictions, dim=1).argmax(dim=1) == labels).sum().float() / float( labels.size(0) )\n",
    "     #   accuracy = ((predictions== labels).sum().float() / float( labels.size(0) ))\n",
    "      #  return accuracy    \n",
    "    \n",
    "    def training(self):\n",
    "        Max_Accu_Val = .0 # start with zero accuracy to compare the results\n",
    "        lamda = torch.tensor(0.6).type(torch.float).requires_grad_() \n",
    "        lamda = torch.tensor(0.6,requires_grad=True, device=\"cuda\")\n",
    "        self.optimizer2 = optim.Adam([lamda], lr=1E-4)\n",
    "        error = []\n",
    "        acctest_=[]\n",
    "        acctrain_=[]\n",
    "        \n",
    "        self.model = self.model.double()\n",
    "        for e in range(self.epochs): # for every epoch\n",
    "            CounterTrain =0 # initialize the values with zero, this used to calculate the number of  training epochs\n",
    "            CounterVal =0  # this used as number of test epochs\n",
    "            train_loss = 0.0 # initialize the training loss with zero\n",
    "            TrainAccAll = 0.0 # the summed accuracy for the whole training epoch\n",
    "            VallAccAll = 0.0 # the summed accuracy of the whole test epoch\n",
    "            lossTrainAll = 0.0 # the summed loss for the whole training epoch\n",
    "            lossValAll = 0.0 # the summed loss for the whole test epoch\n",
    "            lossVarAll=0\n",
    "            acctest=0\n",
    "            countertest=0\n",
    "            lossTestAll=0.0\n",
    "            self.model.train() \n",
    "            \n",
    "            # signal a training process\n",
    "            for data, labels in self.train_dl: # for every input and output in train data loader\n",
    "                self.model.train() \n",
    "\n",
    "                if torch.cuda.is_available(): # check if cuda is a vailable\n",
    "                   data, labels = data.cuda(), labels.cuda() # feed the input and output to cuda\n",
    "               \n",
    "                self.optimizer.zero_grad() # zero the gradient, required by pytorch\n",
    "               \n",
    "                target = self.model(data.double()) # calculate the input\n",
    "               #outnorm = [float(i)/max(out) for i in out]\n",
    "\n",
    "                dataiter = iter(self.val_dl)\n",
    "                inputsTest, labelsTest  = next(dataiter)\n",
    "                labelsTest = labelsTest.to(\"cuda\")\n",
    "                inputsTest = inputsTest.to(\"cuda\")\n",
    "                self.model.eval() \n",
    "                targettest= self.model(inputsTest.double()) \n",
    "                \n",
    "                acctest += self.accuracy(targettest, labelsTest)\n",
    "                countertest+=1\n",
    "                \n",
    "                \n",
    "                l1 =1# sum(p.abs().sum() for p in self.model.parameters())\n",
    "                \n",
    "                lossOriginal = self.criterion(target.double(), labels.double())\n",
    "              \n",
    "                loss =lossOriginal+lamda*l1\n",
    "                #x= ((1/((-lossOriginal+lamda)/(2*labels-1))))\n",
    "                x = (1/(labels-torch.sqrt(torch.abs(lossOriginal-lamda))+.0000001))-1\n",
    "                varout = torch.abs(torch.var(-(x*x-1)/(x+.0000001)))\n",
    "                #varout =torch.var( -torch.log(torch.abs(1/(labels-torch.sqrt(torch.abs(lossOriginal-lamda)))-1)))\n",
    "                #varout = torch.var(labels-lossOriginal-lamda)\n",
    "                #varout =torch.var(-(((1/((-loss+lamda)/(2*labels-1))-1))+(.5*(torch.abs(1/((-loss+lamda)/(2*labels-1))-1))*(torch.abs(1/((-loss+lamda)/(2*labels-1))-1)))))\n",
    "                #varout=torch.var(((((2*labels/((lossOriginal-lamda*l1)+2*labels+.00001))-1))-1)+.5*((((2*labels/((lossOriginal-lamda*l1)+2*labels+.00001))-1))-1)*((((2*labels/((lossOriginal-lamda*l1)+2*labels+.00001))-1))-1))\n",
    "                loss.backward(retain_graph=True)# neural network backward calculations\n",
    "                self.optimizer2.zero_grad()\n",
    "                #var_=torch.abs(torch.var(out)-.1*torch.max(out))\n",
    "                varout.backward()\n",
    "                self.optimizer.step() \n",
    "                self.optimizer2.step()\n",
    "                # optimize\n",
    "                \n",
    "                train_loss = lossOriginal.item()# sotre loss values\n",
    "                TrainAcc = self.accuracy(target.double(),labels.double()) # calculate training accuracy\n",
    "                CounterTrain+=1 # update training counter\n",
    "                TrainAccAll +=TrainAcc # update training accuracy the whole epoch\n",
    "                lossTrainAll += train_loss # update training loss the whole epoch\n",
    "                #if CounterTrain%5==0:\n",
    "                if CounterTrain%1==0:\n",
    "                    print(\"lamda\",lamda)\n",
    "                    print(varout)\n",
    "                    print(\"acc Test:\", acctest/countertest)\n",
    "                    print(e,'Training acc',TrainAccAll /CounterTrain,'loss',lossTrainAll/CounterTrain,CounterTrain*50)\n",
    "                    #traced_cell = torch.jit.trace(self.model,data.double())\n",
    "                    #traced_cell.save('./atrial_Fib_model.zip')\n",
    "                    #print(\"Conerted!\")\n",
    "                \n",
    "                    error.append(lossTrainAll/CounterTrain)\n",
    "                    acctest_.append(acctest/countertest)\n",
    "                    acctrain_.append(TrainAccAll/CounterTrain)   \n",
    "        return acctest_,acctrain_\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "b17fd0df-f057-472b-8330-5ff1b9d0e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=train_(1000,net,trainloader,testloader,criterion,optimizer)#with lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "07dac5f1-48a3-4fdb-b6de-9491b9cc2c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9915, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "0 Training acc 0.72 loss 0.692106471716237 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9801, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "1 Training acc 0.72 loss 0.686920435686066 50\n",
      "lamda tensor(0.6003, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9691, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "2 Training acc 0.72 loss 0.6818536990125786 50\n",
      "lamda tensor(0.6004, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9586, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "3 Training acc 0.72 loss 0.6769185442182452 50\n",
      "lamda tensor(0.6005, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "4 Training acc 0.72 loss 0.6721120987276376 50\n",
      "lamda tensor(0.6006, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "5 Training acc 0.72 loss 0.6674251693944815 50\n",
      "lamda tensor(0.6007, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9294, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "6 Training acc 0.72 loss 0.6628609817508007 50\n",
      "lamda tensor(0.6008, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9205, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "7 Training acc 0.72 loss 0.6584199172013341 50\n",
      "lamda tensor(0.6009, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9120, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "8 Training acc 0.72 loss 0.6541139017122247 50\n",
      "lamda tensor(0.6010, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9038, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "9 Training acc 0.72 loss 0.6499447141173585 50\n",
      "lamda tensor(0.6011, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8960, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "10 Training acc 0.72 loss 0.6458999955238093 50\n",
      "lamda tensor(0.6012, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8885, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "11 Training acc 0.72 loss 0.6419848469846293 50\n",
      "lamda tensor(0.6013, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8813, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "12 Training acc 0.72 loss 0.6381997883958002 50\n",
      "lamda tensor(0.6014, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8745, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "13 Training acc 0.72 loss 0.6345350618540948 50\n",
      "lamda tensor(0.6015, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8679, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "14 Training acc 0.72 loss 0.6309870662891208 50\n",
      "lamda tensor(0.6016, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8617, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "15 Training acc 0.72 loss 0.6275638328494271 50\n",
      "lamda tensor(0.6017, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8557, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "16 Training acc 0.72 loss 0.6242663510927465 50\n",
      "lamda tensor(0.6018, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "17 Training acc 0.72 loss 0.621091637997804 50\n",
      "lamda tensor(0.6019, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8446, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "18 Training acc 0.72 loss 0.6180402958658128 50\n",
      "lamda tensor(0.6020, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8394, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "19 Training acc 0.72 loss 0.6151076939036134 50\n",
      "lamda tensor(0.6021, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8344, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "20 Training acc 0.72 loss 0.6122828686570839 50\n",
      "lamda tensor(0.6022, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8297, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "21 Training acc 0.72 loss 0.6095621223446427 50\n",
      "lamda tensor(0.6022, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8252, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "22 Training acc 0.72 loss 0.6069483211127794 50\n",
      "lamda tensor(0.6023, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8209, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "23 Training acc 0.72 loss 0.6044457371020485 50\n",
      "lamda tensor(0.6024, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "24 Training acc 0.72 loss 0.6020505814849535 50\n",
      "lamda tensor(0.6025, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8214, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "25 Training acc 0.72 loss 0.5999122448050236 50\n",
      "lamda tensor(0.6025, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8246, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "26 Training acc 0.72 loss 0.5980686374486448 50\n",
      "lamda tensor(0.6026, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8273, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "27 Training acc 0.72 loss 0.5964855516396633 50\n",
      "lamda tensor(0.6026, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8296, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "28 Training acc 0.72 loss 0.595132923474604 50\n",
      "lamda tensor(0.6026, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8316, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "29 Training acc 0.72 loss 0.5939777307150244 50\n",
      "lamda tensor(0.6026, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8332, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "30 Training acc 0.72 loss 0.5929976629003422 50\n",
      "lamda tensor(0.6026, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8346, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "31 Training acc 0.72 loss 0.5921738973883729 50\n",
      "lamda tensor(0.6026, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8357, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "32 Training acc 0.72 loss 0.5914868147342703 50\n",
      "lamda tensor(0.6025, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8367, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "33 Training acc 0.72 loss 0.5909221451663046 50\n",
      "lamda tensor(0.6025, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8374, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "34 Training acc 0.72 loss 0.590465197019434 50\n",
      "lamda tensor(0.6025, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "35 Training acc 0.72 loss 0.5901047327138846 50\n",
      "lamda tensor(0.6024, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "36 Training acc 0.72 loss 0.5898305098973986 50\n",
      "lamda tensor(0.6024, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8386, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "37 Training acc 0.72 loss 0.5896342262723945 50\n",
      "lamda tensor(0.6023, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "38 Training acc 0.72 loss 0.5895088154002377 50\n",
      "lamda tensor(0.6022, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "39 Training acc 0.72 loss 0.5894470679426249 50\n",
      "lamda tensor(0.6022, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8386, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "40 Training acc 0.72 loss 0.5894426187932593 50\n",
      "lamda tensor(0.6021, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8384, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "41 Training acc 0.72 loss 0.5894898776303777 50\n",
      "lamda tensor(0.6020, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8382, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "42 Training acc 0.72 loss 0.5895844657571728 50\n",
      "lamda tensor(0.6020, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8378, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "43 Training acc 0.72 loss 0.5897218382839318 50\n",
      "lamda tensor(0.6019, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8374, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "44 Training acc 0.72 loss 0.5898981145755142 50\n",
      "lamda tensor(0.6018, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8369, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "45 Training acc 0.72 loss 0.5901102133747468 50\n",
      "lamda tensor(0.6017, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "46 Training acc 0.72 loss 0.5903548957134416 50\n",
      "lamda tensor(0.6017, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "47 Training acc 0.72 loss 0.5906291000175868 50\n",
      "lamda tensor(0.6016, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8351, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "48 Training acc 0.72 loss 0.5909300248400958 50\n",
      "lamda tensor(0.6015, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8344, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "49 Training acc 0.72 loss 0.5912552072686394 50\n",
      "lamda tensor(0.6014, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8337, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "50 Training acc 0.72 loss 0.5916023616688701 50\n",
      "lamda tensor(0.6013, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "51 Training acc 0.72 loss 0.5919695625206393 50\n",
      "lamda tensor(0.6012, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8322, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "52 Training acc 0.72 loss 0.5923562422236976 50\n",
      "lamda tensor(0.6011, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8313, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "53 Training acc 0.72 loss 0.5927607504247002 50\n",
      "lamda tensor(0.6011, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8305, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "54 Training acc 0.72 loss 0.5931816564670408 50\n",
      "lamda tensor(0.6010, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8296, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "55 Training acc 0.72 loss 0.5936182186858195 50\n",
      "lamda tensor(0.6009, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8287, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "56 Training acc 0.72 loss 0.5940694284940901 50\n",
      "lamda tensor(0.6008, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8278, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "57 Training acc 0.72 loss 0.5945340296367907 50\n",
      "lamda tensor(0.6007, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8268, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "58 Training acc 0.72 loss 0.595011394650249 50\n",
      "lamda tensor(0.6006, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8259, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "59 Training acc 0.72 loss 0.5955005094402579 50\n",
      "lamda tensor(0.6005, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8249, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "60 Training acc 0.72 loss 0.596000369766513 50\n",
      "lamda tensor(0.6004, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8239, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "61 Training acc 0.72 loss 0.5965101631276468 50\n",
      "lamda tensor(0.6003, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8229, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "62 Training acc 0.72 loss 0.5970294837935056 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8219, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "63 Training acc 0.72 loss 0.5975572207751245 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8208, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "64 Training acc 0.72 loss 0.5980923366510295 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8198, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "65 Training acc 0.72 loss 0.5986335677652803 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8188, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "66 Training acc 0.72 loss 0.5991802013608479 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "67 Training acc 0.72 loss 0.5997259089844278 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "68 Training acc 0.72 loss 0.6001382650655976 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8184, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "69 Training acc 0.72 loss 0.6003681244050524 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8184, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "70 Training acc 0.72 loss 0.6003494791501568 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "71 Training acc 0.72 loss 0.6001088448518195 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8197, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "72 Training acc 0.72 loss 0.5996958625438417 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8203, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "73 Training acc 0.72 loss 0.6013956680398443 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8229, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "74 Training acc 0.72 loss 0.6028928957957694 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8252, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "75 Training acc 0.72 loss 0.6041903771605858 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8271, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "76 Training acc 0.72 loss 0.6052980823345163 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8287, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "77 Training acc 0.72 loss 0.6062250124139696 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8301, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "78 Training acc 0.72 loss 0.6069818906751119 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8311, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "79 Training acc 0.72 loss 0.6075803624028269 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8319, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "80 Training acc 0.72 loss 0.6080312567117344 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "81 Training acc 0.72 loss 0.6083454811600619 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8329, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "82 Training acc 0.72 loss 0.6085346690391079 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "83 Training acc 0.72 loss 0.6086102165085132 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "84 Training acc 0.72 loss 0.6085824675140733 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8328, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "85 Training acc 0.72 loss 0.6084619264421155 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "86 Training acc 0.72 loss 0.6082582728664999 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "87 Training acc 0.72 loss 0.6079804479427486 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8315, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "88 Training acc 0.72 loss 0.6076372133756988 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "89 Training acc 0.72 loss 0.6072362432743736 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8300, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "90 Training acc 0.72 loss 0.6067840524113101 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8292, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "91 Training acc 0.72 loss 0.6062878540073704 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8283, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "92 Training acc 0.72 loss 0.6057537526588745 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8273, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "93 Training acc 0.72 loss 0.6051877134428525 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8263, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "94 Training acc 0.72 loss 0.604595249504622 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8253, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "95 Training acc 0.72 loss 0.6039796697372881 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8242, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "96 Training acc 0.72 loss 0.6033442526156065 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8231, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "97 Training acc 0.72 loss 0.6026931799503082 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8220, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "98 Training acc 0.72 loss 0.6020287845926602 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8209, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "99 Training acc 0.72 loss 0.6013537228437464 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8198, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "100 Training acc 0.72 loss 0.600670318169247 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8187, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "101 Training acc 0.72 loss 0.5999817459267124 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8187, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "102 Training acc 0.72 loss 0.5992924065409169 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8315, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "103 Training acc 0.72 loss 0.5992128654103954 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8210, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "104 Training acc 0.72 loss 0.5970702498825947 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8243, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "105 Training acc 0.72 loss 0.5951874539483741 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8271, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "106 Training acc 0.72 loss 0.5935305156375038 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8297, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "107 Training acc 0.72 loss 0.5920708766622927 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "108 Training acc 0.72 loss 0.5907844109024277 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8340, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "109 Training acc 0.72 loss 0.5896493543651143 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8357, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "110 Training acc 0.72 loss 0.5886490748565012 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "111 Training acc 0.72 loss 0.5877664765939954 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "112 Training acc 0.72 loss 0.5869839095245317 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8400, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "113 Training acc 0.72 loss 0.5862884544710205 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8411, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "114 Training acc 0.72 loss 0.5856719210981417 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8421, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "115 Training acc 0.72 loss 0.5851258328946224 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8429, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "116 Training acc 0.72 loss 0.5846415914681823 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8437, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "117 Training acc 0.72 loss 0.5842105622075109 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8444, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "118 Training acc 0.72 loss 0.5838263471463997 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8451, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "119 Training acc 0.72 loss 0.58348404155824 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "120 Training acc 0.72 loss 0.5831790196295942 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8461, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "121 Training acc 0.72 loss 0.5829067180395885 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8466, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "122 Training acc 0.72 loss 0.582663512356487 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8470, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "123 Training acc 0.72 loss 0.5824464231087705 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8473, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "124 Training acc 0.72 loss 0.5822528644619855 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8476, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "125 Training acc 0.72 loss 0.5820802182408803 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8479, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "126 Training acc 0.72 loss 0.5819261184501385 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "127 Training acc 0.72 loss 0.5817886143348608 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8484, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "128 Training acc 0.72 loss 0.5816659220813347 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "129 Training acc 0.72 loss 0.5815563717320201 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "130 Training acc 0.72 loss 0.5814586642199056 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "131 Training acc 0.72 loss 0.58137166615366 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "132 Training acc 0.72 loss 0.5812943734368734 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "133 Training acc 0.72 loss 0.5812256881935245 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "134 Training acc 0.72 loss 0.5811647735550379 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "135 Training acc 0.72 loss 0.5811108930901712 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "136 Training acc 0.72 loss 0.5810633056685446 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "137 Training acc 0.72 loss 0.5810214227840482 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "138 Training acc 0.72 loss 0.5809846261390665 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "139 Training acc 0.72 loss 0.580952420330594 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "140 Training acc 0.72 loss 0.5809243538437574 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "141 Training acc 0.72 loss 0.5808999814188163 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "142 Training acc 0.72 loss 0.5808789723964255 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "143 Training acc 0.72 loss 0.5808609952604639 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "144 Training acc 0.72 loss 0.5808457648741406 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "145 Training acc 0.72 loss 0.5808330108240916 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "146 Training acc 0.72 loss 0.5808224906405927 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "147 Training acc 0.72 loss 0.5808139910224628 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "148 Training acc 0.72 loss 0.5808073162122395 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "149 Training acc 0.72 loss 0.5808022981757137 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "150 Training acc 0.72 loss 0.5807987696455557 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "151 Training acc 0.72 loss 0.5807965987167857 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "152 Training acc 0.72 loss 0.5807956454932537 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "153 Training acc 0.72 loss 0.5807957983174111 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "154 Training acc 0.72 loss 0.5807969464706177 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "155 Training acc 0.72 loss 0.5807989965125645 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "156 Training acc 0.72 loss 0.580801871846118 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "157 Training acc 0.72 loss 0.5808054401275701 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "158 Training acc 0.72 loss 0.5808095436100882 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8500, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "159 Training acc 0.72 loss 0.5808142623671637 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "160 Training acc 0.72 loss 0.5808195242688646 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "161 Training acc 0.72 loss 0.5808252607497737 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "162 Training acc 0.72 loss 0.5808314591024666 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "163 Training acc 0.72 loss 0.580838079068515 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "164 Training acc 0.72 loss 0.5808450785252787 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "165 Training acc 0.72 loss 0.5808524180540627 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "166 Training acc 0.72 loss 0.5808600757996962 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "167 Training acc 0.72 loss 0.5808680236133907 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "168 Training acc 0.72 loss 0.5808762357106532 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "169 Training acc 0.72 loss 0.5808846893395219 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "170 Training acc 0.72 loss 0.5808933721419676 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "171 Training acc 0.72 loss 0.580902260151604 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "172 Training acc 0.72 loss 0.5809113370545642 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "173 Training acc 0.72 loss 0.5809205886582757 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "174 Training acc 0.72 loss 0.5809300007501176 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "175 Training acc 0.72 loss 0.5809395537394082 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "176 Training acc 0.72 loss 0.5809492398196776 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "177 Training acc 0.72 loss 0.5809590514363606 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8497, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "178 Training acc 0.72 loss 0.5809689743700864 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "179 Training acc 0.72 loss 0.5809789971942029 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "180 Training acc 0.72 loss 0.580989115035927 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "181 Training acc 0.72 loss 0.5809993275802287 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "182 Training acc 0.72 loss 0.5810096239797171 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "183 Training acc 0.72 loss 0.5810199997856652 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "184 Training acc 0.72 loss 0.5810304524140497 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "185 Training acc 0.72 loss 0.5810409790880873 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "186 Training acc 0.72 loss 0.5810515781660128 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "187 Training acc 0.72 loss 0.5810622460692577 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8495, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "188 Training acc 0.72 loss 0.5810729785200064 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "189 Training acc 0.72 loss 0.5810837691046987 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "190 Training acc 0.72 loss 0.5810946138020257 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "191 Training acc 0.72 loss 0.5811055092624124 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "192 Training acc 0.72 loss 0.5811164551424086 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "193 Training acc 0.72 loss 0.5811274441544912 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "194 Training acc 0.72 loss 0.5811384706543671 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "195 Training acc 0.72 loss 0.5811495440040052 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "196 Training acc 0.72 loss 0.5811606626815831 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "197 Training acc 0.72 loss 0.5811718208065478 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "198 Training acc 0.72 loss 0.5811830193142203 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "199 Training acc 0.72 loss 0.5811942757166971 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "200 Training acc 0.72 loss 0.5812055793687297 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "201 Training acc 0.72 loss 0.5812169229055474 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "202 Training acc 0.72 loss 0.5812283076003021 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8492, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "203 Training acc 0.72 loss 0.5812397327266271 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "204 Training acc 0.72 loss 0.5812511953882173 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "205 Training acc 0.72 loss 0.5812626914918568 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "206 Training acc 0.72 loss 0.5812742227861314 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8491, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "207 Training acc 0.72 loss 0.5812857944190395 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8490, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "208 Training acc 0.72 loss 0.5812973971433247 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8490, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "209 Training acc 0.72 loss 0.5813090163417796 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8490, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "210 Training acc 0.72 loss 0.5813206690237017 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8490, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "211 Training acc 0.72 loss 0.581332350846314 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8490, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "212 Training acc 0.72 loss 0.5813440680052562 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "213 Training acc 0.72 loss 0.581355810727718 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "214 Training acc 0.72 loss 0.5813675881751292 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "215 Training acc 0.72 loss 0.5813794019762435 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8489, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "216 Training acc 0.72 loss 0.5813912553354014 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "217 Training acc 0.72 loss 0.5814031467543992 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "218 Training acc 0.72 loss 0.5814150703290487 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "219 Training acc 0.72 loss 0.5814270278950795 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "220 Training acc 0.72 loss 0.5814390231237747 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "221 Training acc 0.72 loss 0.5814510596843708 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8487, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "222 Training acc 0.72 loss 0.5814631234429951 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8487, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "223 Training acc 0.72 loss 0.5814752080486029 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8487, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "224 Training acc 0.72 loss 0.5814873265632755 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8487, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "225 Training acc 0.72 loss 0.5814994711685347 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "226 Training acc 0.72 loss 0.5815116492071126 50\n",
      "lamda tensor(0.6002, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "227 Training acc 0.72 loss 0.581523859448405 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "228 Training acc 0.72 loss 0.5815360957606868 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "229 Training acc 0.72 loss 0.5815483711619035 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8486, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "230 Training acc 0.72 loss 0.5815606753336086 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "231 Training acc 0.72 loss 0.5815730112124837 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "232 Training acc 0.72 loss 0.5815853818024019 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "233 Training acc 0.72 loss 0.581597783877335 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "234 Training acc 0.72 loss 0.581610214904717 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8484, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "235 Training acc 0.72 loss 0.5816226802559952 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8484, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "236 Training acc 0.72 loss 0.5816351769018436 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8484, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "237 Training acc 0.72 loss 0.5816477041143625 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8484, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "238 Training acc 0.72 loss 0.581660259415648 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "239 Training acc 0.72 loss 0.5816728450312109 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "240 Training acc 0.72 loss 0.5816854567673322 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "241 Training acc 0.72 loss 0.5816980934221913 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "242 Training acc 0.72 loss 0.581710765117224 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "243 Training acc 0.72 loss 0.5817234683607639 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "244 Training acc 0.72 loss 0.5817362017292285 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "245 Training acc 0.72 loss 0.5817489646248736 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "246 Training acc 0.72 loss 0.5817617531034959 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "247 Training acc 0.72 loss 0.5817745756004274 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8481, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "248 Training acc 0.72 loss 0.5817874284275241 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8481, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "249 Training acc 0.72 loss 0.5818003155587176 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8481, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "250 Training acc 0.72 loss 0.5818132385738154 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8481, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "251 Training acc 0.72 loss 0.5818261953650722 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8480, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "252 Training acc 0.72 loss 0.5818391827017166 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8480, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "253 Training acc 0.72 loss 0.5818521955000109 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8480, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "254 Training acc 0.72 loss 0.5818652494113158 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8480, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "255 Training acc 0.72 loss 0.5818783371446676 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8479, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "256 Training acc 0.72 loss 0.5818914571833765 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8479, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "257 Training acc 0.72 loss 0.5819046088735769 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8479, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "258 Training acc 0.72 loss 0.5819177930686624 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8479, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "259 Training acc 0.72 loss 0.5819310012654457 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8478, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "260 Training acc 0.72 loss 0.5819442382943343 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8478, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "261 Training acc 0.72 loss 0.5819575041460788 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8478, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "262 Training acc 0.72 loss 0.581970799487937 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8478, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "263 Training acc 0.72 loss 0.581984124380094 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8477, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "264 Training acc 0.72 loss 0.5819974731905487 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8477, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "265 Training acc 0.72 loss 0.5820108206645493 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8477, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "266 Training acc 0.72 loss 0.5820242019775256 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8477, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "267 Training acc 0.72 loss 0.5820376174241705 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8476, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "268 Training acc 0.72 loss 0.5820510509494154 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8476, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "269 Training acc 0.72 loss 0.5820645063251754 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8476, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "270 Training acc 0.72 loss 0.5820779906806042 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8476, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "271 Training acc 0.72 loss 0.5820915125300312 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8475, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "272 Training acc 0.72 loss 0.5821050834886158 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8475, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "273 Training acc 0.72 loss 0.5821186806777957 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8475, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "274 Training acc 0.72 loss 0.58213230495365 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8475, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "275 Training acc 0.72 loss 0.5821459607667208 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8474, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "276 Training acc 0.72 loss 0.5821596450975961 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8474, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "277 Training acc 0.72 loss 0.5821733611395055 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8474, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "278 Training acc 0.72 loss 0.5821871115792241 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8474, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "279 Training acc 0.72 loss 0.582200904528195 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8473, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "280 Training acc 0.72 loss 0.5822147430521949 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8473, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "281 Training acc 0.72 loss 0.5822286192704297 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8473, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "282 Training acc 0.72 loss 0.5822425317440425 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8472, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "283 Training acc 0.72 loss 0.582256479561495 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8472, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "284 Training acc 0.72 loss 0.5822704647254692 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8472, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "285 Training acc 0.72 loss 0.5822844856290452 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8472, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "286 Training acc 0.72 loss 0.5822985466156494 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8471, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "287 Training acc 0.72 loss 0.5823126417594966 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8471, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "288 Training acc 0.72 loss 0.5823267725353507 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8471, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "289 Training acc 0.72 loss 0.5823409310861156 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8471, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "290 Training acc 0.72 loss 0.5823551047488258 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8470, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "291 Training acc 0.72 loss 0.5823693014297847 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8470, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "292 Training acc 0.72 loss 0.582383522978466 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8470, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "293 Training acc 0.72 loss 0.5823977726317145 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8470, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "294 Training acc 0.72 loss 0.582412046101471 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8469, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "295 Training acc 0.72 loss 0.5824263511134831 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8469, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "296 Training acc 0.72 loss 0.5824406832385404 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8469, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "297 Training acc 0.72 loss 0.5824550432828113 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8468, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "298 Training acc 0.72 loss 0.5824694367821487 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8468, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "299 Training acc 0.72 loss 0.58248384424212 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8468, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "300 Training acc 0.72 loss 0.5824982857608617 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8468, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "301 Training acc 0.72 loss 0.5825127586527366 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8467, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "302 Training acc 0.72 loss 0.582527259584223 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8467, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "303 Training acc 0.72 loss 0.5825417802874997 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8467, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "304 Training acc 0.72 loss 0.5825563257515358 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8467, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "305 Training acc 0.72 loss 0.5825709045563761 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8466, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "306 Training acc 0.72 loss 0.5825855203159261 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8466, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "307 Training acc 0.72 loss 0.5826001603105191 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8466, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "308 Training acc 0.72 loss 0.582614829430032 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8465, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "309 Training acc 0.72 loss 0.5826295286823089 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8465, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "310 Training acc 0.72 loss 0.5826442567294895 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8465, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "311 Training acc 0.72 loss 0.5826590152724261 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8465, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "312 Training acc 0.72 loss 0.5826738085458412 50\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "313 Training acc 0.72 loss 0.5826886367355562 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "314 Training acc 0.72 loss 0.5827034904441181 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "315 Training acc 0.72 loss 0.5827183643943277 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "316 Training acc 0.72 loss 0.5827332691606423 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8463, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "317 Training acc 0.72 loss 0.5827482042440691 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8463, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "318 Training acc 0.72 loss 0.5827631718157226 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8463, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "319 Training acc 0.72 loss 0.5827781744703504 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8462, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "320 Training acc 0.72 loss 0.5827932101335161 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8462, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "321 Training acc 0.72 loss 0.5828082751076699 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8462, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "322 Training acc 0.72 loss 0.5828233707725022 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8462, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "323 Training acc 0.72 loss 0.5828384928831812 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8461, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "324 Training acc 0.72 loss 0.5828536412372264 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8461, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "325 Training acc 0.72 loss 0.5828688144392496 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8461, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "326 Training acc 0.72 loss 0.5828840117071747 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8460, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "327 Training acc 0.72 loss 0.5828992375537351 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8460, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "328 Training acc 0.72 loss 0.5829144885788895 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8460, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "329 Training acc 0.72 loss 0.5829297734957894 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8460, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "330 Training acc 0.72 loss 0.5829450887035548 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8459, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "331 Training acc 0.72 loss 0.5829604362724036 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8459, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "332 Training acc 0.72 loss 0.5829758137874133 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8459, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "333 Training acc 0.72 loss 0.582991217245707 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8458, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "334 Training acc 0.72 loss 0.5830066512729544 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8458, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "335 Training acc 0.72 loss 0.5830221252240707 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8458, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "336 Training acc 0.72 loss 0.5830376428035617 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8458, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "337 Training acc 0.72 loss 0.5830531875025369 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8457, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "338 Training acc 0.72 loss 0.5830687573360539 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8457, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "339 Training acc 0.72 loss 0.5830843653563782 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8457, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "340 Training acc 0.72 loss 0.5831000045810307 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "341 Training acc 0.72 loss 0.5831156787001285 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "342 Training acc 0.72 loss 0.5831313801959199 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "343 Training acc 0.72 loss 0.5831471158169612 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "344 Training acc 0.72 loss 0.5831628858491891 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8455, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "345 Training acc 0.72 loss 0.5831786956084728 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8455, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "346 Training acc 0.72 loss 0.5831945334362166 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8455, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "347 Training acc 0.72 loss 0.5832104054743275 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8454, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "348 Training acc 0.72 loss 0.5832263223127465 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8454, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "349 Training acc 0.72 loss 0.5832422829558678 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8454, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "350 Training acc 0.72 loss 0.5832582749966028 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "351 Training acc 0.72 loss 0.5832742904368485 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "352 Training acc 0.72 loss 0.583290337094113 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "353 Training acc 0.72 loss 0.5833064278577021 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "354 Training acc 0.72 loss 0.5833225577773294 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8452, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "355 Training acc 0.72 loss 0.5833387224537455 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8452, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "356 Training acc 0.72 loss 0.5833549287165826 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8452, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "357 Training acc 0.72 loss 0.5833711652358854 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8451, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "358 Training acc 0.72 loss 0.5833874363269166 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8451, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "359 Training acc 0.72 loss 0.5834037501266763 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8451, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "360 Training acc 0.72 loss 0.5834200963707219 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8450, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "361 Training acc 0.72 loss 0.5834364820856635 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8450, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "362 Training acc 0.72 loss 0.5834528822521837 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8450, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "363 Training acc 0.72 loss 0.5834693066975829 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8450, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "364 Training acc 0.72 loss 0.5834857491609013 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "365 Training acc 0.72 loss 0.5835022299586998 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "366 Training acc 0.72 loss 0.5835187358670484 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8449, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "367 Training acc 0.72 loss 0.5835352748903311 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8448, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "368 Training acc 0.72 loss 0.5835518463839403 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8448, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "369 Training acc 0.72 loss 0.5835684485199896 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8448, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "370 Training acc 0.72 loss 0.5835850837472792 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8447, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "371 Training acc 0.72 loss 0.5836017503641167 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8447, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "372 Training acc 0.72 loss 0.5836184486183327 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8447, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "373 Training acc 0.72 loss 0.5836351788529834 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8446, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "374 Training acc 0.72 loss 0.5836519338771017 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8446, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "375 Training acc 0.72 loss 0.583668724627791 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8446, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "376 Training acc 0.72 loss 0.5836855551353608 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "377 Training acc 0.72 loss 0.5837024142327341 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "378 Training acc 0.72 loss 0.5837193048483964 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "379 Training acc 0.72 loss 0.5837362492759439 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "380 Training acc 0.72 loss 0.5837532223831595 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8444, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "381 Training acc 0.72 loss 0.5837702445231596 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8444, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "382 Training acc 0.72 loss 0.5837873092805362 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8444, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "383 Training acc 0.72 loss 0.5838044095091698 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8443, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "384 Training acc 0.72 loss 0.5838215212309441 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8443, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "385 Training acc 0.72 loss 0.583838675586221 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8443, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "386 Training acc 0.72 loss 0.5838558607515247 50\n",
      "lamda tensor(0.6000, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8442, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "387 Training acc 0.72 loss 0.5838730863074146 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8442, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "388 Training acc 0.72 loss 0.5838903516500303 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8442, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "389 Training acc 0.72 loss 0.5839076473891092 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8441, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "390 Training acc 0.72 loss 0.5839249767534936 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8441, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "391 Training acc 0.72 loss 0.5839423454838412 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8441, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "392 Training acc 0.72 loss 0.5839597693556513 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8440, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "393 Training acc 0.72 loss 0.5839772345689257 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8440, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "394 Training acc 0.72 loss 0.583994746831246 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8440, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "395 Training acc 0.72 loss 0.5840122908330145 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8439, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "396 Training acc 0.72 loss 0.5840298617624261 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8439, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "397 Training acc 0.72 loss 0.5840474668126321 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8439, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "398 Training acc 0.72 loss 0.5840651105725586 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8438, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "399 Training acc 0.72 loss 0.584082793593318 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8438, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "400 Training acc 0.72 loss 0.5841005066805688 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8438, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "401 Training acc 0.72 loss 0.5841182467365365 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8437, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "402 Training acc 0.72 loss 0.5841360261533254 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8437, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "403 Training acc 0.72 loss 0.584153849096685 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8437, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "404 Training acc 0.72 loss 0.5841717126396704 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8436, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "405 Training acc 0.72 loss 0.5841896275639753 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8436, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "406 Training acc 0.72 loss 0.5842075896158347 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8436, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "407 Training acc 0.72 loss 0.5842256008970907 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8435, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "408 Training acc 0.72 loss 0.5842436484417346 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8435, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "409 Training acc 0.72 loss 0.5842617356250539 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8435, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "410 Training acc 0.72 loss 0.5842798466861744 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "411 Training acc 0.72 loss 0.5842979743996084 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "412 Training acc 0.72 loss 0.5843161505294567 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "413 Training acc 0.72 loss 0.5843343764209562 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8433, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "414 Training acc 0.72 loss 0.5843526396048577 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8433, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "415 Training acc 0.72 loss 0.584370924754603 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8433, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "416 Training acc 0.72 loss 0.5843892505497762 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8432, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "417 Training acc 0.72 loss 0.5844076171178219 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8432, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "418 Training acc 0.72 loss 0.5844260133989374 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8432, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "419 Training acc 0.72 loss 0.5844444275399394 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "420 Training acc 0.72 loss 0.5844628546102648 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "421 Training acc 0.72 loss 0.5844813238139595 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "422 Training acc 0.72 loss 0.5844998243605166 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8430, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "423 Training acc 0.72 loss 0.5845183546478229 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8430, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "424 Training acc 0.72 loss 0.5845369146531917 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8430, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "425 Training acc 0.72 loss 0.5845555000151337 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8429, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "426 Training acc 0.72 loss 0.584574118904193 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8429, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "427 Training acc 0.72 loss 0.5845927724823642 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8429, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "428 Training acc 0.72 loss 0.584611455208745 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8428, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "429 Training acc 0.72 loss 0.5846301729808007 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8428, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "430 Training acc 0.72 loss 0.5846489378929973 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8428, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "431 Training acc 0.72 loss 0.5846677437777001 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "432 Training acc 0.72 loss 0.5846865939268745 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "433 Training acc 0.72 loss 0.5847054797901611 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8427, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "434 Training acc 0.72 loss 0.5847244072873372 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8426, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "435 Training acc 0.72 loss 0.5847433719699939 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8426, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "436 Training acc 0.72 loss 0.5847623662950586 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8426, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "437 Training acc 0.72 loss 0.5847814169469624 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8425, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "438 Training acc 0.72 loss 0.5848004883636867 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8425, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "439 Training acc 0.72 loss 0.5848196168140607 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8424, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "440 Training acc 0.72 loss 0.5848387885552093 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8424, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "441 Training acc 0.72 loss 0.5848580184028717 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8424, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "442 Training acc 0.72 loss 0.5848772757672411 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8423, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "443 Training acc 0.72 loss 0.5848965711573108 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8423, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "444 Training acc 0.72 loss 0.5849159093971785 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8423, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "445 Training acc 0.72 loss 0.5849352922021634 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8422, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "446 Training acc 0.72 loss 0.584954722225171 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8422, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "447 Training acc 0.72 loss 0.5849742030542149 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8422, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "448 Training acc 0.72 loss 0.5849937203481645 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8421, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "449 Training acc 0.72 loss 0.5850132665045189 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8421, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "450 Training acc 0.72 loss 0.5850328479445456 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8420, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "451 Training acc 0.72 loss 0.5850524609489285 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8420, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "452 Training acc 0.72 loss 0.5850721230769159 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8420, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "453 Training acc 0.72 loss 0.5850918133161929 50\n",
      "lamda tensor(0.5999, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8419, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "454 Training acc 0.72 loss 0.5851115475785006 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8419, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "455 Training acc 0.72 loss 0.5851313248663821 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8419, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "456 Training acc 0.72 loss 0.5851511269175105 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8418, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "457 Training acc 0.72 loss 0.585170942514273 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8418, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "458 Training acc 0.72 loss 0.5851908117628278 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8418, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "459 Training acc 0.72 loss 0.5852107205554135 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8417, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "460 Training acc 0.72 loss 0.5852306760982099 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8417, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "461 Training acc 0.72 loss 0.5852506823138816 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8416, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "462 Training acc 0.72 loss 0.5852707318500905 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8416, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "463 Training acc 0.72 loss 0.5852908158160712 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8416, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "464 Training acc 0.72 loss 0.5853109475020405 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "465 Training acc 0.72 loss 0.5853311159139846 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "466 Training acc 0.72 loss 0.5853513284821455 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "467 Training acc 0.72 loss 0.5853715888762119 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8414, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "468 Training acc 0.72 loss 0.5853918995947524 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8414, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "469 Training acc 0.72 loss 0.5854122555017414 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8413, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "470 Training acc 0.72 loss 0.5854326595100806 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8413, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "471 Training acc 0.72 loss 0.5854531212704596 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8413, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "472 Training acc 0.72 loss 0.5854736322383098 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8412, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "473 Training acc 0.72 loss 0.5854941794191415 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8412, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "474 Training acc 0.72 loss 0.5855147568271755 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8412, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "475 Training acc 0.72 loss 0.5855353743368495 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8411, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "476 Training acc 0.72 loss 0.5855560359793921 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8411, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "477 Training acc 0.72 loss 0.5855767315315173 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8410, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "478 Training acc 0.72 loss 0.5855974389290081 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8410, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "479 Training acc 0.72 loss 0.5856181849635437 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8410, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "480 Training acc 0.72 loss 0.5856389640337155 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8409, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "481 Training acc 0.72 loss 0.5856597908550676 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8409, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "482 Training acc 0.72 loss 0.585680659388149 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8409, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "483 Training acc 0.72 loss 0.585701560268965 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8408, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "484 Training acc 0.72 loss 0.5857225309052073 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8408, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "485 Training acc 0.72 loss 0.5857435498095978 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8407, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "486 Training acc 0.72 loss 0.5857645996018619 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8407, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "487 Training acc 0.72 loss 0.5857857034423851 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8407, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "488 Training acc 0.72 loss 0.5858068527537615 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8406, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "489 Training acc 0.72 loss 0.5858280565253252 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8406, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "490 Training acc 0.72 loss 0.5858493174673659 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8405, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "491 Training acc 0.72 loss 0.5858706329566755 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8405, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "492 Training acc 0.72 loss 0.5858920098482674 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8405, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "493 Training acc 0.72 loss 0.5859134274587989 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8404, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "494 Training acc 0.72 loss 0.585934875357239 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8404, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "495 Training acc 0.72 loss 0.5859563706144418 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8403, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "496 Training acc 0.72 loss 0.5859779102540063 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8403, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "497 Training acc 0.72 loss 0.5859994956119734 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8403, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "498 Training acc 0.72 loss 0.5860211198024787 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8402, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "499 Training acc 0.72 loss 0.5860427867630654 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8402, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "500 Training acc 0.72 loss 0.586064510830087 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8401, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "501 Training acc 0.72 loss 0.5860862861701027 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8401, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "502 Training acc 0.72 loss 0.5861080970907145 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8401, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "503 Training acc 0.72 loss 0.586129952890936 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8400, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "504 Training acc 0.72 loss 0.5861518593520718 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8400, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "505 Training acc 0.72 loss 0.5861738016237619 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8399, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "506 Training acc 0.72 loss 0.5861957715302506 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8399, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "507 Training acc 0.72 loss 0.5862177775986132 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8399, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "508 Training acc 0.72 loss 0.5862398089716847 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8398, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "509 Training acc 0.72 loss 0.5862618748935264 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8398, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "510 Training acc 0.72 loss 0.5862839802374266 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8397, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "511 Training acc 0.72 loss 0.5863061446624227 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8397, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "512 Training acc 0.72 loss 0.5863283446864396 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8397, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "513 Training acc 0.72 loss 0.5863505961815497 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8396, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "514 Training acc 0.72 loss 0.5863729160297233 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8396, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "515 Training acc 0.72 loss 0.5863952628757043 50\n",
      "lamda tensor(0.5998, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8395, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "516 Training acc 0.72 loss 0.586417665455799 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8395, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "517 Training acc 0.72 loss 0.5864401062324116 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8395, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "518 Training acc 0.72 loss 0.5864626047891289 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8394, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "519 Training acc 0.72 loss 0.5864851576374425 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8394, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "520 Training acc 0.72 loss 0.5865077543424273 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8393, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "521 Training acc 0.72 loss 0.5865304252841943 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8393, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "522 Training acc 0.72 loss 0.5865531486071665 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8393, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "523 Training acc 0.72 loss 0.586575931115057 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8392, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "524 Training acc 0.72 loss 0.5865987696187439 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8392, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "525 Training acc 0.72 loss 0.5866216671423439 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8391, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "526 Training acc 0.72 loss 0.5866446078044191 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8391, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "527 Training acc 0.72 loss 0.5866675937235709 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8390, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "528 Training acc 0.72 loss 0.5866906231375528 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8390, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "529 Training acc 0.72 loss 0.5867136909253974 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8390, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "530 Training acc 0.72 loss 0.586736807098568 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8389, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "531 Training acc 0.72 loss 0.5867599900545237 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8389, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "532 Training acc 0.72 loss 0.5867832285315768 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8388, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "533 Training acc 0.72 loss 0.5868065133686327 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8388, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "534 Training acc 0.72 loss 0.5868298448274524 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "535 Training acc 0.72 loss 0.5868532238155837 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "536 Training acc 0.72 loss 0.5868766560753619 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "537 Training acc 0.72 loss 0.5869001445754164 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8386, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "538 Training acc 0.72 loss 0.586923676674791 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8386, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "539 Training acc 0.72 loss 0.5869472530161132 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8385, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "540 Training acc 0.72 loss 0.5869708668431001 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8385, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "541 Training acc 0.72 loss 0.5869945219709957 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8384, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "542 Training acc 0.72 loss 0.5870182171113546 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8384, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "543 Training acc 0.72 loss 0.5870419339694288 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8384, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "544 Training acc 0.72 loss 0.5870656937851814 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "545 Training acc 0.72 loss 0.5870894999001695 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "546 Training acc 0.72 loss 0.5871133467824418 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8382, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "547 Training acc 0.72 loss 0.5871372552764959 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8382, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "548 Training acc 0.72 loss 0.5871612136508177 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8381, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "549 Training acc 0.72 loss 0.5871852285626473 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8381, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "550 Training acc 0.72 loss 0.5872092979670053 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8381, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "551 Training acc 0.72 loss 0.5872333758000006 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8380, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "552 Training acc 0.72 loss 0.587257498292154 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8380, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "553 Training acc 0.72 loss 0.5872816750680442 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "554 Training acc 0.72 loss 0.5873059026070565 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "555 Training acc 0.72 loss 0.5873301966715283 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8378, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "556 Training acc 0.72 loss 0.5873545215692045 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8378, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "557 Training acc 0.72 loss 0.5873788694185251 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8377, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "558 Training acc 0.72 loss 0.5874032551896681 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8377, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "559 Training acc 0.72 loss 0.5874276872197379 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8377, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "560 Training acc 0.72 loss 0.5874521733958482 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8376, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "561 Training acc 0.72 loss 0.5874767116014467 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8376, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "562 Training acc 0.72 loss 0.5875013079157485 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8375, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "563 Training acc 0.72 loss 0.5875259437964991 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8375, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "564 Training acc 0.72 loss 0.587550636699377 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8374, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "565 Training acc 0.72 loss 0.5875753995187458 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8374, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "566 Training acc 0.72 loss 0.5876002134549307 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "567 Training acc 0.72 loss 0.5876250646058985 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "568 Training acc 0.72 loss 0.5876499837290549 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "569 Training acc 0.72 loss 0.5876749775668714 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8372, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "570 Training acc 0.72 loss 0.5877000325004533 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8372, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "571 Training acc 0.72 loss 0.587725141506805 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8371, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "572 Training acc 0.72 loss 0.587750291850505 50\n",
      "lamda tensor(0.5997, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8371, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "573 Training acc 0.72 loss 0.5877754935130025 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8370, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "574 Training acc 0.72 loss 0.5878007294318581 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8370, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "575 Training acc 0.72 loss 0.5878260089000041 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8369, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "576 Training acc 0.72 loss 0.5878513312304398 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8369, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "577 Training acc 0.72 loss 0.5878767046294938 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8368, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "578 Training acc 0.72 loss 0.5879021363282289 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8368, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "579 Training acc 0.72 loss 0.5879276384107984 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8367, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "580 Training acc 0.72 loss 0.5879532050328212 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8367, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "581 Training acc 0.72 loss 0.5879788159569012 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8367, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "582 Training acc 0.72 loss 0.588004466815301 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8366, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "583 Training acc 0.72 loss 0.5880301821935972 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8366, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "584 Training acc 0.72 loss 0.5880559766715273 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8365, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "585 Training acc 0.72 loss 0.5880818220172135 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8365, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "586 Training acc 0.72 loss 0.5881077307379856 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "587 Training acc 0.72 loss 0.5881337004774623 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "588 Training acc 0.72 loss 0.5881597266996699 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8363, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "589 Training acc 0.72 loss 0.5881857929278205 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8363, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "590 Training acc 0.72 loss 0.5882119227500849 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8362, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "591 Training acc 0.72 loss 0.588238120238868 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8362, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "592 Training acc 0.72 loss 0.5882643679605171 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8361, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "593 Training acc 0.72 loss 0.5882906449587224 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8361, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "594 Training acc 0.72 loss 0.5883169697670629 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8360, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "595 Training acc 0.72 loss 0.5883433503499078 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8360, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "596 Training acc 0.72 loss 0.5883697721396122 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8359, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "597 Training acc 0.72 loss 0.588396226927922 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8359, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "598 Training acc 0.72 loss 0.5884226991441872 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "599 Training acc 0.72 loss 0.5884492305939851 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "600 Training acc 0.72 loss 0.588475839119087 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "601 Training acc 0.72 loss 0.5885025057071123 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8357, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "602 Training acc 0.72 loss 0.5885292162496233 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8357, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "603 Training acc 0.72 loss 0.5885559644975462 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8356, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "604 Training acc 0.72 loss 0.5885827487488496 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8356, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "605 Training acc 0.72 loss 0.5886095926531285 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8355, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "606 Training acc 0.72 loss 0.5886364805545425 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8355, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "607 Training acc 0.72 loss 0.5886634246446507 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8354, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "608 Training acc 0.72 loss 0.5886904281133984 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8354, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "609 Training acc 0.72 loss 0.588717492299481 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8353, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "610 Training acc 0.72 loss 0.5887446119695583 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8353, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "611 Training acc 0.72 loss 0.588771798270846 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8352, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "612 Training acc 0.72 loss 0.5887990239949362 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8352, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "613 Training acc 0.72 loss 0.5888262930048704 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8351, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "614 Training acc 0.72 loss 0.5888536286313 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8351, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "615 Training acc 0.72 loss 0.5888810238645188 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8350, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "616 Training acc 0.72 loss 0.5889084671877026 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8350, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "617 Training acc 0.72 loss 0.5889359525498428 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8349, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "618 Training acc 0.72 loss 0.5889634821687291 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8349, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "619 Training acc 0.72 loss 0.5889910736087705 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8348, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "620 Training acc 0.72 loss 0.5890187189565684 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8348, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "621 Training acc 0.72 loss 0.5890464258055165 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8347, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "622 Training acc 0.72 loss 0.5890741907058739 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8347, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "623 Training acc 0.72 loss 0.5891020251527654 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8346, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "624 Training acc 0.72 loss 0.5891299181363796 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8346, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "625 Training acc 0.72 loss 0.5891578726279472 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8345, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "626 Training acc 0.72 loss 0.5891858776539391 50\n",
      "lamda tensor(0.5996, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8345, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "627 Training acc 0.72 loss 0.5892139240495861 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8344, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "628 Training acc 0.72 loss 0.5892420128380998 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8344, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "629 Training acc 0.72 loss 0.589270153200979 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "630 Training acc 0.72 loss 0.5892983525051056 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "631 Training acc 0.72 loss 0.5893266128565487 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8342, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "632 Training acc 0.72 loss 0.589354936435121 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8342, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "633 Training acc 0.72 loss 0.5893833370497443 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8341, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "634 Training acc 0.72 loss 0.5894118086783949 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8341, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "635 Training acc 0.72 loss 0.5894403379930647 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8340, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "636 Training acc 0.72 loss 0.5894689183580951 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8340, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "637 Training acc 0.72 loss 0.5894975516025004 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8339, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "638 Training acc 0.72 loss 0.5895262483585981 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8339, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "639 Training acc 0.72 loss 0.5895549924567791 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8338, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "640 Training acc 0.72 loss 0.5895837602030031 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8338, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "641 Training acc 0.72 loss 0.5896125999076737 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8337, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "642 Training acc 0.72 loss 0.5896414998385261 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8336, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "643 Training acc 0.72 loss 0.5896704611410661 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8336, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "644 Training acc 0.72 loss 0.5896994951515202 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8335, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "645 Training acc 0.72 loss 0.5897285780567358 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8335, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "646 Training acc 0.72 loss 0.589757710326784 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8334, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "647 Training acc 0.72 loss 0.5897869283760946 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8334, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "648 Training acc 0.72 loss 0.5898162152407715 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8333, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "649 Training acc 0.72 loss 0.5898455651217274 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8333, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "650 Training acc 0.72 loss 0.5898749747459117 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8332, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "651 Training acc 0.72 loss 0.5899044547907117 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8332, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "652 Training acc 0.72 loss 0.5899339849964621 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8331, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "653 Training acc 0.72 loss 0.5899635675415821 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8331, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "654 Training acc 0.72 loss 0.5899931973455357 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "655 Training acc 0.72 loss 0.5900228723247908 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "656 Training acc 0.72 loss 0.5900526148615786 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8329, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "657 Training acc 0.72 loss 0.5900824339907769 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8329, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "658 Training acc 0.72 loss 0.590112311151173 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8328, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "659 Training acc 0.72 loss 0.5901422669065233 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8327, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "660 Training acc 0.72 loss 0.5901722948929465 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8327, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "661 Training acc 0.72 loss 0.5902023937474596 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "662 Training acc 0.72 loss 0.5902325692844106 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "663 Training acc 0.72 loss 0.5902628156449894 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "664 Training acc 0.72 loss 0.590293126431525 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "665 Training acc 0.72 loss 0.590323513076271 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8324, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "666 Training acc 0.72 loss 0.590353976670895 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8324, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "667 Training acc 0.72 loss 0.590384512474898 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8323, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "668 Training acc 0.72 loss 0.5904151300577689 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8323, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "669 Training acc 0.72 loss 0.5904458328814107 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8322, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "670 Training acc 0.72 loss 0.5904765897141468 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8322, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "671 Training acc 0.72 loss 0.5905074080878925 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8321, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "672 Training acc 0.72 loss 0.5905383015782993 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "673 Training acc 0.72 loss 0.5905692916347764 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "674 Training acc 0.72 loss 0.5906003598603842 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8319, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "675 Training acc 0.72 loss 0.5906315170798918 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8319, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "676 Training acc 0.72 loss 0.5906627425108218 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8318, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "677 Training acc 0.72 loss 0.5906940491376339 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8318, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "678 Training acc 0.72 loss 0.5907254403294268 50\n",
      "lamda tensor(0.5995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8317, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "679 Training acc 0.72 loss 0.5907569158912461 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8316, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "680 Training acc 0.72 loss 0.5907884709413314 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8316, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "681 Training acc 0.72 loss 0.5908201373606606 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8315, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "682 Training acc 0.72 loss 0.5908519197195585 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8315, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "683 Training acc 0.72 loss 0.5908838085263697 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8314, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "684 Training acc 0.72 loss 0.5909157840768976 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8314, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "685 Training acc 0.72 loss 0.590947861898547 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8313, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "686 Training acc 0.72 loss 0.5909800158188951 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8313, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "687 Training acc 0.72 loss 0.5910122492063113 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8312, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "688 Training acc 0.72 loss 0.5910445685869392 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8311, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "689 Training acc 0.72 loss 0.5910769698041615 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8311, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "690 Training acc 0.72 loss 0.5911094643441103 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8310, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "691 Training acc 0.72 loss 0.5911420661585753 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8310, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "692 Training acc 0.72 loss 0.5911747651612551 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8309, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "693 Training acc 0.72 loss 0.5912075797235253 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "694 Training acc 0.72 loss 0.5912404969872679 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "695 Training acc 0.72 loss 0.5912734793720611 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8307, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "696 Training acc 0.72 loss 0.5913065631392842 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8307, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "697 Training acc 0.72 loss 0.5913397470881698 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8306, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "698 Training acc 0.72 loss 0.5913730340702953 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8305, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "699 Training acc 0.72 loss 0.5914063923909813 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8305, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "700 Training acc 0.72 loss 0.5914398402664305 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8304, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "701 Training acc 0.72 loss 0.5914733766470596 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8304, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "702 Training acc 0.72 loss 0.5915069886130125 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8303, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "703 Training acc 0.72 loss 0.5915406823091125 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8303, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "704 Training acc 0.72 loss 0.5915744523896785 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8302, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "705 Training acc 0.72 loss 0.5916083014999257 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8301, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "706 Training acc 0.72 loss 0.5916422503849482 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8301, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "707 Training acc 0.72 loss 0.5916762794157376 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8300, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "708 Training acc 0.72 loss 0.59171038866592 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8300, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "709 Training acc 0.72 loss 0.5917445905479964 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8299, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "710 Training acc 0.72 loss 0.5917788997996621 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8298, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "711 Training acc 0.72 loss 0.5918132812082748 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8298, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "712 Training acc 0.72 loss 0.5918477358333999 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8297, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "713 Training acc 0.72 loss 0.5918822586090551 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8296, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "714 Training acc 0.72 loss 0.5919168625145645 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8296, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "715 Training acc 0.72 loss 0.5919515555421144 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8295, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "716 Training acc 0.72 loss 0.5919863412587747 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8295, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "717 Training acc 0.72 loss 0.5920212019046611 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8294, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "718 Training acc 0.72 loss 0.592056131345445 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8293, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "719 Training acc 0.72 loss 0.5920911399480141 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8293, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "720 Training acc 0.72 loss 0.5921262215597687 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8292, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "721 Training acc 0.72 loss 0.5921613831269094 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8292, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "722 Training acc 0.72 loss 0.5921966232052783 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8291, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "723 Training acc 0.72 loss 0.5922319336395183 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8290, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "724 Training acc 0.72 loss 0.5922673302639735 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8290, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "725 Training acc 0.72 loss 0.5923028053647666 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8289, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "726 Training acc 0.72 loss 0.5923383428461865 50\n",
      "lamda tensor(0.5994, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8288, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "727 Training acc 0.72 loss 0.5923739600966647 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8288, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "728 Training acc 0.72 loss 0.5924096532672695 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8287, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "729 Training acc 0.72 loss 0.5924454404320423 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8286, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "730 Training acc 0.72 loss 0.5924813017066504 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8286, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "731 Training acc 0.72 loss 0.5925172195300985 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8285, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "732 Training acc 0.72 loss 0.5925532125580995 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8285, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "733 Training acc 0.72 loss 0.5925892802509217 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8284, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "734 Training acc 0.72 loss 0.592625425685407 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8283, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "735 Training acc 0.72 loss 0.5926616658996792 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8283, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "736 Training acc 0.72 loss 0.5926980041574932 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8282, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "737 Training acc 0.72 loss 0.5927344442268256 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8281, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "738 Training acc 0.72 loss 0.5927709622432037 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8281, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "739 Training acc 0.72 loss 0.5928075656686695 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8280, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "740 Training acc 0.72 loss 0.5928442213817539 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8279, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "741 Training acc 0.72 loss 0.5928809552820737 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8279, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "742 Training acc 0.72 loss 0.592917792241323 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8278, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "743 Training acc 0.72 loss 0.592954745755137 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8277, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "744 Training acc 0.72 loss 0.5929917866399227 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8277, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "745 Training acc 0.72 loss 0.5930289179988825 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8276, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "746 Training acc 0.72 loss 0.59306615854602 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8276, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "747 Training acc 0.72 loss 0.5931034861068191 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8275, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "748 Training acc 0.72 loss 0.5931408956475607 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8274, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "749 Training acc 0.72 loss 0.593178387632675 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8274, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "750 Training acc 0.72 loss 0.5932159760457002 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8273, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "751 Training acc 0.72 loss 0.593253684644038 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8272, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "752 Training acc 0.72 loss 0.5932915084118109 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8272, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "753 Training acc 0.72 loss 0.5933294288519183 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8271, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "754 Training acc 0.72 loss 0.5933674087070986 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8270, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "755 Training acc 0.72 loss 0.593405479266081 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8270, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "756 Training acc 0.72 loss 0.5934436364488517 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8269, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "757 Training acc 0.72 loss 0.5934818922286079 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8268, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "758 Training acc 0.72 loss 0.5935202514276517 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8268, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "759 Training acc 0.72 loss 0.593558688167484 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8267, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "760 Training acc 0.72 loss 0.5935972185463263 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8266, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "761 Training acc 0.72 loss 0.5936358517409094 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8266, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "762 Training acc 0.72 loss 0.5936745665594533 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8265, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "763 Training acc 0.72 loss 0.5937133243078352 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8264, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "764 Training acc 0.72 loss 0.5937521691473487 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8263, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "765 Training acc 0.72 loss 0.5937910890341174 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8263, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "766 Training acc 0.72 loss 0.5938301088092033 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8262, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "767 Training acc 0.72 loss 0.5938692306973797 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8261, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "768 Training acc 0.72 loss 0.5939084395963754 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8261, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "769 Training acc 0.72 loss 0.5939476861033378 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8260, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "770 Training acc 0.72 loss 0.5939869784867724 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8259, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "771 Training acc 0.72 loss 0.5940263707209384 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8259, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "772 Training acc 0.72 loss 0.5940658956566699 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8258, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "773 Training acc 0.72 loss 0.5941055193304335 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8257, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "774 Training acc 0.72 loss 0.5941452412715553 50\n",
      "lamda tensor(0.5993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8257, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "775 Training acc 0.72 loss 0.594185052807218 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8256, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "776 Training acc 0.72 loss 0.5942249636860745 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8255, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "777 Training acc 0.72 loss 0.594264949356989 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8254, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "778 Training acc 0.72 loss 0.5943050379115158 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8254, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "779 Training acc 0.72 loss 0.5943452410296357 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8253, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "780 Training acc 0.72 loss 0.5943855283880696 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8252, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "781 Training acc 0.72 loss 0.5944259299083047 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8252, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "782 Training acc 0.72 loss 0.5944664338094072 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8251, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "783 Training acc 0.72 loss 0.5945070366173216 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8250, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "784 Training acc 0.72 loss 0.5945477523708591 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8250, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "785 Training acc 0.72 loss 0.594588560889896 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8249, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "786 Training acc 0.72 loss 0.5946294824818309 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8248, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "787 Training acc 0.72 loss 0.5946705158372569 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8247, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "788 Training acc 0.72 loss 0.594711664621248 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8247, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "789 Training acc 0.72 loss 0.5947529267580859 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8246, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "790 Training acc 0.72 loss 0.5947943000983618 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8245, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "791 Training acc 0.72 loss 0.5948358023246351 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8245, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "792 Training acc 0.72 loss 0.5948773624753485 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8244, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "793 Training acc 0.72 loss 0.5949189931432141 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8243, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "794 Training acc 0.72 loss 0.5949607424144998 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8242, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "795 Training acc 0.72 loss 0.5950026130894579 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8242, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "796 Training acc 0.72 loss 0.5950445807120955 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8241, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "797 Training acc 0.72 loss 0.5950866520434998 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8240, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "798 Training acc 0.72 loss 0.5951288212978211 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8239, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "799 Training acc 0.72 loss 0.5951710791160004 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8239, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "800 Training acc 0.72 loss 0.5952134414647184 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8238, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "801 Training acc 0.72 loss 0.5952558990916329 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8237, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "802 Training acc 0.72 loss 0.5952984684574019 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8236, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "803 Training acc 0.72 loss 0.5953411284676154 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8236, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "804 Training acc 0.72 loss 0.5953839131706856 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8235, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "805 Training acc 0.72 loss 0.595426812827188 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8234, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "806 Training acc 0.72 loss 0.5954698237608442 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8233, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "807 Training acc 0.72 loss 0.5955129573131429 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8233, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "808 Training acc 0.72 loss 0.595556204922949 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8232, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "809 Training acc 0.72 loss 0.5955995652697803 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8231, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "810 Training acc 0.72 loss 0.5956430527797544 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8230, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "811 Training acc 0.72 loss 0.5956866454602602 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8230, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "812 Training acc 0.72 loss 0.5957303401491426 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8229, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "813 Training acc 0.72 loss 0.5957741234919275 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8228, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "814 Training acc 0.72 loss 0.5958180211793672 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8227, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "815 Training acc 0.72 loss 0.5958620063670145 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8227, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "816 Training acc 0.72 loss 0.595906084487014 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8226, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "817 Training acc 0.72 loss 0.595950273316299 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8225, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "818 Training acc 0.72 loss 0.595994572025687 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8224, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "819 Training acc 0.72 loss 0.5960389358108547 50\n",
      "lamda tensor(0.5992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8224, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "820 Training acc 0.72 loss 0.596083367064403 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8223, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "821 Training acc 0.72 loss 0.5961279146154502 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8222, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "822 Training acc 0.72 loss 0.5961725671540995 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8221, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "823 Training acc 0.72 loss 0.5962173277133537 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8221, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "824 Training acc 0.72 loss 0.596262204488446 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8220, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "825 Training acc 0.72 loss 0.5963071829467709 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8219, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "826 Training acc 0.72 loss 0.5963522408000087 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8218, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "827 Training acc 0.72 loss 0.5963973782607052 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8217, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "828 Training acc 0.72 loss 0.5964426120662849 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8217, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "829 Training acc 0.72 loss 0.5964879239694572 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8216, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "830 Training acc 0.72 loss 0.5965333735844686 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8215, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "831 Training acc 0.72 loss 0.596578922285854 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8214, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "832 Training acc 0.72 loss 0.596624546400002 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8214, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "833 Training acc 0.72 loss 0.596670285427907 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8213, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "834 Training acc 0.72 loss 0.5967161174623974 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8212, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "835 Training acc 0.72 loss 0.5967620093642299 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8211, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "836 Training acc 0.72 loss 0.59680799808485 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8210, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "837 Training acc 0.72 loss 0.5968540796619483 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8210, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "838 Training acc 0.72 loss 0.5969002211436201 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8209, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "839 Training acc 0.72 loss 0.596946458031264 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8208, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "840 Training acc 0.72 loss 0.5969928115566061 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8207, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "841 Training acc 0.72 loss 0.5970392636878455 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8206, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "842 Training acc 0.72 loss 0.5970858080627724 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8206, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "843 Training acc 0.72 loss 0.5971324166462999 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8205, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "844 Training acc 0.72 loss 0.5971790997327778 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8204, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "845 Training acc 0.72 loss 0.5972258722416898 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8203, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "846 Training acc 0.72 loss 0.5972727277103665 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8202, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "847 Training acc 0.72 loss 0.5973196363397089 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8202, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "848 Training acc 0.72 loss 0.5973666008457944 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8201, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "849 Training acc 0.72 loss 0.5974136320926311 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8200, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "850 Training acc 0.72 loss 0.5974607352859974 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8199, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "851 Training acc 0.72 loss 0.5975078905035442 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8198, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "852 Training acc 0.72 loss 0.597555062418305 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8198, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "853 Training acc 0.72 loss 0.5976023034563921 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8197, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "854 Training acc 0.72 loss 0.5976495956922683 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8196, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "855 Training acc 0.72 loss 0.5976968807810488 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8195, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "856 Training acc 0.72 loss 0.597744152385882 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8195, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "857 Training acc 0.72 loss 0.5977914285819087 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8194, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "858 Training acc 0.72 loss 0.5978387000903196 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8193, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "859 Training acc 0.72 loss 0.597885938549102 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8192, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "860 Training acc 0.72 loss 0.5979331460784503 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8191, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "861 Training acc 0.72 loss 0.5979802743012238 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8191, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "862 Training acc 0.72 loss 0.5980272933439904 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8190, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "863 Training acc 0.72 loss 0.598074145787492 50\n",
      "lamda tensor(0.5991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8189, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "864 Training acc 0.72 loss 0.5981208146740808 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8188, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "865 Training acc 0.72 loss 0.5981672641336443 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8188, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "866 Training acc 0.72 loss 0.5982134396153722 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8187, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "867 Training acc 0.72 loss 0.598259296387945 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8186, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "868 Training acc 0.72 loss 0.598304747235685 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8185, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "869 Training acc 0.72 loss 0.5983497098001611 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8185, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "870 Training acc 0.72 loss 0.5983940686577234 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8184, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "871 Training acc 0.72 loss 0.5984376766739709 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8183, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "872 Training acc 0.72 loss 0.5984803881741624 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8183, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "873 Training acc 0.72 loss 0.5985219850549048 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "874 Training acc 0.72 loss 0.5985622060561943 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "875 Training acc 0.72 loss 0.5986007179646714 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "876 Training acc 0.72 loss 0.5986370913436475 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "877 Training acc 0.72 loss 0.5986707926793805 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "878 Training acc 0.72 loss 0.5987011808873085 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "879 Training acc 0.72 loss 0.5987274456227663 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "880 Training acc 0.72 loss 0.5987486767514475 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "881 Training acc 0.72 loss 0.5987639914664804 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "882 Training acc 0.72 loss 0.5987727012284251 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "883 Training acc 0.72 loss 0.5987745516714786 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "884 Training acc 0.72 loss 0.5987699006363362 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "885 Training acc 0.72 loss 0.5987597465480402 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "886 Training acc 0.72 loss 0.5987454100197688 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8180, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "887 Training acc 0.72 loss 0.5987283151525926 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "888 Training acc 0.72 loss 0.5987097998164836 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "889 Training acc 0.72 loss 0.5986909564861097 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "890 Training acc 0.72 loss 0.5986726272009703 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "891 Training acc 0.72 loss 0.5986554417119461 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "892 Training acc 0.72 loss 0.5986398225542026 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "893 Training acc 0.72 loss 0.5986260435296208 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "894 Training acc 0.72 loss 0.5986142520500317 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "895 Training acc 0.72 loss 0.5986045058162164 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "896 Training acc 0.72 loss 0.598596790681561 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "897 Training acc 0.72 loss 0.5985910409686632 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "898 Training acc 0.72 loss 0.5985871510223503 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "899 Training acc 0.72 loss 0.5985849866412624 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "900 Training acc 0.72 loss 0.5985843914514655 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "901 Training acc 0.72 loss 0.5985851928623214 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "902 Training acc 0.72 loss 0.5985872078176643 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "903 Training acc 0.72 loss 0.5985902459381678 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "904 Training acc 0.72 loss 0.5985941118375242 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "905 Training acc 0.72 loss 0.5985986069343182 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "906 Training acc 0.72 loss 0.5986035319273241 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "907 Training acc 0.72 loss 0.5986086890147364 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "908 Training acc 0.72 loss 0.5986138861289132 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "909 Training acc 0.72 loss 0.5986189397813241 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "910 Training acc 0.72 loss 0.5986236771143858 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "911 Training acc 0.72 loss 0.5986279419809484 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "912 Training acc 0.72 loss 0.598631599671916 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "913 Training acc 0.72 loss 0.5986345381087234 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "914 Training acc 0.72 loss 0.5986366735441874 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "915 Training acc 0.72 loss 0.5986379555193994 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "916 Training acc 0.72 loss 0.5986383660419768 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "917 Training acc 0.72 loss 0.5986379195409391 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "918 Training acc 0.72 loss 0.5986366606622486 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "919 Training acc 0.72 loss 0.5986346628043258 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "920 Training acc 0.72 loss 0.5986320204597038 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "921 Training acc 0.72 loss 0.5986288435167944 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "922 Training acc 0.72 loss 0.5986252503689732 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "923 Training acc 0.72 loss 0.5986213616219735 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "924 Training acc 0.72 loss 0.5986172952909369 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "925 Training acc 0.72 loss 0.5986131618823968 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "926 Training acc 0.72 loss 0.5986090618423405 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "927 Training acc 0.72 loss 0.5986050816215902 50\n",
      "lamda tensor(0.5990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "928 Training acc 0.72 loss 0.5986012934215762 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "929 Training acc 0.72 loss 0.598597754364445 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "930 Training acc 0.72 loss 0.5985945068651974 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "931 Training acc 0.72 loss 0.5985915790210024 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "932 Training acc 0.72 loss 0.5985889853826486 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "933 Training acc 0.72 loss 0.5985867283761894 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "934 Training acc 0.72 loss 0.5985847996814875 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "935 Training acc 0.72 loss 0.5985831802980692 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "936 Training acc 0.72 loss 0.5985818432472922 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "937 Training acc 0.72 loss 0.5985807559479263 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "938 Training acc 0.72 loss 0.5985798814938655 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "939 Training acc 0.72 loss 0.5985791800563001 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "940 Training acc 0.72 loss 0.5985786103415676 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "941 Training acc 0.72 loss 0.598578130873549 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "942 Training acc 0.72 loss 0.598577701196421 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "943 Training acc 0.72 loss 0.5985772829753011 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "944 Training acc 0.72 loss 0.5985768409600276 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "945 Training acc 0.72 loss 0.5985763438010272 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "946 Training acc 0.72 loss 0.5985757647015113 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "947 Training acc 0.72 loss 0.5985750818923052 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "948 Training acc 0.72 loss 0.598574278920172 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "949 Training acc 0.72 loss 0.5985733447460097 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "950 Training acc 0.72 loss 0.5985722736565225 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "951 Training acc 0.72 loss 0.5985710650004884 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "952 Training acc 0.72 loss 0.5985697227682554 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "953 Training acc 0.72 loss 0.5985682550400112 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "954 Training acc 0.72 loss 0.5985666733331835 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "955 Training acc 0.72 loss 0.5985649918615061 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "956 Training acc 0.72 loss 0.5985632266156709 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "957 Training acc 0.72 loss 0.5985613951099569 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "958 Training acc 0.72 loss 0.598559515317155 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "959 Training acc 0.72 loss 0.5985576049259435 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "960 Training acc 0.72 loss 0.5985556809634843 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "961 Training acc 0.72 loss 0.5985537592135078 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "962 Training acc 0.72 loss 0.598551853728986 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "963 Training acc 0.72 loss 0.598549976532552 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "964 Training acc 0.72 loss 0.5985481374614882 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "965 Training acc 0.72 loss 0.5985463442879666 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "966 Training acc 0.72 loss 0.598544602371805 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "967 Training acc 0.72 loss 0.5985429148291436 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "968 Training acc 0.72 loss 0.5985412826547263 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "969 Training acc 0.72 loss 0.5985397049758173 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "970 Training acc 0.72 loss 0.5985381791844616 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "971 Training acc 0.72 loss 0.5985367012252946 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "972 Training acc 0.72 loss 0.5985352658640525 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "973 Training acc 0.72 loss 0.5985338670089269 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "974 Training acc 0.72 loss 0.5985324979423392 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "975 Training acc 0.72 loss 0.5985311494032026 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "976 Training acc 0.72 loss 0.598529812624268 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "977 Training acc 0.72 loss 0.5985284793648047 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "978 Training acc 0.72 loss 0.5985271419574701 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "979 Training acc 0.72 loss 0.5985257936114673 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "980 Training acc 0.72 loss 0.598524428554567 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "981 Training acc 0.72 loss 0.598523042164033 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "982 Training acc 0.72 loss 0.598521630917205 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "983 Training acc 0.72 loss 0.5985201924537167 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "984 Training acc 0.72 loss 0.5985187255544892 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "985 Training acc 0.72 loss 0.598517230163298 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "986 Training acc 0.72 loss 0.5985157071425372 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "987 Training acc 0.72 loss 0.5985141582042243 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "988 Training acc 0.72 loss 0.5985125857700336 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "989 Training acc 0.72 loss 0.598510992820356 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "990 Training acc 0.72 loss 0.5985093827386089 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "991 Training acc 0.72 loss 0.5985077591567384 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "992 Training acc 0.72 loss 0.5985061258072412 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "993 Training acc 0.72 loss 0.5985044865255527 50\n",
      "lamda tensor(0.5989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "994 Training acc 0.72 loss 0.5985028449598453 50\n",
      "lamda tensor(0.5988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "995 Training acc 0.72 loss 0.5985012041339873 50\n",
      "lamda tensor(0.5988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "996 Training acc 0.72 loss 0.5984995669494105 50\n",
      "lamda tensor(0.5988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "997 Training acc 0.72 loss 0.598497935944983 50\n",
      "lamda tensor(0.5988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "998 Training acc 0.72 loss 0.5984963132167309 50\n",
      "lamda tensor(0.5988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8181, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "999 Training acc 0.72 loss 0.5984947002507274 50\n"
     ]
    }
   ],
   "source": [
    "test,tatin=tr.training()#without lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e70fe1-d416-4e31-974f-f95f11f7172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testno,tatinon=test,tatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d226a182-f543-4b3b-b02d-6ffa123accf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d648420a-30d8-4b5b-aaf5-2a3773b63d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Accuarcy with lambda\")\n",
    "plt.plot(test[0:-1:10],label=\"test accuracy\",linestyle='dashed')\n",
    "plt.plot(tatin[0:-1:10],label=\"train accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ea7b538f-ba79-488a-b6f8-539e10302ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Accuarcy without lambda\")\n",
    "plt.plot(np.array(test)*100,label=\"test accuracy\",linestyle='dashed')\n",
    "plt.plot(np.array(tatin)*100,label=\"train accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64be5d-531d-4ceb-86f8-cd1568f98255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
