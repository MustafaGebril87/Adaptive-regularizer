{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161d6a9a-44a0-48e1-b412-941a4c94dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940685ea-1863-4c08-9008-b26b0365d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2822266d-1ff2-43f9-9eea-05591d9fae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e91bd0e0d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split,Subset\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee276a1-b683-4930-810c-b699240f4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c1aab5-2c89-4b17-96d3-091576c96c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv(\"pima-indians-diabetes.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5999ea0-df6c-4672-ac3b-5c9ee384d2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1   2   3    4     5      6   7  8\n",
       "0     6  148  72  35    0  33.6  0.627  50  1\n",
       "1     1   85  66  29    0  26.6  0.351  31  0\n",
       "2     8  183  64   0    0  23.3  0.672  32  1\n",
       "3     1   89  66  23   94  28.1  0.167  21  0\n",
       "4     0  137  40  35  168  43.1  2.288  33  1\n",
       "..   ..  ...  ..  ..  ...   ...    ...  .. ..\n",
       "763  10  101  76  48  180  32.9  0.171  63  0\n",
       "764   2  122  70  27    0  36.8  0.340  27  0\n",
       "765   5  121  72  23  112  26.2  0.245  30  0\n",
       "766   1  126  60   0    0  30.1  0.349  47  1\n",
       "767   1   93  70  31    0  30.4  0.315  23  0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e20be6c-72be-44ae-b3ac-8d8f700447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=data.drop(\"id\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e25c7aa-dcfd-41d5-8321-90705d77c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f74470-69ac-4301-bd1e-9e8eab675a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.drop(8, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35344205-6bdb-42df-8c26-1962471e0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e5bb69-cc7a-4e70-b331-29e0588b3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e583a0b-1de9-4e02-adc8-5926ccbd39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdccdbec-c1ae-4224-a1a0-c70c95926aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "standered=scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae93b731-3ed2-4a61-bf00-8c6b4601f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(standered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e63bc18-9762-49f3-94fa-20cd8289a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):       \n",
    "        \n",
    "        self.features=train\n",
    "        self.labels = target\n",
    "    def __getitem__(self, i):\n",
    "        feaures=self.features.iloc[i]\n",
    "        target=self.labels[i]\n",
    "        return torch.tensor(np.array(feaures)),torch.tensor(np.array(target))\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aba2551-393b-4a7e-a77e-565aeb48c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b59b1f6-69d4-4aee-aeef-6d00a64d51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import torch.nn.functional as f\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,outNumber):\n",
    "        super(Net,self).__init__()\n",
    "        self.outNumber = outNumber\n",
    "        \n",
    "        \n",
    "    \n",
    "        self.fc0 = nn.Linear(\n",
    "            in_features=8, out_features=100 ,bias=True)\n",
    "        self.depth = 150\n",
    "        self.linear_layers = []\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(\n",
    "            in_features=100, out_features=100, bias=True) for index in range(self.depth)])\n",
    "        self.fc = nn.Linear(in_features=100, out_features=self.outNumber, bias=True)\n",
    "        # self.activation = nn.sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.fc0(x)\n",
    "        residual = out\n",
    "        for index, layer in enumerate(self.linear_layers):\n",
    "            out = self.linear_layers[index](out)\n",
    "            out = f.leaky_relu(out)\n",
    "            # out = f.dropout(out, p=self.droup_out)\n",
    "            if index % 3 == 0:\n",
    "                out = out + residual\n",
    "                residual = out\n",
    "        out = self.fc(out)\n",
    "        out = torch.nn.functional.sigmoid(out)\n",
    "        out = torch.squeeze(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eff0bab-8c06-4556-bcca-288d52387392",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net(1)\n",
    "net=net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5e451c4-eb78-479f-a58e-cb089369e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "274d6cc5-bf56-4661-9b6e-fac89855c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    accuracy = metrics.accuracy_score(y_true.cpu().detach().numpy(), y_prob.cpu().detach().numpy() > 0.5)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a38cd4cc-6fe4-4ea7-b04b-5ff1c5ac60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = MyDataset()\n",
    "#dataset_ = Subset(dataset_, np.arange(1000000))\n",
    "# Split into training and test\n",
    "train_size = int(0.7 * len(dataset_))\n",
    "test_size = len(dataset_) - train_size\n",
    "\n",
    "\n",
    "trainset, testset = random_split(dataset_, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False)\n",
    "criterion=nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1.0E-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a22878f5-6ec0-4c52-9532-b81bae0dfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pytorch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.optim as optim \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "class train_:\n",
    "    def __init__(self,epochs,model,train_dl,val_dl,criterion,optimizer):\n",
    "        self.model = model\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "    def accuracy(self,y_prob,y_true):\n",
    "        y_prob = y_prob > 0.5\n",
    "        return (y_true == y_prob).sum().item() / y_true.size(0)    \n",
    "   # def accuracy(self,predictions, labels):# accuracy function from the resulted predictions\n",
    "    #    #accuracy = (torch.softmax(predictions, dim=1).argmax(dim=1) == labels).sum().float() / float( labels.size(0) )\n",
    "     #   accuracy = ((predictions== labels).sum().float() / float( labels.size(0) ))\n",
    "      #  return accuracy    \n",
    "    \n",
    "    def training(self):\n",
    "        Max_Accu_Val = .0 # start with zero accuracy to compare the results\n",
    "        lamda = torch.tensor(0.6).type(torch.float).requires_grad_() \n",
    "        lamda = torch.tensor(0.6,requires_grad=True, device=\"cuda\")\n",
    "        self.optimizer2 = optim.Adam([lamda], lr=1E-2)\n",
    "        error = []\n",
    "        acctest_=[]\n",
    "        acctrain_=[]\n",
    "        \n",
    "        self.model = self.model.double()\n",
    "        for e in range(self.epochs): # for every epoch\n",
    "            CounterTrain =0 # initialize the values with zero, this used to calculate the number of  training epochs\n",
    "            CounterVal =0  # this used as number of test epochs\n",
    "            train_loss = 0.0 # initialize the training loss with zero\n",
    "            TrainAccAll = 0.0 # the summed accuracy for the whole training epoch\n",
    "            VallAccAll = 0.0 # the summed accuracy of the whole test epoch\n",
    "            lossTrainAll = 0.0 # the summed loss for the whole training epoch\n",
    "            lossValAll = 0.0 # the summed loss for the whole test epoch\n",
    "            lossVarAll=0\n",
    "            acctest=0\n",
    "            countertest=0\n",
    "            lossTestAll=0.0\n",
    "            self.model.train() \n",
    "            \n",
    "            # signal a training process\n",
    "            for data, labels in self.train_dl: # for every input and output in train data loader\n",
    "                self.model.train() \n",
    "\n",
    "                if torch.cuda.is_available(): # check if cuda is a vailable\n",
    "                   data, labels = data.cuda(), labels.cuda() # feed the input and output to cuda\n",
    "               \n",
    "                self.optimizer.zero_grad() # zero the gradient, required by pytorch\n",
    "               \n",
    "                target = self.model(data.double()) # calculate the input\n",
    "               #outnorm = [float(i)/max(out) for i in out]\n",
    "\n",
    "                dataiter = iter(self.val_dl)\n",
    "                inputsTest, labelsTest  = next(dataiter)\n",
    "                labelsTest = labelsTest.to(\"cuda\")\n",
    "                inputsTest = inputsTest.to(\"cuda\")\n",
    "                self.model.eval() \n",
    "                targettest= self.model(inputsTest.double()) \n",
    "                \n",
    "                acctest += self.accuracy(targettest, labelsTest)\n",
    "                countertest+=1\n",
    "                \n",
    "                \n",
    "                l1 =1# sum(p.abs().sum() for p in self.model.parameters())\n",
    "                \n",
    "                lossOriginal = self.criterion(target.double(), labels.double())\n",
    "              \n",
    "                loss =lossOriginal+lamda*l1\n",
    "                #x= ((1/((-lossOriginal+lamda)/(2*labels-1))))\n",
    "                x = (1/(labels-torch.sqrt(torch.abs(lossOriginal-lamda))+.0000001))-1\n",
    "                varout = torch.abs(torch.var(-(x*x-1)/(x+.0000001)))\n",
    "                #varout =torch.var( -torch.log(torch.abs(1/(labels-torch.sqrt(torch.abs(lossOriginal-lamda)))-1)))\n",
    "                #varout = torch.var(labels-lossOriginal-lamda)\n",
    "                #varout =torch.var(-(((1/((-loss+lamda)/(2*labels-1))-1))+(.5*(torch.abs(1/((-loss+lamda)/(2*labels-1))-1))*(torch.abs(1/((-loss+lamda)/(2*labels-1))-1)))))\n",
    "                #varout=torch.var(((((2*labels/((lossOriginal-lamda*l1)+2*labels+.00001))-1))-1)+.5*((((2*labels/((lossOriginal-lamda*l1)+2*labels+.00001))-1))-1)*((((2*labels/((lossOriginal-lamda*l1)+2*labels+.00001))-1))-1))\n",
    "                loss.backward(retain_graph=True)# neural network backward calculations\n",
    "                self.optimizer2.zero_grad()\n",
    "                #var_=torch.abs(torch.var(out)-.1*torch.max(out))\n",
    "                varout.backward()\n",
    "                self.optimizer.step() \n",
    "                self.optimizer2.step()\n",
    "                # optimize\n",
    "                \n",
    "                train_loss = lossOriginal.item()# sotre loss values\n",
    "                TrainAcc = self.accuracy(target.double(),labels.double()) # calculate training accuracy\n",
    "                CounterTrain+=1 # update training counter\n",
    "                TrainAccAll +=TrainAcc # update training accuracy the whole epoch\n",
    "                lossTrainAll += train_loss # update training loss the whole epoch\n",
    "                #if CounterTrain%5==0:\n",
    "                if CounterTrain%1==0:\n",
    "                    print(\"lamda\",lamda)\n",
    "                    print(varout)\n",
    "                    print(\"acc Test:\", acctest/countertest)\n",
    "                    print(e,'Training acc',TrainAccAll /CounterTrain,'loss',lossTrainAll/CounterTrain,CounterTrain*50)\n",
    "                    #traced_cell = torch.jit.trace(self.model,data.double())\n",
    "                    #traced_cell.save('./atrial_Fib_model.zip')\n",
    "                    #print(\"Conerted!\")\n",
    "                \n",
    "                    error.append(lossTrainAll/CounterTrain)\n",
    "                    acctest_.append(acctest/countertest)\n",
    "                    acctrain_.append(TrainAccAll/CounterTrain)   \n",
    "        return acctest_,acctrain_\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b17fd0df-f057-472b-8330-5ff1b9d0e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=train_(100,net,trainloader,testloader,criterion,optimizer)#with lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07dac5f1-48a3-4fdb-b6de-9491b9cc2c6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamda tensor(0.6100, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9983, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.41\n",
      "0 Training acc 0.32 loss 0.7068651850530466 50\n",
      "lamda tensor(0.6200, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.43\n",
      "0 Training acc 0.36 loss 0.702365194299607 100\n",
      "lamda tensor(0.6300, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0555, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.44\n",
      "0 Training acc 0.38999999999999996 loss 0.7015943427195223 150\n",
      "lamda tensor(0.6400, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1239, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.4575\n",
      "0 Training acc 0.4225 loss 0.6990302188542592 200\n",
      "lamda tensor(0.6500, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9808, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.476\n",
      "0 Training acc 0.43200000000000005 loss 0.6978708182051652 250\n",
      "lamda tensor(0.6599, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0070, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.49666666666666665\n",
      "0 Training acc 0.46810810810810816 loss 0.6957890823743526 300\n",
      "lamda tensor(0.6696, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7722, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.64\n",
      "1 Training acc 0.71 loss 0.6829673461759539 50\n",
      "lamda tensor(0.6792, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9123, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "1 Training acc 0.685 loss 0.6854290843539206 100\n",
      "lamda tensor(0.6889, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9337, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6666666666666666\n",
      "1 Training acc 0.6833333333333335 loss 0.6838570736602546 150\n",
      "lamda tensor(0.6957, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9345, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6675\n",
      "1 Training acc 0.6875 loss 0.6813516158716072 200\n",
      "lamda tensor(0.7001, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0078, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6599999999999999\n",
      "1 Training acc 0.658 loss 0.6830076881083998 250\n",
      "lamda tensor(0.7026, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0201, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6533333333333333\n",
      "1 Training acc 0.642927927927928 loss 0.6834670090962627 300\n",
      "lamda tensor(0.7034, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9566, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "2 Training acc 0.66 loss 0.6761200692782562 50\n",
      "lamda tensor(0.7030, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9047, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "2 Training acc 0.685 loss 0.6739964614160368 100\n",
      "lamda tensor(0.7014, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9704, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "2 Training acc 0.68 loss 0.6725235688200147 150\n",
      "lamda tensor(0.6988, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9628, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6275000000000001\n",
      "2 Training acc 0.6775 loss 0.6723373476126613 200\n",
      "lamda tensor(0.6953, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0053, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6260000000000001\n",
      "2 Training acc 0.6639999999999999 loss 0.6740500738008308 250\n",
      "lamda tensor(0.6911, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9715, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6250000000000001\n",
      "2 Training acc 0.6614414414414415 loss 0.6746021591171679 300\n",
      "lamda tensor(0.6862, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9827, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "3 Training acc 0.62 loss 0.6752869327158098 50\n",
      "lamda tensor(0.6809, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8802, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "3 Training acc 0.6599999999999999 loss 0.6716762322819401 100\n",
      "lamda tensor(0.6751, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9534, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "3 Training acc 0.6499999999999999 loss 0.6727151633278571 150\n",
      "lamda tensor(0.6689, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9120, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "3 Training acc 0.6549999999999999 loss 0.6725720602168554 200\n",
      "lamda tensor(0.6626, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8943, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "3 Training acc 0.6599999999999999 loss 0.6717434243397169 250\n",
      "lamda tensor(0.6580, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9712, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "3 Training acc 0.658108108108108 loss 0.6731766991905799 300\n",
      "lamda tensor(0.6550, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0029, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "4 Training acc 0.61 loss 0.6789938081366842 50\n",
      "lamda tensor(0.6535, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9981, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "4 Training acc 0.615 loss 0.6787167200401254 100\n",
      "lamda tensor(0.6531, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9515, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "4 Training acc 0.6266666666666666 loss 0.6760200774334492 150\n",
      "lamda tensor(0.6538, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9235, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "4 Training acc 0.6375 loss 0.6744113840482712 200\n",
      "lamda tensor(0.6554, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8835, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.62\n",
      "4 Training acc 0.6479999999999999 loss 0.6724893772643817 250\n",
      "lamda tensor(0.6560, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7581, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6216666666666667\n",
      "4 Training acc 0.6661261261261261 loss 0.6694965543567569 300\n",
      "lamda tensor(0.6577, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9993, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "5 Training acc 0.61 loss 0.6752824787397819 50\n",
      "lamda tensor(0.6602, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9373, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "5 Training acc 0.63 loss 0.671327041588412 100\n",
      "lamda tensor(0.6616, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7744, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "5 Training acc 0.6699999999999999 loss 0.6639723555490656 150\n",
      "lamda tensor(0.6640, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0163, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "5 Training acc 0.6499999999999999 loss 0.6682065427311075 200\n",
      "lamda tensor(0.6651, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8855, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "5 Training acc 0.6559999999999999 loss 0.6666470127772314 250\n",
      "lamda tensor(0.6671, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9387, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "5 Training acc 0.6547747747747747 loss 0.666546128394103 300\n",
      "lamda tensor(0.6700, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9623, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "6 Training acc 0.62 loss 0.6725492388398123 50\n",
      "lamda tensor(0.6715, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9137, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "6 Training acc 0.64 loss 0.6693147639642326 100\n",
      "lamda tensor(0.6719, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9657, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "6 Training acc 0.63 loss 0.6692835677964885 150\n",
      "lamda tensor(0.6714, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8644, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "6 Training acc 0.65 loss 0.6652032914832786 200\n",
      "lamda tensor(0.6699, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9484, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "6 Training acc 0.648 loss 0.6645835558603652 250\n",
      "lamda tensor(0.6676, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8267, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "6 Training acc 0.6661261261261262 loss 0.6582607925867792 300\n",
      "lamda tensor(0.6647, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8401, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "7 Training acc 0.72 loss 0.652310305886831 50\n",
      "lamda tensor(0.6631, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9909, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "7 Training acc 0.655 loss 0.661917887298568 100\n",
      "lamda tensor(0.6627, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9622, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "7 Training acc 0.6433333333333334 loss 0.664095586561171 150\n",
      "lamda tensor(0.6613, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9196, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "7 Training acc 0.6475000000000001 loss 0.6619843993775474 200\n",
      "lamda tensor(0.6592, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9132, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "7 Training acc 0.6500000000000001 loss 0.6611396681616293 250\n",
      "lamda tensor(0.6563, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8412, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "7 Training acc 0.6632882882882883 loss 0.6577761720206842 300\n",
      "lamda tensor(0.6548, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9618, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "8 Training acc 0.62 loss 0.6614970078433987 50\n",
      "lamda tensor(0.6544, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9242, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "8 Training acc 0.635 loss 0.659485506035304 100\n",
      "lamda tensor(0.6531, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7908, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "8 Training acc 0.6699999999999999 loss 0.6549429816275126 150\n",
      "lamda tensor(0.6531, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9876, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "8 Training acc 0.6549999999999999 loss 0.6578516333134844 200\n",
      "lamda tensor(0.6540, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "8 Training acc 0.6519999999999999 loss 0.6587543512502606 250\n",
      "lamda tensor(0.6540, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8697, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "8 Training acc 0.6604504504504504 loss 0.6569264005480129 300\n",
      "lamda tensor(0.6550, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9846, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "9 Training acc 0.61 loss 0.6659033483512667 50\n",
      "lamda tensor(0.6568, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9325, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "9 Training acc 0.63 loss 0.6640097575233296 100\n",
      "lamda tensor(0.6576, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9169, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "9 Training acc 0.6433333333333334 loss 0.6573147399248099 150\n",
      "lamda tensor(0.6573, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8827, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "9 Training acc 0.6525000000000001 loss 0.6568857630350967 200\n",
      "lamda tensor(0.6561, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9170, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "9 Training acc 0.6540000000000001 loss 0.6558365132772199 250\n",
      "lamda tensor(0.6540, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9249, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "9 Training acc 0.6576126126126126 loss 0.6536963902374778 300\n",
      "lamda tensor(0.6512, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8815, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "10 Training acc 0.68 loss 0.6527423688760263 50\n",
      "lamda tensor(0.6497, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9196, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "10 Training acc 0.67 loss 0.6555106963569411 100\n",
      "lamda tensor(0.6474, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8608, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "10 Training acc 0.6833333333333332 loss 0.647944064895167 150\n",
      "lamda tensor(0.6463, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9158, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "10 Training acc 0.6775 loss 0.6490614616089969 200\n",
      "lamda tensor(0.6464, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0135, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "10 Training acc 0.6599999999999999 loss 0.6521156636057368 250\n",
      "lamda tensor(0.6476, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0308, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "10 Training acc 0.6490990990990991 loss 0.6544198035311387 300\n",
      "lamda tensor(0.6497, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9466, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "11 Training acc 0.64 loss 0.6559287754330059 50\n",
      "lamda tensor(0.6506, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8855, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "11 Training acc 0.665 loss 0.6467770440141851 100\n",
      "lamda tensor(0.6525, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0081, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "11 Training acc 0.64 loss 0.6531725620418224 150\n",
      "lamda tensor(0.6553, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9755, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "11 Training acc 0.6325 loss 0.6548538814680749 200\n",
      "lamda tensor(0.6569, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8419, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "11 Training acc 0.65 loss 0.6516643057623741 250\n",
      "lamda tensor(0.6574, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8578, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "11 Training acc 0.6632882882882883 loss 0.6479026537588193 300\n",
      "lamda tensor(0.6568, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9110, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "12 Training acc 0.68 loss 0.6397332258555675 50\n",
      "lamda tensor(0.6554, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9334, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "12 Training acc 0.66 loss 0.6476263139077743 100\n",
      "lamda tensor(0.6531, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9300, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "12 Training acc 0.6566666666666667 loss 0.648272437119933 150\n",
      "lamda tensor(0.6501, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9260, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "12 Training acc 0.66 loss 0.6468637829257541 200\n",
      "lamda tensor(0.6484, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9536, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "12 Training acc 0.654 loss 0.6487433606002195 250\n",
      "lamda tensor(0.6459, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8914, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "12 Training acc 0.6621171171171171 loss 0.6456151348620106 300\n",
      "lamda tensor(0.6447, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "13 Training acc 0.62 loss 0.6559993971144329 50\n",
      "lamda tensor(0.6446, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "13 Training acc 0.625 loss 0.6515400247759793 100\n",
      "lamda tensor(0.6454, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9011, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "13 Training acc 0.64 loss 0.6506512432165827 150\n",
      "lamda tensor(0.6453, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8406, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "13 Training acc 0.6625 loss 0.6454393494743762 200\n",
      "lamda tensor(0.6462, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9453, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "13 Training acc 0.658 loss 0.6469387092864475 250\n",
      "lamda tensor(0.6460, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9232, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "13 Training acc 0.6609459459459459 loss 0.6447914281189955 300\n",
      "lamda tensor(0.6469, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9817, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "14 Training acc 0.6 loss 0.65213323493987 50\n",
      "lamda tensor(0.6467, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9057, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "14 Training acc 0.635 loss 0.6460967406672435 100\n",
      "lamda tensor(0.6456, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8824, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "14 Training acc 0.6566666666666666 loss 0.6428464682400007 150\n",
      "lamda tensor(0.6456, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9595, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "14 Training acc 0.65 loss 0.6445226438404483 200\n",
      "lamda tensor(0.6447, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "14 Training acc 0.658 loss 0.6440123333031413 250\n",
      "lamda tensor(0.6430, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9273, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "14 Training acc 0.6609459459459459 loss 0.6417420138679284 300\n",
      "lamda tensor(0.6423, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9483, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "15 Training acc 0.63 loss 0.6463445361524786 50\n",
      "lamda tensor(0.6427, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9217, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "15 Training acc 0.64 loss 0.645014319274879 100\n",
      "lamda tensor(0.6422, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8476, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "15 Training acc 0.6699999999999999 loss 0.6376889743495898 150\n",
      "lamda tensor(0.6427, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9807, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "15 Training acc 0.6549999999999999 loss 0.6402094403244463 200\n",
      "lamda tensor(0.6423, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8734, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "15 Training acc 0.6599999999999999 loss 0.639659549102032 250\n",
      "lamda tensor(0.6429, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9933, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.63\n",
      "15 Training acc 0.6536036036036036 loss 0.6423156930687712 300\n",
      "lamda tensor(0.6425, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9446, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.64\n",
      "16 Training acc 0.65 loss 0.6414727593679804 50\n",
      "lamda tensor(0.6413, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8795, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.64\n",
      "16 Training acc 0.685 loss 0.6331033623215463 100\n",
      "lamda tensor(0.6391, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9130, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.64\n",
      "16 Training acc 0.6833333333333335 loss 0.634663608557683 150\n",
      "lamda tensor(0.6372, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9200, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6425\n",
      "16 Training acc 0.6775000000000001 loss 0.6358349715837424 200\n",
      "lamda tensor(0.6364, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6439999999999999\n",
      "16 Training acc 0.6700000000000002 loss 0.6380435456103625 250\n",
      "lamda tensor(0.6367, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9426, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6449999999999999\n",
      "16 Training acc 0.6664414414414416 loss 0.6382696697738902 300\n",
      "lamda tensor(0.6379, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9534, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.65\n",
      "17 Training acc 0.64 loss 0.6374425648359057 50\n",
      "lamda tensor(0.6400, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9564, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.655\n",
      "17 Training acc 0.645 loss 0.638840293939218 100\n",
      "lamda tensor(0.6428, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8816, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6566666666666667\n",
      "17 Training acc 0.6533333333333333 loss 0.6396849730621125 150\n",
      "lamda tensor(0.6444, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8893, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6575000000000001\n",
      "17 Training acc 0.665 loss 0.6346642937048739 200\n",
      "lamda tensor(0.6448, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8960, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6580000000000001\n",
      "17 Training acc 0.674 loss 0.633035949314239 250\n",
      "lamda tensor(0.6463, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0482, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6583333333333334\n",
      "17 Training acc 0.6607657657657658 loss 0.6381425584213962 300\n",
      "lamda tensor(0.6467, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9132, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "18 Training acc 0.71 loss 0.6191725883116171 50\n",
      "lamda tensor(0.6461, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8731, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "18 Training acc 0.725 loss 0.620998806840074 100\n",
      "lamda tensor(0.6446, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8227, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "18 Training acc 0.7366666666666667 loss 0.6158855856234656 150\n",
      "lamda tensor(0.6424, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "18 Training acc 0.7125 loss 0.6229178545632907 200\n",
      "lamda tensor(0.6416, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0307, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "18 Training acc 0.682 loss 0.6298198660967467 250\n",
      "lamda tensor(0.6419, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0559, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "18 Training acc 0.653918918918919 loss 0.6340730149684896 300\n",
      "lamda tensor(0.6413, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8885, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "19 Training acc 0.7 loss 0.6191648005749039 50\n",
      "lamda tensor(0.6397, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9313, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.66\n",
      "19 Training acc 0.7 loss 0.6235848659491443 100\n",
      "lamda tensor(0.6373, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9448, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6633333333333334\n",
      "19 Training acc 0.69 loss 0.6244153284442177 150\n",
      "lamda tensor(0.6361, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9619, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.665\n",
      "19 Training acc 0.6825 loss 0.6289330264348308 200\n",
      "lamda tensor(0.6341, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9185, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.666\n",
      "19 Training acc 0.6839999999999999 loss 0.6290800037051787 250\n",
      "lamda tensor(0.6333, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9993, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6666666666666666\n",
      "19 Training acc 0.6826126126126125 loss 0.6326309827509492 300\n",
      "lamda tensor(0.6338, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0248, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "20 Training acc 0.66 loss 0.6533237372848844 50\n",
      "lamda tensor(0.6332, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8880, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "20 Training acc 0.685 loss 0.636827648910399 100\n",
      "lamda tensor(0.6317, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9255, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6733333333333333\n",
      "20 Training acc 0.6866666666666666 loss 0.6344937937356215 150\n",
      "lamda tensor(0.6294, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9267, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6799999999999999\n",
      "20 Training acc 0.7 loss 0.6327845994484592 200\n",
      "lamda tensor(0.6264, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8339, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6859999999999999\n",
      "20 Training acc 0.71 loss 0.6297775561599314 250\n",
      "lamda tensor(0.6248, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9961, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6883333333333334\n",
      "20 Training acc 0.7087837837837837 loss 0.6316703369628577 300\n",
      "lamda tensor(0.6244, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0188, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "21 Training acc 0.62 loss 0.6453304251336448 50\n",
      "lamda tensor(0.6251, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9225, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "21 Training acc 0.6599999999999999 loss 0.6391928864769254 100\n",
      "lamda tensor(0.6248, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8540, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "21 Training acc 0.69 loss 0.6334365130390368 150\n",
      "lamda tensor(0.6237, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7825, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "21 Training acc 0.715 loss 0.6304587920545707 200\n",
      "lamda tensor(0.6238, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0278, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.688\n",
      "21 Training acc 0.716 loss 0.6314762959936483 250\n",
      "lamda tensor(0.6232, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7216, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6866666666666666\n",
      "21 Training acc 0.7272972972972974 loss 0.6273015137113201 300\n",
      "lamda tensor(0.6237, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0355, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "22 Training acc 0.68 loss 0.6481990521484688 50\n",
      "lamda tensor(0.6251, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9326, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "22 Training acc 0.7 loss 0.6363619092040734 100\n",
      "lamda tensor(0.6256, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "22 Training acc 0.7066666666666667 loss 0.6324460704973097 150\n",
      "lamda tensor(0.6252, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7983, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6924999999999999\n",
      "22 Training acc 0.7175 loss 0.627423828160309 200\n",
      "lamda tensor(0.6257, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8894, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.694\n",
      "22 Training acc 0.7300000000000001 loss 0.6281188237623839 250\n",
      "lamda tensor(0.6267, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9378, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.695\n",
      "22 Training acc 0.729954954954955 loss 0.6277711709264212 300\n",
      "lamda tensor(0.6266, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9624, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "23 Training acc 0.76 loss 0.6212416694289589 50\n",
      "lamda tensor(0.6256, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8096, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "23 Training acc 0.75 loss 0.6198004324163232 100\n",
      "lamda tensor(0.6257, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9652, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6999999999999998\n",
      "23 Training acc 0.73 loss 0.6240255006393794 150\n",
      "lamda tensor(0.6249, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8269, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "23 Training acc 0.745 loss 0.6225767153790353 200\n",
      "lamda tensor(0.6254, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0364, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "23 Training acc 0.732 loss 0.6262365406851116 250\n",
      "lamda tensor(0.6250, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7287, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7000000000000001\n",
      "23 Training acc 0.7496396396396396 loss 0.6223860611508731 300\n",
      "lamda tensor(0.6237, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8554, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "24 Training acc 0.73 loss 0.6209282406160493 50\n",
      "lamda tensor(0.6216, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8739, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "24 Training acc 0.76 loss 0.6149756574856694 100\n",
      "lamda tensor(0.6207, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9741, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7033333333333333\n",
      "24 Training acc 0.7433333333333333 loss 0.6209878085402808 150\n",
      "lamda tensor(0.6190, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8787, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.705\n",
      "24 Training acc 0.7575000000000001 loss 0.6165881199519694 200\n",
      "lamda tensor(0.6186, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0467, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.704\n",
      "24 Training acc 0.74 loss 0.6225371189962612 250\n",
      "lamda tensor(0.6193, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9749, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7033333333333333\n",
      "24 Training acc 0.7472972972972972 loss 0.6225589706963958 300\n",
      "lamda tensor(0.6209, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9257, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "25 Training acc 0.74 loss 0.6296632335292817 50\n",
      "lamda tensor(0.6215, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8544, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "25 Training acc 0.78 loss 0.6186928663532176 100\n",
      "lamda tensor(0.6230, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9876, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6999999999999998\n",
      "25 Training acc 0.77 loss 0.6213476851371057 150\n",
      "lamda tensor(0.6235, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9009, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "25 Training acc 0.7675000000000001 loss 0.6207204829145545 200\n",
      "lamda tensor(0.6229, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8693, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.702\n",
      "25 Training acc 0.76 loss 0.6206847587829978 250\n",
      "lamda tensor(0.6213, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0251, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7033333333333333\n",
      "25 Training acc 0.7594594594594595 loss 0.6207228025181565 300\n",
      "lamda tensor(0.6189, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9661, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "26 Training acc 0.76 loss 0.6188216049841744 50\n",
      "lamda tensor(0.6174, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8654, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "26 Training acc 0.76 loss 0.6191264285349507 100\n",
      "lamda tensor(0.6153, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8082, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "26 Training acc 0.7766666666666667 loss 0.612152157480013 150\n",
      "lamda tensor(0.6144, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9742, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "26 Training acc 0.7625 loss 0.617121592678475 200\n",
      "lamda tensor(0.6147, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0016, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "26 Training acc 0.752 loss 0.6197840060568195 250\n",
      "lamda tensor(0.6139, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "26 Training acc 0.7618018018018017 loss 0.6159272667227895 300\n",
      "lamda tensor(0.6143, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9694, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "27 Training acc 0.74 loss 0.618126683368502 50\n",
      "lamda tensor(0.6138, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7863, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "27 Training acc 0.77 loss 0.6133758532137547 100\n",
      "lamda tensor(0.6143, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9637, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "27 Training acc 0.7666666666666666 loss 0.6155471071722721 150\n",
      "lamda tensor(0.6158, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9153, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "27 Training acc 0.75 loss 0.6164259522827494 200\n",
      "lamda tensor(0.6162, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9457, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.706\n",
      "27 Training acc 0.746 loss 0.6159031965624454 250\n",
      "lamda tensor(0.6155, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8912, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7033333333333333\n",
      "27 Training acc 0.7568018018018018 loss 0.6128915435729811 300\n",
      "lamda tensor(0.6160, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9715, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "28 Training acc 0.72 loss 0.6308824380726241 50\n",
      "lamda tensor(0.6154, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9013, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "28 Training acc 0.72 loss 0.6212181706777122 100\n",
      "lamda tensor(0.6140, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8110, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "28 Training acc 0.75 loss 0.6122951502150281 150\n",
      "lamda tensor(0.6118, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9434, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "28 Training acc 0.745 loss 0.6125362070399154 200\n",
      "lamda tensor(0.6089, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9431, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "28 Training acc 0.746 loss 0.6111052688989709 250\n",
      "lamda tensor(0.6051, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0213, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "28 Training acc 0.7477927927927928 loss 0.6097295916315909 300\n",
      "lamda tensor(0.6007, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9291, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "29 Training acc 0.78 loss 0.5929160489396916 50\n",
      "lamda tensor(0.5976, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8860, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "29 Training acc 0.77 loss 0.5987077795739092 100\n",
      "lamda tensor(0.5959, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9321, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "29 Training acc 0.7600000000000001 loss 0.6029359068327759 150\n",
      "lamda tensor(0.5955, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9984, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6924999999999999\n",
      "29 Training acc 0.7525000000000001 loss 0.6058827712870198 200\n",
      "lamda tensor(0.5961, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8990, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.694\n",
      "29 Training acc 0.748 loss 0.6077065011788483 250\n",
      "lamda tensor(0.5978, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0506, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.695\n",
      "29 Training acc 0.744954954954955 loss 0.6105655011489447 300\n",
      "lamda tensor(0.6003, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9657, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "30 Training acc 0.73 loss 0.6000631857869537 50\n",
      "lamda tensor(0.6036, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9030, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "30 Training acc 0.73 loss 0.6068240006061427 100\n",
      "lamda tensor(0.6076, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9826, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7066666666666667\n",
      "30 Training acc 0.7133333333333334 loss 0.609392179464899 150\n",
      "lamda tensor(0.6102, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9392, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7050000000000001\n",
      "30 Training acc 0.7375 loss 0.6027580172215726 200\n",
      "lamda tensor(0.6134, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8694, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7020000000000001\n",
      "30 Training acc 0.7300000000000001 loss 0.6048326545666922 250\n",
      "lamda tensor(0.6153, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9277, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7016666666666667\n",
      "30 Training acc 0.7344594594594596 loss 0.5999587184896762 300\n",
      "lamda tensor(0.6160, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9130, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "31 Training acc 0.73 loss 0.6118678706458462 50\n",
      "lamda tensor(0.6177, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9640, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "31 Training acc 0.7050000000000001 loss 0.614661815991033 100\n",
      "lamda tensor(0.6182, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9386, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6966666666666667\n",
      "31 Training acc 0.7166666666666668 loss 0.6075410210744797 150\n",
      "lamda tensor(0.6176, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8576, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.695\n",
      "31 Training acc 0.7300000000000001 loss 0.5982162046044393 200\n",
      "lamda tensor(0.6160, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9959, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.694\n",
      "31 Training acc 0.7220000000000001 loss 0.5976275679868385 250\n",
      "lamda tensor(0.6135, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0445, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6933333333333334\n",
      "31 Training acc 0.7187837837837838 loss 0.5963692649832275 300\n",
      "lamda tensor(0.6101, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9848, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "32 Training acc 0.73 loss 0.5913836587380306 50\n",
      "lamda tensor(0.6060, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8880, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "32 Training acc 0.755 loss 0.5846877052336668 100\n",
      "lamda tensor(0.6035, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9960, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "32 Training acc 0.7133333333333334 loss 0.5937793310617118 150\n",
      "lamda tensor(0.6022, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9699, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "32 Training acc 0.7050000000000001 loss 0.5985360889838592 200\n",
      "lamda tensor(0.6001, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9008, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "32 Training acc 0.7140000000000001 loss 0.5968628054047779 250\n",
      "lamda tensor(0.5975, device='cuda:0', requires_grad=True)\n",
      "tensor(0.6722, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "32 Training acc 0.7256306306306307 loss 0.5921660196067028 300\n",
      "lamda tensor(0.5962, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0392, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.67\n",
      "33 Training acc 0.64 loss 0.6242586650782223 50\n",
      "lamda tensor(0.5941, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9831, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6799999999999999\n",
      "33 Training acc 0.655 loss 0.6087907826055741 100\n",
      "lamda tensor(0.5913, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8201, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6866666666666665\n",
      "33 Training acc 0.6900000000000001 loss 0.60276146311001 150\n",
      "lamda tensor(0.5898, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9620, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.69\n",
      "33 Training acc 0.6975 loss 0.6025285645952283 200\n",
      "lamda tensor(0.5875, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8901, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.692\n",
      "33 Training acc 0.712 loss 0.598753237245323 250\n",
      "lamda tensor(0.5848, device='cuda:0', requires_grad=True)\n",
      "tensor(0.5413, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6916666666666668\n",
      "33 Training acc 0.732972972972973 loss 0.5872370579121342 300\n",
      "lamda tensor(0.5834, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9785, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "34 Training acc 0.71 loss 0.6036920142657654 50\n",
      "lamda tensor(0.5833, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9718, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "34 Training acc 0.7 loss 0.6013257802983363 100\n",
      "lamda tensor(0.5842, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9691, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6999999999999998\n",
      "34 Training acc 0.7066666666666667 loss 0.600026794352857 150\n",
      "lamda tensor(0.5841, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8369, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6974999999999999\n",
      "34 Training acc 0.7175 loss 0.5953227911977592 200\n",
      "lamda tensor(0.5850, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8737, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6979999999999998\n",
      "34 Training acc 0.72 loss 0.5941598784464127 250\n",
      "lamda tensor(0.5847, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0006, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6983333333333333\n",
      "34 Training acc 0.7306306306306306 loss 0.5898048229007568 300\n",
      "lamda tensor(0.5855, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0027, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "35 Training acc 0.7 loss 0.6055414436450969 50\n",
      "lamda tensor(0.5853, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8212, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "35 Training acc 0.745 loss 0.5819786094239414 100\n",
      "lamda tensor(0.5862, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9924, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6999999999999998\n",
      "35 Training acc 0.7366666666666667 loss 0.5883673225803953 150\n",
      "lamda tensor(0.5881, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9700, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "35 Training acc 0.7324999999999999 loss 0.5914964588103365 200\n",
      "lamda tensor(0.5888, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9172, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "35 Training acc 0.728 loss 0.5882082556282111 250\n",
      "lamda tensor(0.5904, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8806, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7000000000000001\n",
      "35 Training acc 0.7282882882882883 loss 0.5903795514315825 300\n",
      "lamda tensor(0.5924, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8801, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "36 Training acc 0.74 loss 0.5907836054239103 50\n",
      "lamda tensor(0.5933, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8972, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "36 Training acc 0.765 loss 0.5778466714811336 100\n",
      "lamda tensor(0.5931, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6999999999999998\n",
      "36 Training acc 0.7533333333333333 loss 0.5826625655818299 150\n",
      "lamda tensor(0.5940, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0335, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "36 Training acc 0.7274999999999999 loss 0.592145210484937 200\n",
      "lamda tensor(0.5938, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9153, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "36 Training acc 0.732 loss 0.5850755345706784 250\n",
      "lamda tensor(0.5926, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0031, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7000000000000001\n",
      "36 Training acc 0.7271171171171171 loss 0.5855256410101658 300\n",
      "lamda tensor(0.5925, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9685, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "37 Training acc 0.72 loss 0.5963578077620373 50\n",
      "lamda tensor(0.5932, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9076, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "37 Training acc 0.705 loss 0.5946641159758654 100\n",
      "lamda tensor(0.5927, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9182, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6999999999999998\n",
      "37 Training acc 0.73 loss 0.5781262798338496 150\n",
      "lamda tensor(0.5913, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9461, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "37 Training acc 0.7375 loss 0.5765190225609969 200\n",
      "lamda tensor(0.5910, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9165, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "37 Training acc 0.7300000000000001 loss 0.5805495182229333 250\n",
      "lamda tensor(0.5918, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0332, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7000000000000001\n",
      "37 Training acc 0.729954954954955 loss 0.5857261543350383 300\n",
      "lamda tensor(0.5936, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9247, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "38 Training acc 0.71 loss 0.5948271187345663 50\n",
      "lamda tensor(0.5942, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9454, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "38 Training acc 0.6950000000000001 loss 0.5932602371456982 100\n",
      "lamda tensor(0.5936, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.6999999999999998\n",
      "38 Training acc 0.7366666666666667 loss 0.5809349822075017 150\n",
      "lamda tensor(0.5921, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9471, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "38 Training acc 0.735 loss 0.5803926635464425 200\n",
      "lamda tensor(0.5898, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8777, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "38 Training acc 0.74 loss 0.5774560639680205 250\n",
      "lamda tensor(0.5889, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0708, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7000000000000001\n",
      "38 Training acc 0.7202702702702704 loss 0.5834346581483602 300\n",
      "lamda tensor(0.5870, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7\n",
      "39 Training acc 0.77 loss 0.5712652125447797 50\n",
      "lamda tensor(0.5863, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9447, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.705\n",
      "39 Training acc 0.75 loss 0.5827935713597999 100\n",
      "lamda tensor(0.5847, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9365, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7066666666666667\n",
      "39 Training acc 0.7433333333333333 loss 0.5829739129127193 150\n",
      "lamda tensor(0.5823, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9141, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7075\n",
      "39 Training acc 0.745 loss 0.5805565312788288 200\n",
      "lamda tensor(0.5791, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9128, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.708\n",
      "39 Training acc 0.734 loss 0.5802335784050472 250\n",
      "lamda tensor(0.5753, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8505, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7083333333333334\n",
      "39 Training acc 0.7422972972972973 loss 0.5761086828486356 300\n",
      "lamda tensor(0.5730, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9893, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "40 Training acc 0.71 loss 0.5943905659788221 50\n",
      "lamda tensor(0.5720, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9865, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "40 Training acc 0.72 loss 0.5925410585778526 100\n",
      "lamda tensor(0.5701, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9065, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7133333333333333\n",
      "40 Training acc 0.7399999999999999 loss 0.5806281950907787 150\n",
      "lamda tensor(0.5676, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8331, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7149999999999999\n",
      "40 Training acc 0.7449999999999999 loss 0.5778391912649667 200\n",
      "lamda tensor(0.5663, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8962, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7159999999999999\n",
      "40 Training acc 0.7459999999999999 loss 0.5793848583899738 250\n",
      "lamda tensor(0.5641, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0330, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7166666666666665\n",
      "40 Training acc 0.7477927927927928 loss 0.5762431334266735 300\n",
      "lamda tensor(0.5630, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8968, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "41 Training acc 0.78 loss 0.5659837815672575 50\n",
      "lamda tensor(0.5631, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9793, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "41 Training acc 0.75 loss 0.5800970584426219 100\n",
      "lamda tensor(0.5643, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9953, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7200000000000001\n",
      "41 Training acc 0.7366666666666667 loss 0.5835160865413426 150\n",
      "lamda tensor(0.5645, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8369, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7175\n",
      "41 Training acc 0.7475 loss 0.5779705161941721 200\n",
      "lamda tensor(0.5636, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9566, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.716\n",
      "41 Training acc 0.7540000000000001 loss 0.5737278702457248 250\n",
      "lamda tensor(0.5640, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0415, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.715\n",
      "41 Training acc 0.740945945945946 loss 0.5781202048644093 300\n",
      "lamda tensor(0.5655, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0050, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "42 Training acc 0.72 loss 0.581756719251858 50\n",
      "lamda tensor(0.5678, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9981, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "42 Training acc 0.715 loss 0.5879018403620004 100\n",
      "lamda tensor(0.5710, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9392, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "42 Training acc 0.7200000000000001 loss 0.5826905482493352 150\n",
      "lamda tensor(0.5729, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8216, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "42 Training acc 0.7250000000000001 loss 0.5786979308323696 200\n",
      "lamda tensor(0.5737, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9323, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "42 Training acc 0.74 loss 0.5733192300158364 250\n",
      "lamda tensor(0.5734, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9052, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.71\n",
      "42 Training acc 0.7518018018018018 loss 0.5690553994132005 300\n",
      "lamda tensor(0.5722, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8204, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "43 Training acc 0.72 loss 0.5698001644073836 50\n",
      "lamda tensor(0.5701, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9615, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "43 Training acc 0.73 loss 0.5658531312020079 100\n",
      "lamda tensor(0.5672, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9420, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7200000000000001\n",
      "43 Training acc 0.75 loss 0.5632168000437675 150\n",
      "lamda tensor(0.5656, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9155, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "43 Training acc 0.75 loss 0.5654202733296204 200\n",
      "lamda tensor(0.5652, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9560, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "43 Training acc 0.746 loss 0.5680927937941735 250\n",
      "lamda tensor(0.5659, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0108, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7199999999999999\n",
      "43 Training acc 0.7568018018018018 loss 0.569246375370467 300\n",
      "lamda tensor(0.5656, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8540, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "44 Training acc 0.79 loss 0.562653515627903 50\n",
      "lamda tensor(0.5664, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9659, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "44 Training acc 0.775 loss 0.56776176824055 100\n",
      "lamda tensor(0.5661, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9303, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7200000000000001\n",
      "44 Training acc 0.7533333333333333 loss 0.5653089577262042 150\n",
      "lamda tensor(0.5649, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9167, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "44 Training acc 0.7549999999999999 loss 0.5641402324982203 200\n",
      "lamda tensor(0.5628, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8872, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "44 Training acc 0.76 loss 0.5616789582038976 250\n",
      "lamda tensor(0.5622, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1653, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7199999999999999\n",
      "44 Training acc 0.7414414414414413 loss 0.5725188071968522 300\n",
      "lamda tensor(0.5627, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8744, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "45 Training acc 0.73 loss 0.5680278068708199 50\n",
      "lamda tensor(0.5641, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9334, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "45 Training acc 0.725 loss 0.5659929027160371 100\n",
      "lamda tensor(0.5664, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9747, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7200000000000001\n",
      "45 Training acc 0.7233333333333333 loss 0.5710180759800084 150\n",
      "lamda tensor(0.5695, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0001, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "45 Training acc 0.715 loss 0.5710793218269299 200\n",
      "lamda tensor(0.5713, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8989, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "45 Training acc 0.732 loss 0.5650763496833336 250\n",
      "lamda tensor(0.5722, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7294, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7199999999999999\n",
      "45 Training acc 0.7541441441441442 loss 0.5623365316717668 300\n",
      "lamda tensor(0.5740, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9414, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "46 Training acc 0.7 loss 0.5777405527070252 50\n",
      "lamda tensor(0.5766, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9667, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "46 Training acc 0.71 loss 0.5796822580321603 100\n",
      "lamda tensor(0.5779, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9742, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7200000000000001\n",
      "46 Training acc 0.7466666666666666 loss 0.5619819336682679 150\n",
      "lamda tensor(0.5780, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9613, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "46 Training acc 0.76 loss 0.5569602491202403 200\n",
      "lamda tensor(0.5770, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9102, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "46 Training acc 0.766 loss 0.5560411048452341 250\n",
      "lamda tensor(0.5774, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0922, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7216666666666667\n",
      "46 Training acc 0.741936936936937 loss 0.5674508787155018 300\n",
      "lamda tensor(0.5769, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7699, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "47 Training acc 0.76 loss 0.5415282094197461 50\n",
      "lamda tensor(0.5755, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9211, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.725\n",
      "47 Training acc 0.79 loss 0.543571370508405 100\n",
      "lamda tensor(0.5752, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9595, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7233333333333333\n",
      "47 Training acc 0.7866666666666667 loss 0.5555303350875271 150\n",
      "lamda tensor(0.5739, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0150, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7224999999999999\n",
      "47 Training acc 0.7775000000000001 loss 0.558037026781083 200\n",
      "lamda tensor(0.5737, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9620, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7219999999999999\n",
      "47 Training acc 0.764 loss 0.5633087417489273 250\n",
      "lamda tensor(0.5723, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0496, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7216666666666666\n",
      "47 Training acc 0.7672972972972975 loss 0.5527795348719242 300\n",
      "lamda tensor(0.5700, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9624, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "48 Training acc 0.76 loss 0.5558255467442371 50\n",
      "lamda tensor(0.5671, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8547, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "48 Training acc 0.745 loss 0.5562284539273774 100\n",
      "lamda tensor(0.5634, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8884, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7200000000000001\n",
      "48 Training acc 0.77 loss 0.5456880856311446 150\n",
      "lamda tensor(0.5611, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9429, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7225\n",
      "48 Training acc 0.7775000000000001 loss 0.5517086384948445 200\n",
      "lamda tensor(0.5581, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9535, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.724\n",
      "48 Training acc 0.7780000000000001 loss 0.5534351447495532 250\n",
      "lamda tensor(0.5568, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1904, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.725\n",
      "48 Training acc 0.760945945945946 loss 0.5660778759276566 300\n",
      "lamda tensor(0.5567, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9847, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "49 Training acc 0.79 loss 0.5688100191696015 50\n",
      "lamda tensor(0.5555, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "49 Training acc 0.795 loss 0.5506224355049856 100\n",
      "lamda tensor(0.5555, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8933, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7333333333333334\n",
      "49 Training acc 0.7799999999999999 loss 0.5548913575696147 150\n",
      "lamda tensor(0.5564, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9160, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7350000000000001\n",
      "49 Training acc 0.7775 loss 0.5563131945636601 200\n",
      "lamda tensor(0.5582, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9423, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7360000000000001\n",
      "49 Training acc 0.7699999999999999 loss 0.558788259959226 250\n",
      "lamda tensor(0.5589, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9052, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7366666666666668\n",
      "49 Training acc 0.7632882882882882 loss 0.5583004710757512 300\n",
      "lamda tensor(0.5606, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9732, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "50 Training acc 0.75 loss 0.5751355679344714 50\n",
      "lamda tensor(0.5610, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9736, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "50 Training acc 0.77 loss 0.5567714777164452 100\n",
      "lamda tensor(0.5604, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9078, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "50 Training acc 0.7933333333333333 loss 0.547109999364162 150\n",
      "lamda tensor(0.5589, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9313, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "50 Training acc 0.79 loss 0.5471122416724032 200\n",
      "lamda tensor(0.5585, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9499, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "50 Training acc 0.776 loss 0.5527287477934106 250\n",
      "lamda tensor(0.5593, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9781, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "50 Training acc 0.7592792792792792 loss 0.5572411311633999 300\n",
      "lamda tensor(0.5590, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8945, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "51 Training acc 0.8 loss 0.5332063553447531 50\n",
      "lamda tensor(0.5598, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9780, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.725\n",
      "51 Training acc 0.755 loss 0.5528052645827404 100\n",
      "lamda tensor(0.5594, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9440, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7266666666666666\n",
      "51 Training acc 0.7733333333333334 loss 0.5484670713874641 150\n",
      "lamda tensor(0.5601, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9035, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7274999999999999\n",
      "51 Training acc 0.7775000000000001 loss 0.5526135257350604 200\n",
      "lamda tensor(0.5617, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9650, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.728\n",
      "51 Training acc 0.768 loss 0.5554714290118716 250\n",
      "lamda tensor(0.5620, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0385, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "51 Training acc 0.7841441441441441 loss 0.5450823031917996 300\n",
      "lamda tensor(0.5614, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8447, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "52 Training acc 0.78 loss 0.5544607848199847 50\n",
      "lamda tensor(0.5598, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9549, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "52 Training acc 0.755 loss 0.5515882560162807 100\n",
      "lamda tensor(0.5574, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9533, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7399999999999999\n",
      "52 Training acc 0.7433333333333333 loss 0.5540952975682908 150\n",
      "lamda tensor(0.5542, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9851, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "52 Training acc 0.7575000000000001 loss 0.5479491483761263 200\n",
      "lamda tensor(0.5523, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9237, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "52 Training acc 0.76 loss 0.549696034331838 250\n",
      "lamda tensor(0.5496, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9156, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7400000000000001\n",
      "52 Training acc 0.7729729729729731 loss 0.5448896743218854 300\n",
      "lamda tensor(0.5482, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9162, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "53 Training acc 0.76 loss 0.562193965285516 50\n",
      "lamda tensor(0.5458, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9295, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "53 Training acc 0.78 loss 0.528214155297694 100\n",
      "lamda tensor(0.5448, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0020, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7399999999999999\n",
      "53 Training acc 0.7733333333333334 loss 0.5409489926116485 150\n",
      "lamda tensor(0.5429, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9101, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "53 Training acc 0.7775000000000001 loss 0.5376258142022735 200\n",
      "lamda tensor(0.5422, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9286, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "53 Training acc 0.774 loss 0.5410517103839941 250\n",
      "lamda tensor(0.5429, device='cuda:0', requires_grad=True)\n",
      "tensor(1.2014, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7400000000000001\n",
      "53 Training acc 0.753108108108108 loss 0.5538114292482446 300\n",
      "lamda tensor(0.5446, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9385, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "54 Training acc 0.72 loss 0.5749960602177936 50\n",
      "lamda tensor(0.5450, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9763, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "54 Training acc 0.78 loss 0.541630599192582 100\n",
      "lamda tensor(0.5444, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9300, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7366666666666667\n",
      "54 Training acc 0.7866666666666667 loss 0.5361234945035424 150\n",
      "lamda tensor(0.5449, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9528, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.735\n",
      "54 Training acc 0.7675000000000001 loss 0.5443203291176142 200\n",
      "lamda tensor(0.5444, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9635, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.734\n",
      "54 Training acc 0.7700000000000001 loss 0.5442097622917189 250\n",
      "lamda tensor(0.5449, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7316666666666666\n",
      "54 Training acc 0.7722972972972973 loss 0.5453301877755558 300\n",
      "lamda tensor(0.5444, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8887, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "55 Training acc 0.8 loss 0.5219893693662833 50\n",
      "lamda tensor(0.5451, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0710, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.72\n",
      "55 Training acc 0.765 loss 0.5511255460644691 100\n",
      "lamda tensor(0.5448, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9174, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7200000000000001\n",
      "55 Training acc 0.7766666666666667 loss 0.5393196883544942 150\n",
      "lamda tensor(0.5435, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9111, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7250000000000001\n",
      "55 Training acc 0.7825 loss 0.5400780635001987 200\n",
      "lamda tensor(0.5435, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9910, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7280000000000001\n",
      "55 Training acc 0.766 loss 0.546923681877026 250\n",
      "lamda tensor(0.5424, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8743, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7300000000000001\n",
      "55 Training acc 0.777972972972973 loss 0.5401842805715601 300\n",
      "lamda tensor(0.5405, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9152, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "56 Training acc 0.78 loss 0.5225557408278599 50\n",
      "lamda tensor(0.5398, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9360, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "56 Training acc 0.75 loss 0.5329053407304407 100\n",
      "lamda tensor(0.5402, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9692, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7399999999999999\n",
      "56 Training acc 0.7399999999999999 loss 0.5461416413109106 150\n",
      "lamda tensor(0.5396, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9220, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "56 Training acc 0.7599999999999999 loss 0.5407496968010277 200\n",
      "lamda tensor(0.5401, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9561, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.742\n",
      "56 Training acc 0.7719999999999999 loss 0.5420219697554618 250\n",
      "lamda tensor(0.5416, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9893, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7433333333333333\n",
      "56 Training acc 0.7694594594594594 loss 0.5461633410466193 300\n",
      "lamda tensor(0.5420, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8722, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "57 Training acc 0.75 loss 0.5369892184233814 50\n",
      "lamda tensor(0.5414, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9036, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "57 Training acc 0.7849999999999999 loss 0.5239813137039404 100\n",
      "lamda tensor(0.5398, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9344, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "57 Training acc 0.7833333333333332 loss 0.5270482142920151 150\n",
      "lamda tensor(0.5375, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8903, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "57 Training acc 0.7874999999999999 loss 0.5286694495205889 200\n",
      "lamda tensor(0.5364, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0078, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "57 Training acc 0.7779999999999999 loss 0.5342457165280928 250\n",
      "lamda tensor(0.5370, device='cuda:0', requires_grad=True)\n",
      "tensor(1.2508, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "57 Training acc 0.7654504504504503 loss 0.5506950427667403 300\n",
      "lamda tensor(0.5366, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8959, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "58 Training acc 0.77 loss 0.5355713657599368 50\n",
      "lamda tensor(0.5373, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0331, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "58 Training acc 0.745 loss 0.5561143256964802 100\n",
      "lamda tensor(0.5372, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7523, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "58 Training acc 0.7533333333333333 loss 0.5415974035453033 150\n",
      "lamda tensor(0.5381, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0018, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "58 Training acc 0.76 loss 0.5412590777092962 200\n",
      "lamda tensor(0.5399, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9145, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.752\n",
      "58 Training acc 0.764 loss 0.5414767838205253 250\n",
      "lamda tensor(0.5405, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9728, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7533333333333333\n",
      "58 Training acc 0.7808108108108108 loss 0.5381133030959996 300\n",
      "lamda tensor(0.5420, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9584, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "59 Training acc 0.7 loss 0.5611460010294969 50\n",
      "lamda tensor(0.5423, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9583, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "59 Training acc 0.77 loss 0.5264146517107295 100\n",
      "lamda tensor(0.5437, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9738, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "59 Training acc 0.7633333333333333 loss 0.5354846135963697 150\n",
      "lamda tensor(0.5459, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9851, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "59 Training acc 0.755 loss 0.5430829056820364 200\n",
      "lamda tensor(0.5471, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8575, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "59 Training acc 0.766 loss 0.5385875571146883 250\n",
      "lamda tensor(0.5470, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0292, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "59 Training acc 0.7734684684684684 loss 0.538358089527195 300\n",
      "lamda tensor(0.5459, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9391, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "60 Training acc 0.74 loss 0.5425965384726396 50\n",
      "lamda tensor(0.5460, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9587, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "60 Training acc 0.735 loss 0.561592364970364 100\n",
      "lamda tensor(0.5450, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9828, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "60 Training acc 0.7466666666666667 loss 0.5454061400348439 150\n",
      "lamda tensor(0.5430, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0022, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7525\n",
      "60 Training acc 0.765 loss 0.5362324124096299 200\n",
      "lamda tensor(0.5402, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9000, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7539999999999999\n",
      "60 Training acc 0.766 loss 0.5335594390972386 250\n",
      "lamda tensor(0.5388, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0209, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7549999999999999\n",
      "60 Training acc 0.777972972972973 loss 0.539123791395046 300\n",
      "lamda tensor(0.5386, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9554, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "61 Training acc 0.75 loss 0.5517106294910765 50\n",
      "lamda tensor(0.5394, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9536, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "61 Training acc 0.765 loss 0.5482336124564458 100\n",
      "lamda tensor(0.5392, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8806, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "61 Training acc 0.7733333333333334 loss 0.5359330523079341 150\n",
      "lamda tensor(0.5399, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9228, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "61 Training acc 0.76 loss 0.5389252671285983 200\n",
      "lamda tensor(0.5396, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9693, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "61 Training acc 0.766 loss 0.535126169831594 250\n",
      "lamda tensor(0.5382, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9992, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "61 Training acc 0.7824774774774775 loss 0.5305957041855666 300\n",
      "lamda tensor(0.5380, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "62 Training acc 0.73 loss 0.5686849741639887 50\n",
      "lamda tensor(0.5388, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9681, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "62 Training acc 0.745 loss 0.5630198742084536 100\n",
      "lamda tensor(0.5385, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9688, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7466666666666667\n",
      "62 Training acc 0.7733333333333333 loss 0.5463061954589642 150\n",
      "lamda tensor(0.5372, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9795, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7450000000000001\n",
      "62 Training acc 0.7849999999999999 loss 0.5331108482061623 200\n",
      "lamda tensor(0.5370, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9485, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7420000000000001\n",
      "62 Training acc 0.7699999999999999 loss 0.5346318787556854 250\n",
      "lamda tensor(0.5357, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9532, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7400000000000001\n",
      "62 Training acc 0.7858108108108107 loss 0.5220783733493581 300\n",
      "lamda tensor(0.5356, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9779, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "63 Training acc 0.76 loss 0.544326600414292 50\n",
      "lamda tensor(0.5346, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8793, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "63 Training acc 0.78 loss 0.535669641897504 100\n",
      "lamda tensor(0.5327, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9372, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "63 Training acc 0.7833333333333333 loss 0.5248427008035947 150\n",
      "lamda tensor(0.5299, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9153, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "63 Training acc 0.7875000000000001 loss 0.5197196525601688 200\n",
      "lamda tensor(0.5284, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9324, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "63 Training acc 0.776 loss 0.523178234127748 250\n",
      "lamda tensor(0.5284, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1488, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.73\n",
      "63 Training acc 0.7682882882882883 loss 0.533595633562952 300\n",
      "lamda tensor(0.5294, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0138, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "64 Training acc 0.77 loss 0.5430983106146599 50\n",
      "lamda tensor(0.5296, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7352, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "64 Training acc 0.79 loss 0.513677031607075 100\n",
      "lamda tensor(0.5286, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9704, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "64 Training acc 0.7966666666666667 loss 0.5157893042326541 150\n",
      "lamda tensor(0.5268, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9201, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "64 Training acc 0.7925 loss 0.5153364555421974 200\n",
      "lamda tensor(0.5263, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9929, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "64 Training acc 0.784 loss 0.5239835453451336 250\n",
      "lamda tensor(0.5270, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0810, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "64 Training acc 0.7659459459459459 loss 0.531453110568401 300\n",
      "lamda tensor(0.5265, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9934, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "65 Training acc 0.85 loss 0.4889012407909749 50\n",
      "lamda tensor(0.5271, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9583, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "65 Training acc 0.8 loss 0.5149138003858317 100\n",
      "lamda tensor(0.5288, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0383, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "65 Training acc 0.7733333333333334 loss 0.5331528701411624 150\n",
      "lamda tensor(0.5294, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8704, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "65 Training acc 0.7750000000000001 loss 0.5288955296153665 200\n",
      "lamda tensor(0.5290, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9014, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "65 Training acc 0.7760000000000001 loss 0.524833401383271 250\n",
      "lamda tensor(0.5297, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0332, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "65 Training acc 0.772792792792793 loss 0.5289615495664037 300\n",
      "lamda tensor(0.5313, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9456, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "66 Training acc 0.78 loss 0.537485725351177 50\n",
      "lamda tensor(0.5319, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8750, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "66 Training acc 0.785 loss 0.5267715807706266 100\n",
      "lamda tensor(0.5332, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8247, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "66 Training acc 0.7766666666666667 loss 0.530529966827027 150\n",
      "lamda tensor(0.5335, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9464, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "66 Training acc 0.765 loss 0.5291583197603998 200\n",
      "lamda tensor(0.5326, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9928, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "66 Training acc 0.774 loss 0.5268080676775708 250\n",
      "lamda tensor(0.5306, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0866, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "66 Training acc 0.7801351351351351 loss 0.521711763663868 300\n",
      "lamda tensor(0.5278, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9950, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "67 Training acc 0.77 loss 0.5135177539170257 50\n",
      "lamda tensor(0.5243, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8844, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "67 Training acc 0.8049999999999999 loss 0.5056170704666472 100\n",
      "lamda tensor(0.5221, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8878, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "67 Training acc 0.7799999999999999 loss 0.5134670952997964 150\n",
      "lamda tensor(0.5191, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9399, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "67 Training acc 0.7825 loss 0.5128595209389664 200\n",
      "lamda tensor(0.5174, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9274, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "67 Training acc 0.7779999999999999 loss 0.5178170953135305 250\n",
      "lamda tensor(0.5172, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1428, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "67 Training acc 0.7744594594594595 loss 0.5268821499540342 300\n",
      "lamda tensor(0.5182, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1037, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "68 Training acc 0.71 loss 0.5699241392376555 50\n",
      "lamda tensor(0.5180, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9665, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "68 Training acc 0.7649999999999999 loss 0.5283286741080339 100\n",
      "lamda tensor(0.5170, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8227, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7399999999999999\n",
      "68 Training acc 0.7833333333333332 loss 0.5232301868245627 150\n",
      "lamda tensor(0.5151, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9028, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "68 Training acc 0.7799999999999999 loss 0.5183816507796907 200\n",
      "lamda tensor(0.5145, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9551, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "68 Training acc 0.7739999999999999 loss 0.522868166575823 250\n",
      "lamda tensor(0.5127, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0553, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7400000000000001\n",
      "68 Training acc 0.7801351351351351 loss 0.514341714465177 300\n",
      "lamda tensor(0.5121, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9048, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "69 Training acc 0.76 loss 0.5354205926686507 50\n",
      "lamda tensor(0.5107, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8781, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "69 Training acc 0.78 loss 0.5198262454135676 100\n",
      "lamda tensor(0.5105, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0522, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7399999999999999\n",
      "69 Training acc 0.7566666666666667 loss 0.5330529212987692 150\n",
      "lamda tensor(0.5095, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8691, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "69 Training acc 0.7675000000000001 loss 0.5267236630712933 200\n",
      "lamda tensor(0.5075, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9931, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.74\n",
      "69 Training acc 0.784 loss 0.5169128772586353 250\n",
      "lamda tensor(0.5068, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0840, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7400000000000001\n",
      "69 Training acc 0.7749549549549549 loss 0.5197716103108213 300\n",
      "lamda tensor(0.5073, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9642, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "70 Training acc 0.77 loss 0.5304000899686772 50\n",
      "lamda tensor(0.5086, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9253, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "70 Training acc 0.745 loss 0.5314507967227872 100\n",
      "lamda tensor(0.5089, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9359, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "70 Training acc 0.7666666666666666 loss 0.5186012880424689 150\n",
      "lamda tensor(0.5102, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9612, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "70 Training acc 0.77 loss 0.5201459206782351 200\n",
      "lamda tensor(0.5103, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9271, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "70 Training acc 0.776 loss 0.5159357029071409 250\n",
      "lamda tensor(0.5115, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0103, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "70 Training acc 0.7818018018018017 loss 0.5186142565164813 300\n",
      "lamda tensor(0.5137, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9551, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "71 Training acc 0.74 loss 0.5443922959152603 50\n",
      "lamda tensor(0.5147, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8317, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "71 Training acc 0.755 loss 0.5238413282808929 100\n",
      "lamda tensor(0.5147, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8737, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "71 Training acc 0.77 loss 0.5159565653477932 150\n",
      "lamda tensor(0.5159, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0510, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "71 Training acc 0.77 loss 0.5237142014713867 200\n",
      "lamda tensor(0.5158, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0126, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "71 Training acc 0.778 loss 0.5170011746862593 250\n",
      "lamda tensor(0.5147, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9856, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "71 Training acc 0.796981981981982 loss 0.5126305661740856 300\n",
      "lamda tensor(0.5127, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9657, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "72 Training acc 0.77 loss 0.5022507162314794 50\n",
      "lamda tensor(0.5099, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8964, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "72 Training acc 0.785 loss 0.5026315758384505 100\n",
      "lamda tensor(0.5086, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1005, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "72 Training acc 0.7666666666666666 loss 0.5254882594389213 150\n",
      "lamda tensor(0.5085, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9320, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "72 Training acc 0.7649999999999999 loss 0.5265164610638119 200\n",
      "lamda tensor(0.5074, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8704, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "72 Training acc 0.7779999999999999 loss 0.5203816179393858 250\n",
      "lamda tensor(0.5053, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0769, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "72 Training acc 0.7879729729729729 loss 0.5070193236676842 300\n",
      "lamda tensor(0.5044, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9671, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "73 Training acc 0.74 loss 0.5370085466192901 50\n",
      "lamda tensor(0.5047, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0020, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.755\n",
      "73 Training acc 0.775 loss 0.530988522290696 100\n",
      "lamda tensor(0.5059, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9132, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7533333333333333\n",
      "73 Training acc 0.79 loss 0.523403179207997 150\n",
      "lamda tensor(0.5059, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9112, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7525\n",
      "73 Training acc 0.8025 loss 0.5124847403580273 200\n",
      "lamda tensor(0.5051, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8841, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.752\n",
      "73 Training acc 0.792 loss 0.5106201966897623 250\n",
      "lamda tensor(0.5056, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1003, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7516666666666666\n",
      "73 Training acc 0.7816216216216216 loss 0.5181958013623545 300\n",
      "lamda tensor(0.5051, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8848, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "74 Training acc 0.81 loss 0.502442827625605 50\n",
      "lamda tensor(0.5036, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9305, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "74 Training acc 0.795 loss 0.4973108780980212 100\n",
      "lamda tensor(0.5036, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1064, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "74 Training acc 0.7799999999999999 loss 0.5194554229007206 150\n",
      "lamda tensor(0.5027, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7019, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "74 Training acc 0.7825 loss 0.5097855265748624 200\n",
      "lamda tensor(0.5031, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0034, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "74 Training acc 0.784 loss 0.509682660241094 250\n",
      "lamda tensor(0.5044, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0135, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "74 Training acc 0.7884684684684684 loss 0.5124510561244274 300\n",
      "lamda tensor(0.5049, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7132, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "75 Training acc 0.83 loss 0.47393936063839903 50\n",
      "lamda tensor(0.5065, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0537, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "75 Training acc 0.77 loss 0.5078758073988072 100\n",
      "lamda tensor(0.5089, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9655, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "75 Training acc 0.77 loss 0.5081400936338492 150\n",
      "lamda tensor(0.5101, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8976, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "75 Training acc 0.7775000000000001 loss 0.5077419470132077 200\n",
      "lamda tensor(0.5122, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9379, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "75 Training acc 0.786 loss 0.5089679883396608 250\n",
      "lamda tensor(0.5130, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0121, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7516666666666666\n",
      "75 Training acc 0.7811261261261261 loss 0.507766922305967 300\n",
      "lamda tensor(0.5127, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9662, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "76 Training acc 0.81 loss 0.48841379702564286 50\n",
      "lamda tensor(0.5135, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9454, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "76 Training acc 0.785 loss 0.5044179962405025 100\n",
      "lamda tensor(0.5152, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9919, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "76 Training acc 0.7766666666666667 loss 0.5159578968964156 150\n",
      "lamda tensor(0.5156, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9987, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "76 Training acc 0.7875 loss 0.508536068967156 200\n",
      "lamda tensor(0.5152, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8228, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.758\n",
      "76 Training acc 0.782 loss 0.508949507353644 250\n",
      "lamda tensor(0.5137, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0026, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7566666666666667\n",
      "76 Training acc 0.7913063063063063 loss 0.5013099757957366 300\n",
      "lamda tensor(0.5113, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0244, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "77 Training acc 0.8 loss 0.4904955566282517 50\n",
      "lamda tensor(0.5081, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9001, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "77 Training acc 0.795 loss 0.49505286901527046 100\n",
      "lamda tensor(0.5062, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9036, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "77 Training acc 0.7966666666666667 loss 0.5012931385403954 150\n",
      "lamda tensor(0.5036, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8774, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "77 Training acc 0.7875000000000001 loss 0.49837758553759626 200\n",
      "lamda tensor(0.5022, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9009, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "77 Training acc 0.7860000000000001 loss 0.5018445268201815 250\n",
      "lamda tensor(0.5020, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0724, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "77 Training acc 0.7856306306306307 loss 0.5059395835755517 300\n",
      "lamda tensor(0.5009, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9697, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "78 Training acc 0.83 loss 0.47567410494191664 50\n",
      "lamda tensor(0.4989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8839, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "78 Training acc 0.825 loss 0.473354210296013 100\n",
      "lamda tensor(0.4981, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9250, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "78 Training acc 0.7933333333333333 loss 0.48519645916552606 150\n",
      "lamda tensor(0.4983, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8999, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "78 Training acc 0.8049999999999999 loss 0.48932416584057536 200\n",
      "lamda tensor(0.4998, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1585, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "78 Training acc 0.78 loss 0.506773685624754 250\n",
      "lamda tensor(0.5000, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0300, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "78 Training acc 0.7941441441441442 loss 0.494816693864449 300\n",
      "lamda tensor(0.4992, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8978, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "79 Training acc 0.78 loss 0.4811049994175805 50\n",
      "lamda tensor(0.4975, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9240, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "79 Training acc 0.79 loss 0.47788904078271444 100\n",
      "lamda tensor(0.4949, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9846, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "79 Training acc 0.8066666666666666 loss 0.4686124013558484 150\n",
      "lamda tensor(0.4938, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0533, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "79 Training acc 0.79 loss 0.4932446170902157 200\n",
      "lamda tensor(0.4938, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9861, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "79 Training acc 0.784 loss 0.4990225619174097 250\n",
      "lamda tensor(0.4949, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0628, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "79 Training acc 0.7884684684684684 loss 0.5009810869872928 300\n",
      "lamda tensor(0.4950, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9155, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "80 Training acc 0.76 loss 0.4827826322618293 50\n",
      "lamda tensor(0.4941, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8694, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.755\n",
      "80 Training acc 0.7949999999999999 loss 0.4828553480579654 100\n",
      "lamda tensor(0.4944, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9277, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7566666666666667\n",
      "80 Training acc 0.7933333333333333 loss 0.4904181951311756 150\n",
      "lamda tensor(0.4956, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0089, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7575000000000001\n",
      "80 Training acc 0.7925 loss 0.49305610741068717 200\n",
      "lamda tensor(0.4978, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9955, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.758\n",
      "80 Training acc 0.782 loss 0.5013832488730917 250\n",
      "lamda tensor(0.4989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7818, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7583333333333333\n",
      "80 Training acc 0.7868018018018018 loss 0.4980953787149376 300\n",
      "lamda tensor(0.5009, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9053, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "81 Training acc 0.77 loss 0.5134775278415427 50\n",
      "lamda tensor(0.5038, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0238, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "81 Training acc 0.735 loss 0.5250771260544896 100\n",
      "lamda tensor(0.5074, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9130, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "81 Training acc 0.7533333333333333 loss 0.5191502203462829 150\n",
      "lamda tensor(0.5096, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8843, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "81 Training acc 0.7725 loss 0.5061483316495151 200\n",
      "lamda tensor(0.5106, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9836, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "81 Training acc 0.776 loss 0.49886253902367345 250\n",
      "lamda tensor(0.5103, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0841, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "81 Training acc 0.7863063063063063 loss 0.4963725130989507 300\n",
      "lamda tensor(0.5090, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9684, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "82 Training acc 0.78 loss 0.501745656059168 50\n",
      "lamda tensor(0.5069, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8828, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "82 Training acc 0.775 loss 0.49554531082935915 100\n",
      "lamda tensor(0.5040, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9407, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "82 Training acc 0.7866666666666667 loss 0.4882544990956775 150\n",
      "lamda tensor(0.5004, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8973, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "82 Training acc 0.7875000000000001 loss 0.4875366120558148 200\n",
      "lamda tensor(0.4982, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9319, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "82 Training acc 0.782 loss 0.4914787128404992 250\n",
      "lamda tensor(0.4974, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1216, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "82 Training acc 0.7913063063063063 loss 0.4997816875169316 300\n",
      "lamda tensor(0.4958, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8430, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "83 Training acc 0.77 loss 0.48038978099255786 50\n",
      "lamda tensor(0.4932, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0692, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "83 Training acc 0.7949999999999999 loss 0.4598913703330736 100\n",
      "lamda tensor(0.4920, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0570, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "83 Training acc 0.7933333333333333 loss 0.48645606003263214 150\n",
      "lamda tensor(0.4919, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9727, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "83 Training acc 0.7875 loss 0.49326798560583407 200\n",
      "lamda tensor(0.4910, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8823, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "83 Training acc 0.782 loss 0.4909403943488134 250\n",
      "lamda tensor(0.4912, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0338, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "83 Training acc 0.7777927927927929 loss 0.49894732032604816 300\n",
      "lamda tensor(0.4925, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9924, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "84 Training acc 0.77 loss 0.5118352690375738 50\n",
      "lamda tensor(0.4946, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9159, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "84 Training acc 0.765 loss 0.5046935567999795 100\n",
      "lamda tensor(0.4975, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9219, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "84 Training acc 0.7600000000000001 loss 0.5041064074679915 150\n",
      "lamda tensor(0.4990, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9658, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "84 Training acc 0.775 loss 0.4993305444831356 200\n",
      "lamda tensor(0.4995, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9146, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "84 Training acc 0.78 loss 0.4953611171168184 250\n",
      "lamda tensor(0.4989, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9155, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "84 Training acc 0.7941441441441442 loss 0.486227985513441 300\n",
      "lamda tensor(0.4973, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "85 Training acc 0.84 loss 0.46894914248445874 50\n",
      "lamda tensor(0.4949, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9498, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "85 Training acc 0.81 loss 0.4750036850414072 100\n",
      "lamda tensor(0.4933, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8942, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "85 Training acc 0.8000000000000002 loss 0.4817585698325207 150\n",
      "lamda tensor(0.4910, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8855, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "85 Training acc 0.8025000000000001 loss 0.48163124120967427 200\n",
      "lamda tensor(0.4899, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9438, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "85 Training acc 0.792 loss 0.4848762735101914 250\n",
      "lamda tensor(0.4902, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1824, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "85 Training acc 0.7726126126126127 loss 0.49704267590732926 300\n",
      "lamda tensor(0.4914, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9328, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "86 Training acc 0.76 loss 0.49112041773484977 50\n",
      "lamda tensor(0.4915, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9159, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "86 Training acc 0.77 loss 0.48872566155823327 100\n",
      "lamda tensor(0.4907, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8839, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7666666666666666\n",
      "86 Training acc 0.7633333333333333 loss 0.4859211613302219 150\n",
      "lamda tensor(0.4910, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9626, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "86 Training acc 0.77 loss 0.4898194620518576 200\n",
      "lamda tensor(0.4922, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9175, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.772\n",
      "86 Training acc 0.77 loss 0.49270249920566006 250\n",
      "lamda tensor(0.4922, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0135, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7733333333333334\n",
      "86 Training acc 0.7813063063063064 loss 0.4887471055413645 300\n",
      "lamda tensor(0.4931, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8842, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "87 Training acc 0.72 loss 0.5035497387421108 50\n",
      "lamda tensor(0.4950, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9739, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "87 Training acc 0.735 loss 0.5040105792284159 100\n",
      "lamda tensor(0.4977, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9435, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7799999999999999\n",
      "87 Training acc 0.7533333333333333 loss 0.5032487985753166 150\n",
      "lamda tensor(0.4991, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9529, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "87 Training acc 0.7675 loss 0.4899448461280138 200\n",
      "lamda tensor(0.4993, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9706, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "87 Training acc 0.7699999999999999 loss 0.4898369486027514 250\n",
      "lamda tensor(0.4985, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9068, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7783333333333333\n",
      "87 Training acc 0.7813063063063063 loss 0.4869457139333034 300\n",
      "lamda tensor(0.4968, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8963, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "88 Training acc 0.83 loss 0.4889125393017041 50\n",
      "lamda tensor(0.4943, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9709, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "88 Training acc 0.7849999999999999 loss 0.48533165076816487 100\n",
      "lamda tensor(0.4910, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9461, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "88 Training acc 0.79 loss 0.47357843869721084 150\n",
      "lamda tensor(0.4872, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8342, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "88 Training acc 0.7875000000000001 loss 0.4776058902053853 200\n",
      "lamda tensor(0.4850, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0841, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.772\n",
      "88 Training acc 0.776 loss 0.4896281206885574 250\n",
      "lamda tensor(0.4818, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0909, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7733333333333334\n",
      "88 Training acc 0.790810810810811 loss 0.4810430154657497 300\n",
      "lamda tensor(0.4800, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0074, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "89 Training acc 0.79 loss 0.5097332044217319 50\n",
      "lamda tensor(0.4774, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9493, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "89 Training acc 0.77 loss 0.48999379268140064 100\n",
      "lamda tensor(0.4740, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9944, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7799999999999999\n",
      "89 Training acc 0.79 loss 0.4768520586687852 150\n",
      "lamda tensor(0.4720, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0087, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "89 Training acc 0.7675000000000001 loss 0.49085746758986304 200\n",
      "lamda tensor(0.4712, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9063, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "89 Training acc 0.772 loss 0.4901012537705067 250\n",
      "lamda tensor(0.4696, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8033, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7800000000000001\n",
      "89 Training acc 0.782972972972973 loss 0.48204581229373084 300\n",
      "lamda tensor(0.4691, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9091, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.78\n",
      "90 Training acc 0.81 loss 0.48617655064051063 50\n",
      "lamda tensor(0.4679, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8020, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.775\n",
      "90 Training acc 0.79 loss 0.4759016261353127 100\n",
      "lamda tensor(0.4657, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9626, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7733333333333334\n",
      "90 Training acc 0.8066666666666666 loss 0.465626639395276 150\n",
      "lamda tensor(0.4647, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9151, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7725000000000001\n",
      "90 Training acc 0.8025 loss 0.46863830934963296 200\n",
      "lamda tensor(0.4651, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1199, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7700000000000001\n",
      "90 Training acc 0.79 loss 0.47978710219805565 250\n",
      "lamda tensor(0.4668, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1883, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7683333333333334\n",
      "90 Training acc 0.7754504504504505 loss 0.4918014460703925 300\n",
      "lamda tensor(0.4693, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9982, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "91 Training acc 0.79 loss 0.4737646647235593 50\n",
      "lamda tensor(0.4725, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8581, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "91 Training acc 0.79 loss 0.48440756740949026 100\n",
      "lamda tensor(0.4746, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7745, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "91 Training acc 0.7966666666666667 loss 0.4767848708828372 150\n",
      "lamda tensor(0.4775, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9307, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "91 Training acc 0.79 loss 0.47779746160068337 200\n",
      "lamda tensor(0.4811, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0547, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "91 Training acc 0.784 loss 0.48452144761508 250\n",
      "lamda tensor(0.4834, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0046, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "91 Training acc 0.7794594594594595 loss 0.47824961968251406 300\n",
      "lamda tensor(0.4844, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9152, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "92 Training acc 0.79 loss 0.4713956247510436 50\n",
      "lamda tensor(0.4843, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9534, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "92 Training acc 0.815 loss 0.44008165840692626 100\n",
      "lamda tensor(0.4833, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8813, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "92 Training acc 0.8166666666666665 loss 0.4515918666253973 150\n",
      "lamda tensor(0.4834, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0232, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "92 Training acc 0.815 loss 0.4619150109508942 200\n",
      "lamda tensor(0.4846, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9367, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.75\n",
      "92 Training acc 0.7939999999999999 loss 0.4724506041318851 250\n",
      "lamda tensor(0.4869, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1895, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7516666666666666\n",
      "92 Training acc 0.7742792792792792 loss 0.48901266186491293 300\n",
      "lamda tensor(0.4901, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9335, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "93 Training acc 0.76 loss 0.5164746038195992 50\n",
      "lamda tensor(0.4919, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9527, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "93 Training acc 0.785 loss 0.5004062762468469 100\n",
      "lamda tensor(0.4925, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9832, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "93 Training acc 0.79 loss 0.48196290022878663 150\n",
      "lamda tensor(0.4941, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0185, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "93 Training acc 0.7775000000000001 loss 0.49065014422561726 200\n",
      "lamda tensor(0.4944, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9510, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "93 Training acc 0.78 loss 0.47839580270096616 250\n",
      "lamda tensor(0.4937, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0114, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "93 Training acc 0.7896396396396397 loss 0.47737027147011585 300\n",
      "lamda tensor(0.4940, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8802, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "94 Training acc 0.75 loss 0.5215315512367638 50\n",
      "lamda tensor(0.4932, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9736, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "94 Training acc 0.7849999999999999 loss 0.493605684048851 100\n",
      "lamda tensor(0.4914, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9930, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "94 Training acc 0.7933333333333333 loss 0.4737943777114262 150\n",
      "lamda tensor(0.4888, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9358, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7625000000000001\n",
      "94 Training acc 0.7925 loss 0.47427980718864815 200\n",
      "lamda tensor(0.4875, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9978, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.764\n",
      "94 Training acc 0.782 loss 0.4808870091280671 250\n",
      "lamda tensor(0.4851, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1193, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.765\n",
      "94 Training acc 0.7913063063063063 loss 0.47356881796560074 300\n",
      "lamda tensor(0.4840, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9545, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "95 Training acc 0.76 loss 0.5254045198283633 50\n",
      "lamda tensor(0.4820, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9343, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "95 Training acc 0.79 loss 0.4897535679902906 100\n",
      "lamda tensor(0.4813, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9648, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "95 Training acc 0.7766666666666667 loss 0.497371164235129 150\n",
      "lamda tensor(0.4795, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0072, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "95 Training acc 0.7875 loss 0.48368645454682535 200\n",
      "lamda tensor(0.4789, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0003, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.768\n",
      "95 Training acc 0.78 loss 0.4859320663043574 250\n",
      "lamda tensor(0.4772, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0609, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7666666666666666\n",
      "95 Training acc 0.7896396396396397 loss 0.4680568828155171 300\n",
      "lamda tensor(0.4766, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8948, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "96 Training acc 0.76 loss 0.5034562259110656 50\n",
      "lamda tensor(0.4751, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9778, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "96 Training acc 0.7949999999999999 loss 0.4748400687051572 100\n",
      "lamda tensor(0.4748, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0780, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "96 Training acc 0.7699999999999999 loss 0.48876096866409036 150\n",
      "lamda tensor(0.4736, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8974, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "96 Training acc 0.7774999999999999 loss 0.48271983060274537 200\n",
      "lamda tensor(0.4716, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9165, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "96 Training acc 0.7799999999999999 loss 0.4783642921331902 250\n",
      "lamda tensor(0.4689, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9059, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "96 Training acc 0.7896396396396396 loss 0.47678304075047856 300\n",
      "lamda tensor(0.4675, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0376, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "97 Training acc 0.78 loss 0.48934911060122405 50\n",
      "lamda tensor(0.4655, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7743, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "97 Training acc 0.79 loss 0.47299734484455774 100\n",
      "lamda tensor(0.4646, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9284, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.77\n",
      "97 Training acc 0.7966666666666667 loss 0.4768403859923427 150\n",
      "lamda tensor(0.4648, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9624, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.775\n",
      "97 Training acc 0.8 loss 0.475144663172392 200\n",
      "lamda tensor(0.4659, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8550, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.778\n",
      "97 Training acc 0.796 loss 0.4738471822341923 250\n",
      "lamda tensor(0.4682, device='cuda:0', requires_grad=True)\n",
      "tensor(1.1532, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7783333333333333\n",
      "97 Training acc 0.7714414414414416 loss 0.48469169948110385 300\n",
      "lamda tensor(0.4692, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9580, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.79\n",
      "98 Training acc 0.79 loss 0.45969593957622673 50\n",
      "lamda tensor(0.4710, device='cuda:0', requires_grad=True)\n",
      "tensor(0.7646, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.79\n",
      "98 Training acc 0.775 loss 0.47357574092964255 100\n",
      "lamda tensor(0.4734, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8089, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7833333333333333\n",
      "98 Training acc 0.7833333333333333 loss 0.47927287096918597 150\n",
      "lamda tensor(0.4767, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0305, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7775000000000001\n",
      "98 Training acc 0.78 loss 0.4820732591469623 200\n",
      "lamda tensor(0.4785, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0413, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.774\n",
      "98 Training acc 0.79 loss 0.4731438599015784 250\n",
      "lamda tensor(0.4813, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0922, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7716666666666666\n",
      "98 Training acc 0.7754504504504505 loss 0.48193927499653594 300\n",
      "lamda tensor(0.4828, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9494, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "99 Training acc 0.79 loss 0.45855314610763503 50\n",
      "lamda tensor(0.4832, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9753, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "99 Training acc 0.8049999999999999 loss 0.4560997969587473 100\n",
      "lamda tensor(0.4845, device='cuda:0', requires_grad=True)\n",
      "tensor(1.0014, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7600000000000001\n",
      "99 Training acc 0.7933333333333333 loss 0.4718853866420492 150\n",
      "lamda tensor(0.4848, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9047, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "99 Training acc 0.7999999999999999 loss 0.47149143379166164 200\n",
      "lamda tensor(0.4841, device='cuda:0', requires_grad=True)\n",
      "tensor(0.8984, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.76\n",
      "99 Training acc 0.7979999999999999 loss 0.47200886537990083 250\n",
      "lamda tensor(0.4825, device='cuda:0', requires_grad=True)\n",
      "tensor(0.9425, device='cuda:0', dtype=torch.float64, grad_fn=<AbsBackward0>)\n",
      "acc Test: 0.7599999999999999\n",
      "99 Training acc 0.7866216216216215 loss 0.4735367074799164 300\n"
     ]
    }
   ],
   "source": [
    "test,tatin=tr.training()#without lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e70fe1-d416-4e31-974f-f95f11f7172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testno,tatinon=test,tatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d226a182-f543-4b3b-b02d-6ffa123accf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d648420a-30d8-4b5b-aaf5-2a3773b63d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Accuarcy with lambda\")\n",
    "plt.plot(test[0:-1:10],label=\"test accuracy\",linestyle='dashed')\n",
    "plt.plot(tatin[0:-1:10],label=\"train accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea7b538f-ba79-488a-b6f8-539e10302ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Accuarcy without lambda\")\n",
    "plt.plot(np.array(test)*100,label=\"test accuracy\",linestyle='dashed')\n",
    "plt.plot(np.array(tatin)*100,label=\"train accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64be5d-531d-4ceb-86f8-cd1568f98255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
